{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YOHHi18DoWnX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from typing import List, Optional, Tuple, Union\n",
        "\n",
        "import math\n",
        "import random\n",
        "\n",
        "import time\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCwFlxL9om9c",
        "outputId": "4019aa1f-076e-4ace-9ff5-1444835de612"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model part 1"
      ],
      "metadata": {
        "id": "3PlZ8D5paBNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "class TokenImportancesExtractor(nn.Module):\n",
        "\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "\n",
        "        #self.encoder = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        self.decoder = AutoModel.from_pretrained(model_name)\n",
        "        #self.decoder.config.add_cross_attention=True\n",
        "\n",
        "        self.linear = nn.Linear(self.decoder.config.hidden_size, 1)\n",
        "\n",
        "\n",
        "    def forward(self, \n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None\n",
        "        ):\n",
        "\n",
        "        decoder_outputs = self.decoder(\n",
        "            input_ids = input_ids,\n",
        "            attention_mask = attention_mask\n",
        "        )\n",
        "\n",
        "        decoder_hidden_states = decoder_outputs[0]\n",
        "\n",
        "        logits = self.linear(decoder_hidden_states)\n",
        "\n",
        "        return nn.functional.sigmoid(logits) "
      ],
      "metadata": {
        "id": "AMh2u_dtZ4zm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model part 2"
      ],
      "metadata": {
        "id": "SlMdFjaPZ5WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "import tempfile\n",
        "import warnings\n",
        "from typing import Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "\n",
        "from transformers.configuration_utils import PretrainedConfig\n",
        "from transformers.modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "from transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n",
        "from transformers.models.auto.configuration_auto import AutoConfig\n",
        "from transformers.models.auto.modeling_auto import AutoModel, AutoModelForCausalLM\n",
        "from transformers.models.encoder_decoder.configuration_encoder_decoder import EncoderDecoderConfig\n",
        "\n",
        "from transformers.models.encoder_decoder.modeling_encoder_decoder import DEPRECATION_WARNING, shift_tokens_right\n",
        "\n",
        "\n",
        "def forward_encdec(\n",
        "    self,\n",
        "    input_ids: Optional[torch.LongTensor] = None,\n",
        "    token_importances: Optional[torch.LongTensor] = None,\n",
        "    attention_mask: Optional[torch.FloatTensor] = None,\n",
        "    decoder_input_ids: Optional[torch.LongTensor] = None,\n",
        "    decoder_attention_mask: Optional[torch.BoolTensor] = None,\n",
        "    encoder_outputs: Optional[Tuple[torch.FloatTensor]] = None,\n",
        "    past_key_values: Tuple[Tuple[torch.FloatTensor]] = None,\n",
        "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "    decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "    labels: Optional[torch.LongTensor] = None,\n",
        "    use_cache: Optional[bool] = None,\n",
        "    output_attentions: Optional[bool] = None,\n",
        "    output_hidden_states: Optional[bool] = None,\n",
        "    return_dict: Optional[bool] = None,\n",
        "    **kwargs,\n",
        ") -> Union[Tuple, Seq2SeqLMOutput]:\n",
        "    r\"\"\"\n",
        "    Returns:\n",
        "    Examples:\n",
        "    ```python\n",
        "    >>> from transformers import EncoderDecoderModel, BertTokenizer\n",
        "    >>> import torch\n",
        "    >>> tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    >>> model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "    ...     \"bert-base-uncased\", \"bert-base-uncased\"\n",
        "    ... )  # initialize Bert2Bert from pre-trained checkpoints\n",
        "    >>> # training\n",
        "    >>> model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "    >>> model.config.pad_token_id = tokenizer.pad_token_id\n",
        "    >>> model.config.vocab_size = model.config.decoder.vocab_size\n",
        "    >>> input_ids = tokenizer(\"This is a really long text\", return_tensors=\"pt\").input_ids\n",
        "    >>> labels = tokenizer(\"This is the corresponding summary\", return_tensors=\"pt\").input_ids\n",
        "    >>> outputs = model(input_ids=input_ids, labels=labels)\n",
        "    >>> loss, logits = outputs.loss, outputs.logits\n",
        "    >>> # save and load from pretrained\n",
        "    >>> model.save_pretrained(\"bert2bert\")\n",
        "    >>> model = EncoderDecoderModel.from_pretrained(\"bert2bert\")\n",
        "    >>> # generation\n",
        "    >>> generated = model.generate(input_ids)\n",
        "    ```\"\"\"\n",
        "\n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "    kwargs_encoder = {argument: value for argument, value in kwargs.items() if not argument.startswith(\"decoder_\")}\n",
        "\n",
        "    kwargs_decoder = {\n",
        "        argument[len(\"decoder_\") :]: value for argument, value in kwargs.items() if argument.startswith(\"decoder_\")\n",
        "    }\n",
        "\n",
        "    if encoder_outputs is None:\n",
        "        encoder_outputs = self.encoder(\n",
        "            input_ids=input_ids,\n",
        "            token_importances=token_importances,\n",
        "            attention_mask=attention_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "            **kwargs_encoder,\n",
        "        )\n",
        "    elif isinstance(encoder_outputs, tuple):\n",
        "        encoder_outputs = BaseModelOutput(*encoder_outputs)\n",
        "\n",
        "    encoder_hidden_states = encoder_outputs[0]\n",
        "\n",
        "    # optionally project encoder_hidden_states\n",
        "    if (\n",
        "        self.encoder.config.hidden_size != self.decoder.config.hidden_size\n",
        "        and self.decoder.config.cross_attention_hidden_size is None\n",
        "    ):\n",
        "        encoder_hidden_states = self.enc_to_dec_proj(encoder_hidden_states)\n",
        "\n",
        "    if (labels is not None) and (decoder_input_ids is None and decoder_inputs_embeds is None):\n",
        "        decoder_input_ids = shift_tokens_right(\n",
        "            labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
        "        )\n",
        "\n",
        "    # Decode\n",
        "    decoder_outputs = self.decoder(\n",
        "        input_ids=decoder_input_ids,\n",
        "        attention_mask=decoder_attention_mask,\n",
        "        encoder_hidden_states=encoder_hidden_states,\n",
        "        encoder_attention_mask=attention_mask,\n",
        "        inputs_embeds=decoder_inputs_embeds,\n",
        "        output_attentions=output_attentions,\n",
        "        output_hidden_states=output_hidden_states,\n",
        "        use_cache=use_cache,\n",
        "        past_key_values=past_key_values,\n",
        "        return_dict=return_dict,\n",
        "        **kwargs_decoder,\n",
        "    )\n",
        "\n",
        "    # Compute loss independent from decoder (as some shift the logits inside them)\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "        warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
        "        logits = decoder_outputs.logits if return_dict else decoder_outputs[0]\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        loss = loss_fct(logits.reshape(-1, self.decoder.config.vocab_size), labels.view(-1))\n",
        "\n",
        "    if not return_dict:\n",
        "        if loss is not None:\n",
        "            return (loss,) + decoder_outputs + encoder_outputs\n",
        "        else:\n",
        "            return decoder_outputs + encoder_outputs\n",
        "\n",
        "    return Seq2SeqLMOutput(\n",
        "        loss=loss,\n",
        "        logits=decoder_outputs.logits,\n",
        "        past_key_values=decoder_outputs.past_key_values,\n",
        "        decoder_hidden_states=decoder_outputs.hidden_states,\n",
        "        decoder_attentions=decoder_outputs.attentions,\n",
        "        cross_attentions=decoder_outputs.cross_attentions,\n",
        "        encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "        encoder_hidden_states=encoder_outputs.hidden_states,\n",
        "        encoder_attentions=encoder_outputs.attentions,\n",
        "    )"
      ],
      "metadata": {
        "id": "6ZkIQzP8oZs1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
        "    CausalLMOutputWithCrossAttentions,\n",
        "    MaskedLMOutput,\n",
        "    MultipleChoiceModelOutput,\n",
        "    NextSentencePredictorOutput,\n",
        "    QuestionAnsweringModelOutput,\n",
        "    SequenceClassifierOutput,\n",
        "    TokenClassifierOutput,\n",
        ")\n",
        "\n",
        "def forward_enc(\n",
        "    self,\n",
        "    input_ids: Optional[torch.Tensor] = None,\n",
        "    token_importances: Optional[torch.LongTensor] = None,\n",
        "    attention_mask: Optional[torch.Tensor] = None,\n",
        "    token_type_ids: Optional[torch.Tensor] = None,\n",
        "    position_ids: Optional[torch.Tensor] = None,\n",
        "    head_mask: Optional[torch.Tensor] = None,\n",
        "    inputs_embeds: Optional[torch.Tensor] = None,\n",
        "    encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "    encoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "    past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "    use_cache: Optional[bool] = None,\n",
        "    output_attentions: Optional[bool] = None,\n",
        "    output_hidden_states: Optional[bool] = None,\n",
        "    return_dict: Optional[bool] = None,\n",
        ") -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n",
        "    r\"\"\"\n",
        "    encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
        "        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "        the model is configured as a decoder.\n",
        "    encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "        Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "        the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
        "        - 1 for tokens that are **not masked**,\n",
        "        - 0 for tokens that are **masked**.\n",
        "    past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "        Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "        If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
        "        don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
        "        `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
        "    use_cache (`bool`, *optional*):\n",
        "        If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
        "        `past_key_values`).\n",
        "    \"\"\"\n",
        "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "    output_hidden_states = (\n",
        "        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "    )\n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "    if self.config.is_decoder:\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "    else:\n",
        "        use_cache = False\n",
        "\n",
        "    if input_ids is not None and inputs_embeds is not None:\n",
        "        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "    elif input_ids is not None:\n",
        "        input_shape = input_ids.size()\n",
        "    elif inputs_embeds is not None:\n",
        "        input_shape = inputs_embeds.size()[:-1]\n",
        "    else:\n",
        "        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "    batch_size, seq_length = input_shape\n",
        "    device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "    # past_key_values_length\n",
        "    past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "    if attention_mask is None:\n",
        "        attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
        "\n",
        "    if token_type_ids is None:\n",
        "        if hasattr(self.embeddings, \"token_type_ids\"):\n",
        "            buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
        "            buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
        "            token_type_ids = buffered_token_type_ids_expanded\n",
        "        else:\n",
        "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "    # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "    # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "    extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n",
        "\n",
        "    # If a 2D or 3D attention mask is provided for the cross-attention\n",
        "    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "    if self.config.is_decoder and encoder_hidden_states is not None:\n",
        "        encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "        if encoder_attention_mask is None:\n",
        "            encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
        "        encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "    else:\n",
        "        encoder_extended_attention_mask = None\n",
        "\n",
        "    # Prepare head mask if needed\n",
        "    # 1.0 in head_mask indicate we keep the head\n",
        "    # attention_probs has shape bsz x n_heads x N x N\n",
        "    # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "    # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "    head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "    embedding_output = self.embeddings(\n",
        "        input_ids=input_ids,\n",
        "        position_ids=position_ids,\n",
        "        token_type_ids=token_type_ids,\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        past_key_values_length=past_key_values_length,\n",
        "    )\n",
        "    encoder_outputs = self.encoder(\n",
        "        embedding_output,\n",
        "        token_importances=token_importances,\n",
        "        attention_mask=extended_attention_mask,\n",
        "        head_mask=head_mask,\n",
        "        encoder_hidden_states=encoder_hidden_states,\n",
        "        encoder_attention_mask=encoder_extended_attention_mask,\n",
        "        past_key_values=past_key_values,\n",
        "        use_cache=use_cache,\n",
        "        output_attentions=output_attentions,\n",
        "        output_hidden_states=output_hidden_states,\n",
        "        return_dict=return_dict,\n",
        "    )\n",
        "    sequence_output = encoder_outputs[0]\n",
        "    pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "    if not return_dict:\n",
        "        return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "    return BaseModelOutputWithPoolingAndCrossAttentions(\n",
        "        last_hidden_state=sequence_output,\n",
        "        pooler_output=pooled_output,\n",
        "        past_key_values=encoder_outputs.past_key_values,\n",
        "        hidden_states=encoder_outputs.hidden_states,\n",
        "        attentions=encoder_outputs.attentions,\n",
        "        cross_attentions=encoder_outputs.cross_attentions,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "5cNBE1A7UdAa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "def forward_enc_enc(\n",
        "    self,\n",
        "    hidden_states: torch.Tensor,\n",
        "    token_importances: Optional[torch.LongTensor] = None,\n",
        "    attention_mask: Optional[torch.FloatTensor] = None,\n",
        "    head_mask: Optional[torch.FloatTensor] = None,\n",
        "    encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "    encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "    use_cache: Optional[bool] = None,\n",
        "    output_attentions: Optional[bool] = False,\n",
        "    output_hidden_states: Optional[bool] = False,\n",
        "    return_dict: Optional[bool] = True,\n",
        ") -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
        "    all_hidden_states = () if output_hidden_states else None\n",
        "    all_self_attentions = () if output_attentions else None\n",
        "    all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "\n",
        "    next_decoder_cache = () if use_cache else None\n",
        "    for i, layer_module in enumerate(self.layer):\n",
        "\n",
        "        hidden_states = hidden_states + layer_module.linear( token_importances )\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "        past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "        if self.gradient_checkpointing and self.training:\n",
        "\n",
        "            if use_cache:\n",
        "                logger.warning(\n",
        "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
        "                )\n",
        "                use_cache = False\n",
        "\n",
        "            def create_custom_forward(module):\n",
        "                def custom_forward(*inputs):\n",
        "                    return module(*inputs, past_key_value, output_attentions)\n",
        "\n",
        "                return custom_forward\n",
        "\n",
        "            layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                create_custom_forward(layer_module),\n",
        "                hidden_states,\n",
        "                attention_mask,\n",
        "                layer_head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "            )\n",
        "        else:\n",
        "            layer_outputs = layer_module(\n",
        "                hidden_states,\n",
        "                attention_mask,\n",
        "                layer_head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                past_key_value,\n",
        "                output_attentions,\n",
        "            )\n",
        "\n",
        "        hidden_states = layer_outputs[0]\n",
        "        if use_cache:\n",
        "            next_decoder_cache += (layer_outputs[-1],)\n",
        "        if output_attentions:\n",
        "            all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "            if self.config.add_cross_attention:\n",
        "                all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "\n",
        "    if output_hidden_states:\n",
        "        all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "    if not return_dict:\n",
        "        return tuple(\n",
        "            v\n",
        "            for v in [\n",
        "                hidden_states,\n",
        "                next_decoder_cache,\n",
        "                all_hidden_states,\n",
        "                all_self_attentions,\n",
        "                all_cross_attentions,\n",
        "            ]\n",
        "            if v is not None\n",
        "        )\n",
        "    return BaseModelOutputWithPastAndCrossAttentions(\n",
        "        last_hidden_state=hidden_states,\n",
        "        past_key_values=next_decoder_cache,\n",
        "        hidden_states=all_hidden_states,\n",
        "        attentions=all_self_attentions,\n",
        "        cross_attentions=all_cross_attentions,\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "qPM_ZN78TuzZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import EncoderDecoderModel, AutoTokenizer\n",
        "\n",
        "model_name = 'prajjwal1/bert-tiny'\n",
        "\n",
        "M1 = TokenImportancesExtractor(model_name)\n",
        "M2 = EncoderDecoderModel.from_encoder_decoder_pretrained(model_name, model_name)\n",
        "\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJE7o-T4orA5",
        "outputId": "a07fd044-39c4-4c84-c1fa-0642872d5f37"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.key.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "M2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzCTRUJ1IL7b",
        "outputId": "2b4dbd83-acfd-48fd-f370-357ed4a689f3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderDecoderModel(\n",
              "  (encoder): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 128)\n",
              "      (token_type_embeddings): Embedding(2, 128)\n",
              "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
              "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
              "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (decoder): BertLMHeadModel(\n",
              "    (bert): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 128)\n",
              "        (token_type_embeddings): Embedding(2, 128)\n",
              "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (cls): BertOnlyMLMHead(\n",
              "      (predictions): BertLMPredictionHead(\n",
              "        (transform): BertPredictionHeadTransform(\n",
              "          (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (transform_act_fn): GELUActivation()\n",
              "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (decoder): Linear(in_features=128, out_features=30522, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import types\n",
        "funcType = type(M2.forward)\n",
        "M2.forward = types.MethodType(forward_encdec, M2)\n",
        "\n",
        "import types\n",
        "funcType = type(M2.encoder.forward)\n",
        "M2.encoder.forward = types.MethodType(forward_enc, M2.encoder)\n",
        "\n",
        "import types\n",
        "funcType = type(M2.encoder.encoder.forward)\n",
        "M2.encoder.encoder.forward = types.MethodType(forward_enc_enc, M2.encoder.encoder)"
      ],
      "metadata": {
        "id": "eW1lTVb2o4LU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for L in M2.encoder.encoder.layer:\n",
        "    linear=nn.Linear(1, M2.encoder.config.hidden_size)\n",
        "\n",
        "    linear.weight=torch.nn.parameter.Parameter(torch.zeros(linear.weight.shape, dtype=linear.weight.dtype)*1e-4)\n",
        "    linear.bias=torch.nn.parameter.Parameter(torch.zeros(linear.bias.shape, dtype=linear.bias.dtype)*1e-4)\n",
        "\n",
        "    L.linear = linear"
      ],
      "metadata": {
        "id": "CD4mi4-qUySO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Datasets"
      ],
      "metadata": {
        "id": "Zfn1_f5dq6Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "\n",
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "        \n",
        "def download_url(url, output_path):\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
        "\n",
        "def download_data(data_path, url_path, suffix):    \n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "        \n",
        "    data_path = os.path.join(data_path, f'{suffix}.json')\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        print(f\"Downloading CoQA {suffix} data split... (it may take a while)\")\n",
        "        download_url(url=url_path, output_path=data_path)\n",
        "        print(\"Download completed!\")"
      ],
      "metadata": {
        "id": "cH20_lBEpyFQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train data\n",
        "train_url = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n",
        "download_data(data_path='coqa', url_path=train_url, suffix='train')\n",
        "\n",
        "# Test data\n",
        "test_url = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n",
        "download_data(data_path='coqa', url_path=test_url, suffix='test')  # <-- Why test? See next slides for an answer!"
      ],
      "metadata": {
        "id": "WuLMLnrMqKx0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c733b4-f7f4-4feb-dc70-da21f9195315"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading CoQA train data split... (it may take a while)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "coqa-train-v1.0.json: 49.0MB [00:05, 9.15MB/s]                            \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download completed!\n",
            "Downloading CoQA test data split... (it may take a while)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "coqa-dev-v1.0.json: 9.09MB [00:00, 20.4MB/s]                            "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join('coqa', 'train.json'), 'r') as j:\n",
        "    train = json.loads(j.read())\n",
        "\n",
        "with open(os.path.join('coqa', 'test.json'), 'r') as j:\n",
        "    test = json.loads(j.read())"
      ],
      "metadata": {
        "id": "eeBU_gKIqL5c"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train['data']\n",
        "test = test['data']\n",
        "\n",
        "for t in train:\n",
        "    indices = [i for i, a in enumerate(t['answers']) if a['input_text'] != 'unknown']\n",
        "    t['questions'] = [q for i, q in enumerate(t['questions']) if i in indices] \n",
        "    t['answers'] = [a for i, a in enumerate(t['answers']) if i in indices]\n",
        "\n",
        "# TODO DO THE SAME FOR TEST\n",
        "\n",
        "#train_answer_indices = [[i for i, a in enumerate(t.answers) if a.input_text != 'unknown'] for t in train]\n",
        "#test_answer_indices = [[i for i, a in enumerate(t.answers) if a.input_text != 'unknown'] for t in test]\n",
        "\n"
      ],
      "metadata": {
        "id": "61cPdU93qMtB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lengths=[len(doc['questions']) for doc in train]"
      ],
      "metadata": {
        "id": "1_xc2GDgqNYu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le=np.cumsum(np.array(lengths,dtype=np.float32))\n",
        "train_end=np.where((le/le[-1])>0.8)[0][0]\n",
        "\n",
        "validation = train[train_end :] \n",
        "train = train[ : train_end]"
      ],
      "metadata": {
        "id": "P9pRwS3JqOF7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train))\n",
        "print(len(validation))"
      ],
      "metadata": {
        "id": "Ue3pl-4LqPEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c64192-2c8e-4f9f-85a5-d47f32c2eff9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5773\n",
            "1426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_train=np.sum([len(doc['questions']) for doc in train])\n",
        "len_val=np.sum([len(doc['questions']) for doc in validation])\n",
        "\n",
        "len_tot=len_train+len_val\n",
        "print(len_train,len_train/len_tot)\n",
        "print(len_val,len_val/len_tot)"
      ],
      "metadata": {
        "id": "G8uQ8ou7qfPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5f87799-acf9-496c-b5d0-e39f72b70aca"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85810 0.7998993251053358\n",
            "21466 0.20010067489466424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, return_history=False):\n",
        "\n",
        "        self.story=[d['story'] for d in data]\n",
        "        self.questions=[d['questions'] for d in data]\n",
        "        self.answers=[d['answers'] for d in data]\n",
        "        lengths = [len(doc['questions']) for doc in data]\n",
        "        self.lengths = np.cumsum(np.array(lengths,dtype=np.int32))\n",
        "        self.R_H=return_history\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.lengths[-1]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        f_idx=int(np.where(self.lengths > idx)[0][0])\n",
        "        if f_idx>0:\n",
        "            q_idx=idx-self.lengths[f_idx-1]\n",
        "        else:\n",
        "            q_idx=idx\n",
        "\n",
        "        passage=self.story[f_idx]\n",
        "        questions=self.questions[f_idx]\n",
        "        answers=self.answers[f_idx]\n",
        "        question=questions[q_idx]['input_text']\n",
        "        answer=answers[q_idx]['input_text']\n",
        "        span_start=int(answers[q_idx]['span_start'])\n",
        "        span_end=int(answers[q_idx]['span_end'])\n",
        "        span_text=answers[q_idx]['span_text']\n",
        "\n",
        "        if self.R_H:\n",
        "            history = np.concatenate([ [questions[i]['input_text'], answers[i]['input_text']] for i in range(q_idx)],0)\n",
        "            return (passage,question,history), (answer, span_start, span_end)\n",
        "\n",
        "        return (passage,question), (answer, span_start, span_end)"
      ],
      "metadata": {
        "id": "yfayHgIcqgdW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size=8\n",
        "steps_per_update=2\n",
        "steps_empty_cache=None\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(CustomImageDataset(train), batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(CustomImageDataset(validation), batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(CustomImageDataset(test), batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "ZkEFhTGqqiLR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the models"
      ],
      "metadata": {
        "id": "qh3tdKD1a4tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func1(probs, target):\n",
        "    loss=  - torch.log(  probs) * (  target) / (torch.sum(  target, 1, keepdim=True)+1)\n",
        "    loss+= - torch.log(1-probs) * (1-target) / (torch.sum(1-target, 1, keepdim=True)+1)\n",
        "    return torch.mean(torch.sum(loss,(1,2)))"
      ],
      "metadata": {
        "id": "Ku1ty3CdbT1k"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train1(model, tokenizer, epochs=1, learning_rate=1e-5, optimizer=None, loss_history=[], steps_per_update=1, steps_empty_cache=None):\n",
        "    \n",
        "    model.to('cuda')\n",
        "\n",
        "    #model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "    #model.config.pad_token_id = tokenizer.pad_token_id\n",
        "    if optimizer is None:\n",
        "        optimizer = torch.optim.AdamW(iter(list(model.parameters())), lr=learning_rate)\n",
        "    \n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "    \n",
        "        torch.cuda.empty_cache()\n",
        "        running_loss = 0.0\n",
        "        optimizer.zero_grad()\n",
        "        start_time = time.time()\n",
        "        for batch_idx, data in enumerate(train_dataloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            (passage, question), (answer, sep_starts, sep_ends) = data\n",
        "\n",
        "            #text_input = [question[i] + ' [SEP] ' + passage[i] for i in range(len(passage))]\n",
        "\n",
        "            inputs = tokenizer(\n",
        "                        question,\n",
        "                        passage,\n",
        "                        max_length=512,\n",
        "                        truncation=True,\n",
        "                        padding=True,\n",
        "                        return_tensors=\"pt\",\n",
        "                    ).to('cuda')\n",
        "            \n",
        "            pred = model.forward(inputs.input_ids,\n",
        "                                inputs.attention_mask)\n",
        "\n",
        "            y=torch.zeros(inputs.input_ids.shape+(1,), device=pred.device)\n",
        "\n",
        "            for i in range(len(sep_starts)):\n",
        "                \n",
        "                start_tok=inputs.char_to_token(i,sep_starts[i],1)\n",
        "                end_tok=inputs.char_to_token(i,sep_ends[i],1)\n",
        "\n",
        "                if start_tok is None:\n",
        "                    start_tok=inputs.char_to_token(i,sep_starts[i]+1,1)\n",
        "                if start_tok is None:\n",
        "                    start_tok=inputs.char_to_token(i,sep_starts[i]-1,1)\n",
        "\n",
        "                if end_tok is None:\n",
        "                    end_tok=inputs.char_to_token(i,sep_ends[i]-1,1)\n",
        "                if end_tok is None:\n",
        "                    end_tok=inputs.char_to_token(i,sep_ends[i]+1,1)\n",
        "                \n",
        "                y[i, start_tok : end_tok] = 1\n",
        "                \n",
        "\n",
        "            loss=loss_func1(pred,y)\n",
        "\n",
        "            loss.backward()\n",
        "            \n",
        "            if batch_idx % steps_per_update == steps_per_update-1:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            if steps_empty_cache is not None:\n",
        "                if batch_idx % steps_empty_cache == steps_empty_cache-1:\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            loss_history.append(loss.detach().cpu().numpy())\n",
        "            \n",
        "            epoch_time = time.time() - start_time\n",
        "            batch_time = epoch_time/(batch_idx+1)\n",
        "\n",
        "            print(f\"epoch: {epoch + 1}/{epochs}, {batch_idx + 1}/{len(train_dataloader)}, {epoch_time:.0f}s {batch_time*1e3:.0f}ms/step, lr: {optimizer.param_groups[0]['lr']:.3g}, loss: {running_loss/(batch_idx+1):.3g}               \")#, end = '\\r'\n",
        "\n",
        "        print(f\"epoch: {epoch + 1}/{epochs}, {batch_idx + 1}/{len(train_dataloader)}, {epoch_time:.0f}s {batch_time*1e3:.0f}ms/step, lr: {optimizer.param_groups[0]['lr']:.3g}, loss: {running_loss/(batch_idx+1):.3g}              \")\n",
        "\n",
        "    return loss_history, optimizer"
      ],
      "metadata": {
        "id": "Iap5_T-WbA5q"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train2(M1, M2, tokenizer, epochs=3, learning_rate=1e-5, optimizer=None, loss_history=[], train_M1=False, steps_per_update=1, steps_empty_cache=None):\n",
        "\n",
        "    M1.to('cuda')\n",
        "    M2.to('cuda')\n",
        "\n",
        "    M2.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "    M2.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    if optimizer is None:\n",
        "        if train_M1:\n",
        "            optimizer = torch.optim.Adam(iter(list(M1.parameters())+list(M2.parameters())), lr=learning_rate)\n",
        "        else:\n",
        "            optimizer = torch.optim.Adam(iter(list(M2.parameters())), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "    \n",
        "        torch.cuda.empty_cache()\n",
        "        running_loss = 0.0\n",
        "        optimizer.zero_grad()\n",
        "        start_time = time.time()\n",
        "        for batch_idx, data in enumerate(train_dataloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            (passage, question), (answer, sep_starts, sep_ends) = data\n",
        "\n",
        "            # text_input = [question[i] + ' [SEP] ' + passage[i] for i in range(len(passage))]\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            \n",
        "            inputs = tokenizer(\n",
        "                        question,\n",
        "                        passage,\n",
        "                        max_length=512,\n",
        "                        truncation=True,\n",
        "                        padding=True,\n",
        "                        return_tensors=\"pt\",\n",
        "                    ).to('cuda')\n",
        "\n",
        "            labels = tokenizer(\n",
        "                answer,\n",
        "                max_length=512,\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                return_tensors=\"pt\",\n",
        "            ).to('cuda')\n",
        "\n",
        "            if train_M1:\n",
        "                O1 = M1.forward(inputs.input_ids,\n",
        "                                inputs.attention_mask)\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    O1 = M1.forward(inputs.input_ids,\n",
        "                                    inputs.attention_mask)\n",
        "\n",
        "            O2 = M2(    input_ids = inputs.input_ids,\n",
        "                         labels = labels.input_ids,\n",
        "                         token_importances = O1)\n",
        "            \n",
        "            loss = O2.loss\n",
        "            loss.backward()\n",
        "\n",
        "            if batch_idx % steps_per_update == steps_per_update-1:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            if steps_empty_cache is not None:\n",
        "                if batch_idx % steps_empty_cache == steps_empty_cache-1:\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            loss_history.append(loss.detach().cpu().numpy())\n",
        "            \n",
        "            epoch_time = time.time() - start_time\n",
        "            batch_time = epoch_time/(batch_idx+1)\n",
        "            \n",
        "            print(f\"epoch: {epoch + 1}/{epochs}, {batch_idx + 1}/{len(train_dataloader)}, {epoch_time:.0f}s {batch_time*1e3:.0f}ms/step, lr: {optimizer.param_groups[0]['lr']:.3g}, loss: {running_loss/(batch_idx+1):.3g}\")\n",
        "\n",
        "        print(f\"epoch: {epoch + 1}/{epochs}, {batch_idx + 1}/{len(train_dataloader)}, {epoch_time:.0f}s {batch_time*1e3:.0f}ms/step, lr: {optimizer.param_groups[0]['lr']:.3g}, loss: {running_loss/(batch_idx+1):.3g}\")\n",
        "\n",
        "    return loss_history, optimizer"
      ],
      "metadata": {
        "id": "R5SQqCBx_uAC"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lh1=[]\n",
        "lh2=[]\n",
        "lh3=[]\n",
        "\n",
        "optim1=None\n",
        "optim2=None\n",
        "optim3=None"
      ],
      "metadata": {
        "id": "PY0pQjDQocXo"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lh1, optim1 = train1(M1, tokenizer, epochs=2, learning_rate=1e-5, optimizer=optim1, loss_history=lh1, steps_per_update=steps_per_update)"
      ],
      "metadata": {
        "id": "zdtSolEDfgHP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ce738d8-032f-4a07-f3ca-32734dac12f3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1/2, 1/10727, 4s 3852ms/step, lr: 1e-05, loss: 1.35               \n",
            "epoch: 1/2, 2/10727, 4s 1963ms/step, lr: 1e-05, loss: 1.32               \n",
            "epoch: 1/2, 3/10727, 4s 1334ms/step, lr: 1e-05, loss: 1.29               \n",
            "epoch: 1/2, 4/10727, 4s 1011ms/step, lr: 1e-05, loss: 1.33               \n",
            "epoch: 1/2, 5/10727, 4s 816ms/step, lr: 1e-05, loss: 1.34               \n",
            "epoch: 1/2, 6/10727, 4s 685ms/step, lr: 1e-05, loss: 1.35               \n",
            "epoch: 1/2, 7/10727, 4s 591ms/step, lr: 1e-05, loss: 1.33               \n",
            "epoch: 1/2, 8/10727, 4s 521ms/step, lr: 1e-05, loss: 1.33               \n",
            "epoch: 1/2, 9/10727, 4s 466ms/step, lr: 1e-05, loss: 1.33               \n",
            "epoch: 1/2, 10/10727, 4s 423ms/step, lr: 1e-05, loss: 1.32               \n",
            "epoch: 1/2, 11/10727, 4s 388ms/step, lr: 1e-05, loss: 1.33               \n",
            "epoch: 1/2, 12/10727, 4s 360ms/step, lr: 1e-05, loss: 1.32               \n",
            "epoch: 1/2, 13/10727, 4s 335ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 14/10727, 4s 317ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 15/10727, 4s 297ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 16/10727, 4s 280ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 17/10727, 5s 266ms/step, lr: 1e-05, loss: 1.32               \n",
            "epoch: 1/2, 18/10727, 5s 253ms/step, lr: 1e-05, loss: 1.32               \n",
            "epoch: 1/2, 19/10727, 5s 241ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 20/10727, 5s 230ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 21/10727, 5s 221ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 22/10727, 5s 212ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 23/10727, 5s 204ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 24/10727, 5s 197ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 25/10727, 5s 190ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 26/10727, 5s 184ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 27/10727, 5s 178ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 28/10727, 5s 173ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 29/10727, 5s 168ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 30/10727, 5s 163ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 31/10727, 5s 159ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 32/10727, 5s 156ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 33/10727, 5s 152ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 34/10727, 5s 148ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 35/10727, 5s 145ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 36/10727, 5s 142ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 37/10727, 5s 138ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 38/10727, 5s 136ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 39/10727, 5s 133ms/step, lr: 1e-05, loss: 1.32               \n",
            "epoch: 1/2, 40/10727, 5s 130ms/step, lr: 1e-05, loss: 1.32               \n",
            "epoch: 1/2, 41/10727, 5s 128ms/step, lr: 1e-05, loss: 1.32               \n",
            "epoch: 1/2, 42/10727, 5s 126ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 43/10727, 5s 123ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 44/10727, 5s 122ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 45/10727, 5s 120ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 46/10727, 5s 119ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 47/10727, 5s 117ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 48/10727, 6s 115ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 49/10727, 6s 113ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 50/10727, 6s 112ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 51/10727, 6s 110ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 52/10727, 6s 109ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 53/10727, 6s 108ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 54/10727, 6s 106ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 55/10727, 6s 105ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 56/10727, 6s 104ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 57/10727, 6s 103ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 58/10727, 6s 102ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 59/10727, 6s 101ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 60/10727, 6s 100ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 61/10727, 6s 99ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 62/10727, 6s 98ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 63/10727, 6s 97ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 64/10727, 6s 96ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 65/10727, 6s 95ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 66/10727, 6s 95ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 67/10727, 6s 94ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 68/10727, 6s 93ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 69/10727, 6s 92ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 70/10727, 6s 91ms/step, lr: 1e-05, loss: 1.31               \n",
            "epoch: 1/2, 71/10727, 6s 90ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 72/10727, 6s 90ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 73/10727, 6s 89ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 74/10727, 7s 88ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 75/10727, 7s 87ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 76/10727, 7s 87ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 77/10727, 7s 86ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 78/10727, 7s 85ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 79/10727, 7s 85ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 80/10727, 7s 85ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 81/10727, 7s 84ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 82/10727, 7s 83ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 83/10727, 7s 83ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 84/10727, 7s 82ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 85/10727, 7s 82ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 86/10727, 7s 81ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 87/10727, 7s 81ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 88/10727, 7s 80ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 89/10727, 7s 80ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 90/10727, 7s 79ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 91/10727, 7s 79ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 92/10727, 7s 78ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 93/10727, 7s 78ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 94/10727, 7s 77ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 95/10727, 7s 77ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 96/10727, 7s 77ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 97/10727, 7s 76ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 98/10727, 7s 76ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 99/10727, 7s 75ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 100/10727, 7s 75ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 101/10727, 8s 74ms/step, lr: 1e-05, loss: 1.3               \n",
            "epoch: 1/2, 102/10727, 8s 74ms/step, lr: 1e-05, loss: 1.29               \n",
            "epoch: 1/2, 103/10727, 8s 73ms/step, lr: 1e-05, loss: 1.29               \n",
            "epoch: 1/2, 104/10727, 8s 73ms/step, lr: 1e-05, loss: 1.29               \n",
            "epoch: 1/2, 105/10727, 8s 73ms/step, lr: 1e-05, loss: 1.29               \n",
            "epoch: 1/2, 106/10727, 8s 72ms/step, lr: 1e-05, loss: 1.29               \n",
            "epoch: 1/2, 107/10727, 8s 72ms/step, lr: 1e-05, loss: 1.29               \n",
            "epoch: 1/2, 108/10727, 8s 71ms/step, lr: 1e-05, loss: 1.29               \n",
            "epoch: 1/2, 109/10727, 8s 71ms/step, lr: 1e-05, loss: 1.29               \n",
            "epoch: 1/2, 110/10727, 8s 71ms/step, lr: 1e-05, loss: 1.29               \n",
            "epoch: 1/2, 111/10727, 8s 70ms/step, lr: 1e-05, loss: 1.29               \n",
            "epoch: 1/2, 112/10727, 8s 70ms/step, lr: 1e-05, loss: 1.29               \n",
            "epoch: 1/2, 113/10727, 8s 69ms/step, lr: 1e-05, loss: 1.29               \n",
            "epoch: 1/2, 114/10727, 8s 69ms/step, lr: 1e-05, loss: 1.29               \n",
            "epoch: 1/2, 115/10727, 8s 69ms/step, lr: 1e-05, loss: 1.29               \n",
            "epoch: 1/2, 116/10727, 8s 68ms/step, lr: 1e-05, loss: 1.29               \n",
            "epoch: 1/2, 117/10727, 8s 68ms/step, lr: 1e-05, loss: 1.29               \n",
            "epoch: 1/2, 118/10727, 8s 68ms/step, lr: 1e-05, loss: 1.29               \n",
            "epoch: 1/2, 119/10727, 8s 68ms/step, lr: 1e-05, loss: 1.29               \n",
            "epoch: 1/2, 120/10727, 8s 67ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 121/10727, 8s 67ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 122/10727, 8s 67ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 123/10727, 8s 66ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 124/10727, 8s 66ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 125/10727, 8s 66ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 126/10727, 8s 65ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 127/10727, 8s 65ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 128/10727, 8s 65ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 129/10727, 8s 65ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 130/10727, 8s 64ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 131/10727, 8s 64ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 132/10727, 8s 64ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 133/10727, 8s 63ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 134/10727, 8s 63ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 135/10727, 8s 63ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 136/10727, 9s 63ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 137/10727, 9s 63ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 138/10727, 9s 62ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 139/10727, 9s 62ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 140/10727, 9s 62ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 141/10727, 9s 62ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 142/10727, 9s 62ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 143/10727, 9s 62ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 144/10727, 9s 62ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 145/10727, 9s 62ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 146/10727, 9s 61ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 147/10727, 9s 61ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 148/10727, 9s 61ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 149/10727, 9s 61ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 150/10727, 9s 61ms/step, lr: 1e-05, loss: 1.28               \n",
            "epoch: 1/2, 151/10727, 9s 61ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 152/10727, 9s 61ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 153/10727, 9s 60ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 154/10727, 9s 60ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 155/10727, 9s 60ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 156/10727, 9s 60ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 157/10727, 9s 60ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 158/10727, 9s 60ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 159/10727, 9s 59ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 160/10727, 9s 59ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 161/10727, 10s 59ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 162/10727, 10s 59ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 163/10727, 10s 59ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 164/10727, 10s 59ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 165/10727, 10s 59ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 166/10727, 10s 59ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 167/10727, 10s 58ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 168/10727, 10s 58ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 169/10727, 10s 58ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 170/10727, 10s 58ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 171/10727, 10s 58ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 172/10727, 10s 58ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 173/10727, 10s 57ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 174/10727, 10s 57ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 175/10727, 10s 57ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 176/10727, 10s 57ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 177/10727, 10s 57ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 178/10727, 10s 57ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 179/10727, 10s 57ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 180/10727, 10s 56ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 181/10727, 10s 56ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 182/10727, 10s 56ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 183/10727, 10s 56ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 184/10727, 10s 56ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 185/10727, 10s 56ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 186/10727, 10s 56ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 187/10727, 10s 56ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 188/10727, 10s 56ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 189/10727, 10s 55ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 190/10727, 11s 55ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 191/10727, 11s 55ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 192/10727, 11s 55ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 193/10727, 11s 55ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 194/10727, 11s 55ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 195/10727, 11s 54ms/step, lr: 1e-05, loss: 1.27               \n",
            "epoch: 1/2, 196/10727, 11s 54ms/step, lr: 1e-05, loss: 1.26               \n",
            "epoch: 1/2, 197/10727, 11s 54ms/step, lr: 1e-05, loss: 1.26               \n",
            "epoch: 1/2, 198/10727, 11s 54ms/step, lr: 1e-05, loss: 1.26               \n",
            "epoch: 1/2, 199/10727, 11s 54ms/step, lr: 1e-05, loss: 1.26               \n",
            "epoch: 1/2, 200/10727, 11s 53ms/step, lr: 1e-05, loss: 1.26               \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-fd5f950902fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-05864f0f7ae0>\u001b[0m in \u001b[0;36mtrain1\u001b[0;34m(model, tokenizer, epochs, learning_rate, optimizer, loss_history, steps_per_update, steps_empty_cache)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_func1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msteps_per_update\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msteps_per_update\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lh1)"
      ],
      "metadata": {
        "id": "iqUqv2k8oXC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lh2, optim2 = train2(M1, M2, tokenizer, epochs=2, learning_rate=1e-5, optimizer=optim2, loss_history=lh2, steps_per_update=steps_per_update)"
      ],
      "metadata": {
        "id": "KUDih-I2PUz8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "28736e19-faba-40a7-beee-20a87643df12"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-979a650b9590>:120: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1/2, 1/10727, 0s 45ms/step, lr: 1e-05, loss: 14.5\n",
            "epoch: 1/2, 2/10727, 0s 43ms/step, lr: 1e-05, loss: 14.5\n",
            "epoch: 1/2, 3/10727, 0s 40ms/step, lr: 1e-05, loss: 14.4\n",
            "epoch: 1/2, 4/10727, 0s 39ms/step, lr: 1e-05, loss: 14.4\n",
            "epoch: 1/2, 5/10727, 0s 39ms/step, lr: 1e-05, loss: 14.4\n",
            "epoch: 1/2, 6/10727, 0s 39ms/step, lr: 1e-05, loss: 14.4\n",
            "epoch: 1/2, 7/10727, 0s 38ms/step, lr: 1e-05, loss: 14.4\n",
            "epoch: 1/2, 8/10727, 0s 37ms/step, lr: 1e-05, loss: 14.4\n",
            "epoch: 1/2, 9/10727, 0s 36ms/step, lr: 1e-05, loss: 14.5\n",
            "epoch: 1/2, 10/10727, 0s 36ms/step, lr: 1e-05, loss: 14.4\n",
            "epoch: 1/2, 11/10727, 0s 35ms/step, lr: 1e-05, loss: 14.4\n",
            "epoch: 1/2, 12/10727, 0s 34ms/step, lr: 1e-05, loss: 14.4\n",
            "epoch: 1/2, 13/10727, 0s 34ms/step, lr: 1e-05, loss: 14.4\n",
            "epoch: 1/2, 14/10727, 0s 34ms/step, lr: 1e-05, loss: 14.3\n",
            "epoch: 1/2, 15/10727, 0s 33ms/step, lr: 1e-05, loss: 14.3\n",
            "epoch: 1/2, 16/10727, 1s 33ms/step, lr: 1e-05, loss: 14.3\n",
            "epoch: 1/2, 17/10727, 1s 33ms/step, lr: 1e-05, loss: 14.2\n",
            "epoch: 1/2, 18/10727, 1s 33ms/step, lr: 1e-05, loss: 14.2\n",
            "epoch: 1/2, 19/10727, 1s 33ms/step, lr: 1e-05, loss: 14.1\n",
            "epoch: 1/2, 20/10727, 1s 33ms/step, lr: 1e-05, loss: 14.1\n",
            "epoch: 1/2, 21/10727, 1s 33ms/step, lr: 1e-05, loss: 14.1\n",
            "epoch: 1/2, 22/10727, 1s 33ms/step, lr: 1e-05, loss: 14\n",
            "epoch: 1/2, 23/10727, 1s 32ms/step, lr: 1e-05, loss: 14\n",
            "epoch: 1/2, 24/10727, 1s 32ms/step, lr: 1e-05, loss: 14\n",
            "epoch: 1/2, 25/10727, 1s 32ms/step, lr: 1e-05, loss: 14\n",
            "epoch: 1/2, 26/10727, 1s 32ms/step, lr: 1e-05, loss: 13.9\n",
            "epoch: 1/2, 27/10727, 1s 32ms/step, lr: 1e-05, loss: 13.9\n",
            "epoch: 1/2, 28/10727, 1s 32ms/step, lr: 1e-05, loss: 13.9\n",
            "epoch: 1/2, 29/10727, 1s 32ms/step, lr: 1e-05, loss: 13.8\n",
            "epoch: 1/2, 30/10727, 1s 32ms/step, lr: 1e-05, loss: 13.8\n",
            "epoch: 1/2, 31/10727, 1s 32ms/step, lr: 1e-05, loss: 13.8\n",
            "epoch: 1/2, 32/10727, 1s 32ms/step, lr: 1e-05, loss: 13.8\n",
            "epoch: 1/2, 33/10727, 1s 32ms/step, lr: 1e-05, loss: 13.7\n",
            "epoch: 1/2, 34/10727, 1s 32ms/step, lr: 1e-05, loss: 13.7\n",
            "epoch: 1/2, 35/10727, 1s 32ms/step, lr: 1e-05, loss: 13.7\n",
            "epoch: 1/2, 36/10727, 1s 32ms/step, lr: 1e-05, loss: 13.6\n",
            "epoch: 1/2, 37/10727, 1s 32ms/step, lr: 1e-05, loss: 13.6\n",
            "epoch: 1/2, 38/10727, 1s 32ms/step, lr: 1e-05, loss: 13.6\n",
            "epoch: 1/2, 39/10727, 1s 32ms/step, lr: 1e-05, loss: 13.5\n",
            "epoch: 1/2, 40/10727, 1s 32ms/step, lr: 1e-05, loss: 13.5\n",
            "epoch: 1/2, 41/10727, 1s 32ms/step, lr: 1e-05, loss: 13.5\n",
            "epoch: 1/2, 42/10727, 1s 32ms/step, lr: 1e-05, loss: 13.5\n",
            "epoch: 1/2, 43/10727, 1s 32ms/step, lr: 1e-05, loss: 13.4\n",
            "epoch: 1/2, 44/10727, 1s 32ms/step, lr: 1e-05, loss: 13.4\n",
            "epoch: 1/2, 45/10727, 1s 32ms/step, lr: 1e-05, loss: 13.4\n",
            "epoch: 1/2, 46/10727, 1s 32ms/step, lr: 1e-05, loss: 13.3\n",
            "epoch: 1/2, 47/10727, 1s 32ms/step, lr: 1e-05, loss: 13.3\n",
            "epoch: 1/2, 48/10727, 2s 32ms/step, lr: 1e-05, loss: 13.3\n",
            "epoch: 1/2, 49/10727, 2s 32ms/step, lr: 1e-05, loss: 13.2\n",
            "epoch: 1/2, 50/10727, 2s 32ms/step, lr: 1e-05, loss: 13.2\n",
            "epoch: 1/2, 51/10727, 2s 32ms/step, lr: 1e-05, loss: 13.2\n",
            "epoch: 1/2, 52/10727, 2s 32ms/step, lr: 1e-05, loss: 13.2\n",
            "epoch: 1/2, 53/10727, 2s 31ms/step, lr: 1e-05, loss: 13.1\n",
            "epoch: 1/2, 54/10727, 2s 32ms/step, lr: 1e-05, loss: 13.1\n",
            "epoch: 1/2, 55/10727, 2s 31ms/step, lr: 1e-05, loss: 13.1\n",
            "epoch: 1/2, 56/10727, 2s 32ms/step, lr: 1e-05, loss: 13\n",
            "epoch: 1/2, 57/10727, 2s 31ms/step, lr: 1e-05, loss: 13\n",
            "epoch: 1/2, 58/10727, 2s 32ms/step, lr: 1e-05, loss: 13\n",
            "epoch: 1/2, 59/10727, 2s 32ms/step, lr: 1e-05, loss: 13\n",
            "epoch: 1/2, 60/10727, 2s 32ms/step, lr: 1e-05, loss: 12.9\n",
            "epoch: 1/2, 61/10727, 2s 31ms/step, lr: 1e-05, loss: 12.9\n",
            "epoch: 1/2, 62/10727, 2s 31ms/step, lr: 1e-05, loss: 12.9\n",
            "epoch: 1/2, 63/10727, 2s 31ms/step, lr: 1e-05, loss: 12.8\n",
            "epoch: 1/2, 64/10727, 2s 31ms/step, lr: 1e-05, loss: 12.8\n",
            "epoch: 1/2, 65/10727, 2s 31ms/step, lr: 1e-05, loss: 12.8\n",
            "epoch: 1/2, 66/10727, 2s 31ms/step, lr: 1e-05, loss: 12.8\n",
            "epoch: 1/2, 67/10727, 2s 31ms/step, lr: 1e-05, loss: 12.7\n",
            "epoch: 1/2, 68/10727, 2s 31ms/step, lr: 1e-05, loss: 12.7\n",
            "epoch: 1/2, 69/10727, 2s 31ms/step, lr: 1e-05, loss: 12.7\n",
            "epoch: 1/2, 70/10727, 2s 31ms/step, lr: 1e-05, loss: 12.6\n",
            "epoch: 1/2, 71/10727, 2s 31ms/step, lr: 1e-05, loss: 12.6\n",
            "epoch: 1/2, 72/10727, 2s 31ms/step, lr: 1e-05, loss: 12.6\n",
            "epoch: 1/2, 73/10727, 2s 31ms/step, lr: 1e-05, loss: 12.6\n",
            "epoch: 1/2, 74/10727, 2s 31ms/step, lr: 1e-05, loss: 12.5\n",
            "epoch: 1/2, 75/10727, 2s 31ms/step, lr: 1e-05, loss: 12.5\n",
            "epoch: 1/2, 76/10727, 2s 31ms/step, lr: 1e-05, loss: 12.5\n",
            "epoch: 1/2, 77/10727, 2s 31ms/step, lr: 1e-05, loss: 12.4\n",
            "epoch: 1/2, 78/10727, 2s 31ms/step, lr: 1e-05, loss: 12.4\n",
            "epoch: 1/2, 79/10727, 2s 31ms/step, lr: 1e-05, loss: 12.4\n",
            "epoch: 1/2, 80/10727, 2s 31ms/step, lr: 1e-05, loss: 12.4\n",
            "epoch: 1/2, 81/10727, 3s 31ms/step, lr: 1e-05, loss: 12.3\n",
            "epoch: 1/2, 82/10727, 3s 31ms/step, lr: 1e-05, loss: 12.3\n",
            "epoch: 1/2, 83/10727, 3s 31ms/step, lr: 1e-05, loss: 12.3\n",
            "epoch: 1/2, 84/10727, 3s 31ms/step, lr: 1e-05, loss: 12.3\n",
            "epoch: 1/2, 85/10727, 3s 31ms/step, lr: 1e-05, loss: 12.2\n",
            "epoch: 1/2, 86/10727, 3s 31ms/step, lr: 1e-05, loss: 12.2\n",
            "epoch: 1/2, 87/10727, 3s 31ms/step, lr: 1e-05, loss: 12.2\n",
            "epoch: 1/2, 88/10727, 3s 31ms/step, lr: 1e-05, loss: 12.2\n",
            "epoch: 1/2, 89/10727, 3s 31ms/step, lr: 1e-05, loss: 12.1\n",
            "epoch: 1/2, 90/10727, 3s 31ms/step, lr: 1e-05, loss: 12.1\n",
            "epoch: 1/2, 91/10727, 3s 31ms/step, lr: 1e-05, loss: 12.1\n",
            "epoch: 1/2, 92/10727, 3s 31ms/step, lr: 1e-05, loss: 12\n",
            "epoch: 1/2, 93/10727, 3s 31ms/step, lr: 1e-05, loss: 12\n",
            "epoch: 1/2, 94/10727, 3s 31ms/step, lr: 1e-05, loss: 12\n",
            "epoch: 1/2, 95/10727, 3s 31ms/step, lr: 1e-05, loss: 12\n",
            "epoch: 1/2, 96/10727, 3s 31ms/step, lr: 1e-05, loss: 11.9\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-764e18e7557d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-ca36e26df586>\u001b[0m in \u001b[0;36mtrain2\u001b[0;34m(M1, M2, tokenizer, epochs, learning_rate, optimizer, loss_history, train_M1, steps_per_update, steps_empty_cache)\u001b[0m\n\u001b[1;32m     52\u001b[0m                                     inputs.attention_mask)\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             O2 = M2(    input_ids = inputs.input_ids,\n\u001b[0m\u001b[1;32m     55\u001b[0m                          \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                          token_importances = O1)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-979a650b9590>\u001b[0m in \u001b[0;36mforward_encdec\u001b[0;34m(self, input_ids, token_importances, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0muse_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1238\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         )\n\u001b[0;32m-> 1021\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1022\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    608\u001b[0m                 )\n\u001b[1;32m    609\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    611\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0;31m# cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0mcross_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             cross_attention_outputs = self.crossattention(\n\u001b[0m\u001b[1;32m    523\u001b[0m                 \u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         )\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lh2)"
      ],
      "metadata": {
        "id": "V7FVXezsoaR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "4WwJtzel5NKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lh3, optim3 = train2(M1, M2, tokenizer, epochs=1, learning_rate=1e-5, optimizer=optim3, loss_history=lh3, train_M1=True, steps_per_update=steps_per_update, steps_empty_cache=steps_empty_cache)"
      ],
      "metadata": {
        "id": "Q8SWGlT5f7ls",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "106a3f18-dfeb-4f00-dcc8-991fd3660b0f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-979a650b9590>:120: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1/1, 1/10727, 0s 50ms/step, lr: 1e-05, loss: 8.51\n",
            "epoch: 1/1, 2/10727, 0s 51ms/step, lr: 1e-05, loss: 8.77\n",
            "epoch: 1/1, 3/10727, 0s 47ms/step, lr: 1e-05, loss: 8.58\n",
            "epoch: 1/1, 4/10727, 0s 47ms/step, lr: 1e-05, loss: 8.68\n",
            "epoch: 1/1, 5/10727, 0s 44ms/step, lr: 1e-05, loss: 8.78\n",
            "epoch: 1/1, 6/10727, 0s 46ms/step, lr: 1e-05, loss: 8.85\n",
            "epoch: 1/1, 7/10727, 0s 44ms/step, lr: 1e-05, loss: 8.85\n",
            "epoch: 1/1, 8/10727, 0s 44ms/step, lr: 1e-05, loss: 8.87\n",
            "epoch: 1/1, 9/10727, 0s 43ms/step, lr: 1e-05, loss: 8.89\n",
            "epoch: 1/1, 10/10727, 0s 43ms/step, lr: 1e-05, loss: 8.8\n",
            "epoch: 1/1, 11/10727, 0s 42ms/step, lr: 1e-05, loss: 8.76\n",
            "epoch: 1/1, 12/10727, 0s 42ms/step, lr: 1e-05, loss: 8.73\n",
            "epoch: 1/1, 13/10727, 1s 41ms/step, lr: 1e-05, loss: 8.69\n",
            "epoch: 1/1, 14/10727, 1s 41ms/step, lr: 1e-05, loss: 8.65\n",
            "epoch: 1/1, 15/10727, 1s 40ms/step, lr: 1e-05, loss: 8.66\n",
            "epoch: 1/1, 16/10727, 1s 40ms/step, lr: 1e-05, loss: 8.7\n",
            "epoch: 1/1, 17/10727, 1s 40ms/step, lr: 1e-05, loss: 8.66\n",
            "epoch: 1/1, 18/10727, 1s 40ms/step, lr: 1e-05, loss: 8.59\n",
            "epoch: 1/1, 19/10727, 1s 39ms/step, lr: 1e-05, loss: 8.61\n",
            "epoch: 1/1, 20/10727, 1s 39ms/step, lr: 1e-05, loss: 8.58\n",
            "epoch: 1/1, 21/10727, 1s 39ms/step, lr: 1e-05, loss: 8.55\n",
            "epoch: 1/1, 22/10727, 1s 39ms/step, lr: 1e-05, loss: 8.5\n",
            "epoch: 1/1, 23/10727, 1s 39ms/step, lr: 1e-05, loss: 8.53\n",
            "epoch: 1/1, 24/10727, 1s 40ms/step, lr: 1e-05, loss: 8.54\n",
            "epoch: 1/1, 25/10727, 1s 39ms/step, lr: 1e-05, loss: 8.51\n",
            "epoch: 1/1, 26/10727, 1s 39ms/step, lr: 1e-05, loss: 8.49\n",
            "epoch: 1/1, 27/10727, 1s 39ms/step, lr: 1e-05, loss: 8.44\n",
            "epoch: 1/1, 28/10727, 1s 39ms/step, lr: 1e-05, loss: 8.43\n",
            "epoch: 1/1, 29/10727, 1s 39ms/step, lr: 1e-05, loss: 8.42\n",
            "epoch: 1/1, 30/10727, 1s 39ms/step, lr: 1e-05, loss: 8.38\n",
            "epoch: 1/1, 31/10727, 1s 39ms/step, lr: 1e-05, loss: 8.35\n",
            "epoch: 1/1, 32/10727, 1s 39ms/step, lr: 1e-05, loss: 8.31\n",
            "epoch: 1/1, 33/10727, 1s 39ms/step, lr: 1e-05, loss: 8.3\n",
            "epoch: 1/1, 34/10727, 1s 39ms/step, lr: 1e-05, loss: 8.3\n",
            "epoch: 1/1, 35/10727, 1s 38ms/step, lr: 1e-05, loss: 8.3\n",
            "epoch: 1/1, 36/10727, 1s 39ms/step, lr: 1e-05, loss: 8.3\n",
            "epoch: 1/1, 37/10727, 1s 38ms/step, lr: 1e-05, loss: 8.27\n",
            "epoch: 1/1, 38/10727, 1s 38ms/step, lr: 1e-05, loss: 8.25\n",
            "epoch: 1/1, 39/10727, 1s 38ms/step, lr: 1e-05, loss: 8.21\n",
            "epoch: 1/1, 40/10727, 2s 39ms/step, lr: 1e-05, loss: 8.18\n",
            "epoch: 1/1, 41/10727, 2s 38ms/step, lr: 1e-05, loss: 8.15\n",
            "epoch: 1/1, 42/10727, 2s 38ms/step, lr: 1e-05, loss: 8.11\n",
            "epoch: 1/1, 43/10727, 2s 38ms/step, lr: 1e-05, loss: 8.09\n",
            "epoch: 1/1, 44/10727, 2s 38ms/step, lr: 1e-05, loss: 8.06\n",
            "epoch: 1/1, 45/10727, 2s 38ms/step, lr: 1e-05, loss: 8.03\n",
            "epoch: 1/1, 46/10727, 2s 38ms/step, lr: 1e-05, loss: 8.01\n",
            "epoch: 1/1, 47/10727, 2s 38ms/step, lr: 1e-05, loss: 7.99\n",
            "epoch: 1/1, 48/10727, 2s 38ms/step, lr: 1e-05, loss: 7.97\n",
            "epoch: 1/1, 49/10727, 2s 38ms/step, lr: 1e-05, loss: 7.94\n",
            "epoch: 1/1, 50/10727, 2s 38ms/step, lr: 1e-05, loss: 7.92\n",
            "epoch: 1/1, 51/10727, 2s 38ms/step, lr: 1e-05, loss: 7.88\n",
            "epoch: 1/1, 52/10727, 2s 38ms/step, lr: 1e-05, loss: 7.86\n",
            "epoch: 1/1, 53/10727, 2s 38ms/step, lr: 1e-05, loss: 7.83\n",
            "epoch: 1/1, 54/10727, 2s 38ms/step, lr: 1e-05, loss: 7.82\n",
            "epoch: 1/1, 55/10727, 2s 38ms/step, lr: 1e-05, loss: 7.8\n",
            "epoch: 1/1, 56/10727, 2s 39ms/step, lr: 1e-05, loss: 7.77\n",
            "epoch: 1/1, 57/10727, 2s 39ms/step, lr: 1e-05, loss: 7.74\n",
            "epoch: 1/1, 58/10727, 2s 39ms/step, lr: 1e-05, loss: 7.73\n",
            "epoch: 1/1, 59/10727, 2s 39ms/step, lr: 1e-05, loss: 7.73\n",
            "epoch: 1/1, 60/10727, 2s 39ms/step, lr: 1e-05, loss: 7.71\n",
            "epoch: 1/1, 61/10727, 2s 39ms/step, lr: 1e-05, loss: 7.71\n",
            "epoch: 1/1, 62/10727, 2s 39ms/step, lr: 1e-05, loss: 7.69\n",
            "epoch: 1/1, 63/10727, 2s 39ms/step, lr: 1e-05, loss: 7.68\n",
            "epoch: 1/1, 64/10727, 3s 39ms/step, lr: 1e-05, loss: 7.66\n",
            "epoch: 1/1, 65/10727, 3s 39ms/step, lr: 1e-05, loss: 7.64\n",
            "epoch: 1/1, 66/10727, 3s 39ms/step, lr: 1e-05, loss: 7.62\n",
            "epoch: 1/1, 67/10727, 3s 39ms/step, lr: 1e-05, loss: 7.6\n",
            "epoch: 1/1, 68/10727, 3s 39ms/step, lr: 1e-05, loss: 7.59\n",
            "epoch: 1/1, 69/10727, 3s 39ms/step, lr: 1e-05, loss: 7.56\n",
            "epoch: 1/1, 70/10727, 3s 39ms/step, lr: 1e-05, loss: 7.53\n",
            "epoch: 1/1, 71/10727, 3s 39ms/step, lr: 1e-05, loss: 7.52\n",
            "epoch: 1/1, 72/10727, 3s 39ms/step, lr: 1e-05, loss: 7.5\n",
            "epoch: 1/1, 73/10727, 3s 39ms/step, lr: 1e-05, loss: 7.49\n",
            "epoch: 1/1, 74/10727, 3s 39ms/step, lr: 1e-05, loss: 7.47\n",
            "epoch: 1/1, 75/10727, 3s 39ms/step, lr: 1e-05, loss: 7.47\n",
            "epoch: 1/1, 76/10727, 3s 40ms/step, lr: 1e-05, loss: 7.44\n",
            "epoch: 1/1, 77/10727, 3s 40ms/step, lr: 1e-05, loss: 7.43\n",
            "epoch: 1/1, 78/10727, 3s 40ms/step, lr: 1e-05, loss: 7.41\n",
            "epoch: 1/1, 79/10727, 3s 40ms/step, lr: 1e-05, loss: 7.39\n",
            "epoch: 1/1, 80/10727, 3s 40ms/step, lr: 1e-05, loss: 7.37\n",
            "epoch: 1/1, 81/10727, 3s 40ms/step, lr: 1e-05, loss: 7.35\n",
            "epoch: 1/1, 82/10727, 3s 40ms/step, lr: 1e-05, loss: 7.34\n",
            "epoch: 1/1, 83/10727, 3s 40ms/step, lr: 1e-05, loss: 7.32\n",
            "epoch: 1/1, 84/10727, 3s 40ms/step, lr: 1e-05, loss: 7.31\n",
            "epoch: 1/1, 85/10727, 3s 40ms/step, lr: 1e-05, loss: 7.29\n",
            "epoch: 1/1, 86/10727, 3s 40ms/step, lr: 1e-05, loss: 7.26\n",
            "epoch: 1/1, 87/10727, 3s 40ms/step, lr: 1e-05, loss: 7.25\n",
            "epoch: 1/1, 88/10727, 4s 40ms/step, lr: 1e-05, loss: 7.24\n",
            "epoch: 1/1, 89/10727, 4s 40ms/step, lr: 1e-05, loss: 7.23\n",
            "epoch: 1/1, 90/10727, 4s 40ms/step, lr: 1e-05, loss: 7.21\n",
            "epoch: 1/1, 91/10727, 4s 40ms/step, lr: 1e-05, loss: 7.19\n",
            "epoch: 1/1, 92/10727, 4s 40ms/step, lr: 1e-05, loss: 7.17\n",
            "epoch: 1/1, 93/10727, 4s 40ms/step, lr: 1e-05, loss: 7.15\n",
            "epoch: 1/1, 94/10727, 4s 40ms/step, lr: 1e-05, loss: 7.13\n",
            "epoch: 1/1, 95/10727, 4s 40ms/step, lr: 1e-05, loss: 7.11\n",
            "epoch: 1/1, 96/10727, 4s 40ms/step, lr: 1e-05, loss: 7.09\n",
            "epoch: 1/1, 97/10727, 4s 40ms/step, lr: 1e-05, loss: 7.08\n",
            "epoch: 1/1, 98/10727, 4s 40ms/step, lr: 1e-05, loss: 7.06\n",
            "epoch: 1/1, 99/10727, 4s 40ms/step, lr: 1e-05, loss: 7.04\n",
            "epoch: 1/1, 100/10727, 4s 40ms/step, lr: 1e-05, loss: 7.03\n",
            "epoch: 1/1, 101/10727, 4s 40ms/step, lr: 1e-05, loss: 7.02\n",
            "epoch: 1/1, 102/10727, 4s 41ms/step, lr: 1e-05, loss: 7.01\n",
            "epoch: 1/1, 103/10727, 4s 41ms/step, lr: 1e-05, loss: 6.98\n",
            "epoch: 1/1, 104/10727, 4s 41ms/step, lr: 1e-05, loss: 6.97\n",
            "epoch: 1/1, 105/10727, 4s 41ms/step, lr: 1e-05, loss: 6.96\n",
            "epoch: 1/1, 106/10727, 4s 41ms/step, lr: 1e-05, loss: 6.94\n",
            "epoch: 1/1, 107/10727, 4s 41ms/step, lr: 1e-05, loss: 6.92\n",
            "epoch: 1/1, 108/10727, 4s 41ms/step, lr: 1e-05, loss: 6.91\n",
            "epoch: 1/1, 109/10727, 4s 41ms/step, lr: 1e-05, loss: 6.9\n",
            "epoch: 1/1, 110/10727, 4s 41ms/step, lr: 1e-05, loss: 6.89\n",
            "epoch: 1/1, 111/10727, 5s 41ms/step, lr: 1e-05, loss: 6.87\n",
            "epoch: 1/1, 112/10727, 5s 41ms/step, lr: 1e-05, loss: 6.87\n",
            "epoch: 1/1, 113/10727, 5s 41ms/step, lr: 1e-05, loss: 6.86\n",
            "epoch: 1/1, 114/10727, 5s 41ms/step, lr: 1e-05, loss: 6.85\n",
            "epoch: 1/1, 115/10727, 5s 41ms/step, lr: 1e-05, loss: 6.84\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-22ae403b1415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlh3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlh3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_M1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_empty_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_empty_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-ca36e26df586>\u001b[0m in \u001b[0;36mtrain2\u001b[0;34m(M1, M2, tokenizer, epochs, learning_rate, optimizer, loss_history, train_M1, steps_per_update, steps_empty_cache)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msteps_per_update\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msteps_per_update\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             adam(params_with_grad,\n\u001b[0m\u001b[1;32m    235\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    301\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lh3)"
      ],
      "metadata": {
        "id": "FkHcnuG1ogcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f_PQ(M1, M2, tokenizer, passage, question, generation_params=None):\n",
        "    M2.to('cuda')\n",
        "    inputs = tokenizer(\n",
        "            question,\n",
        "            passage,\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to('cuda')\n",
        "\n",
        "    if generation_params is None:\n",
        "        generation_params = {\n",
        "            'do_sample' : False,\n",
        "            'num_beams' : 3,\n",
        "            'repetition_penalty' : 2.\n",
        "        }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        O1 = M1.forward(inputs.input_ids,\n",
        "                        inputs.attention_mask)\n",
        "\n",
        "        generated_ids = M2.generate(inputs.input_ids, token_importances=O1, **generation_params)\n",
        "\n",
        "        print(generated_ids)\n",
        "\n",
        "        generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "        print(generated_text)\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "fc6ZImvdDTTN"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=random.randint(0,len(train))\n",
        "\n",
        "passage = train[i]['story']\n",
        "question = train[i]['questions'][0]['input_text']\n",
        "answer = train[i]['answers'][0]['input_text']\n",
        "\n",
        "print('passage: ',passage)\n",
        "print()\n",
        "print('question: ',question)\n",
        "print()\n",
        "print('Original answer: ',answer)\n",
        "\n",
        "print()\n",
        "print(f_PQ(M1, M2, tokenizer, passage, question))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXtPghESDgHK",
        "outputId": "15247d92-7432-4616-e579-e4adf3dd6133"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passage:  CHAPTER XV \n",
            "\n",
            "There was a deal of cursing and groaning as the men at the bottom of the ladder crawled to their feet. \n",
            "\n",
            "Somebody strike a light, my thumbs out of joint, said one of the men, Parsons, a swarthy, saturnine man, boat-steerer in Standishs boat, in which Harrison was puller. \n",
            "\n",
            "Youll find it knockin about by the bitts, Leach said, sitting down on the edge of the bunk in which I was concealed. \n",
            "\n",
            "There was a fumbling and a scratching of matches, and the sea-lamp flared up, dim and smoky, and in its weird light bare-legged men moved about nursing their bruises and caring for their hurts. Oofty-Oofty laid hold of Parsonss thumb, pulling it out stoutly and snapping it back into place. I noticed at the same time that the Kanakas knuckles were laid open clear across and to the bone. He exhibited them, exposing beautiful white teeth in a grin as he did so, and explaining that the wounds had come from striking Wolf Larsen in the mouth. \n",
            "\n",
            "So it was you, was it, you black beggar? belligerently demanded one Kelly, an Irish-American and a longshoreman, making his first trip to sea, and boat-puller for Kerfoot. \n",
            "\n",
            "As he made the demand he spat out a mouthful of blood and teeth and shoved his pugnacious face close to Oofty-Oofty. The Kanaka leaped backward to his bunk, to return with a second leap, flourishing a long knife. \n",
            "\n",
            "Aw, go lay down, you make me tired, Leach interfered. He was evidently, for all of his youth and inexperience, cock of the forecastle. Gwan, you Kelly. You leave Oofty alone. How in hell did he know it was you in the dark? \n",
            "\n",
            "question:  how was the sea lamp desctibed ?\n",
            "\n",
            "Original answer:  dim and smoky,\n",
            "\n",
            "tensor([[101, 101, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102,\n",
            "         102, 102, 102, 102, 102, 102]], device='cuda:0')\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i=random.randint(0,len(validation))\n",
        "\n",
        "passage = validation[i]['story']\n",
        "question = validation[i]['questions'][0]['input_text']\n",
        "answer = validation[i]['answers'][0]['input_text']\n",
        "\n",
        "print('passage: ',passage)\n",
        "print()\n",
        "print('question: ',question)\n",
        "print()\n",
        "print('Original answer: ',answer)\n",
        "\n",
        "print()\n",
        "print(f_PQ(M1, M2, tokenizer, passage, question))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-mNgato6LH7",
        "outputId": "487ae0c1-cd33-409e-b78e-5b4108ac4a3f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passage:  In order to save money for a trip to Thailand to visit her family and friends, Emma White, a keeper in California, worked overtime. She saved more than 900 dollars and was planning to buy a ticket, but she lost her purse with all her money and credit cards . \n",
            "\n",
            "Emma looked through the garbage and all the buildings where she worked but came up empty-handed. She went home heartbroken. She believed that she had lost her money forever. While Emma was at home, sad and depressed, a homeless man was searching through the garbage looking for things to sell. As he was looking through a garbage bag, he found something wrapped in a plastic bag. \n",
            "\n",
            "The homeless man, who did not want to be recognized, took the purse to Sherry Wesley, because Sherry Wesley knew him from her volunteer work at a homeless shelter. The homeless man came to Sherry Wesley with the wad of money and said, \"This probably belongs to someone that you work with; can you find the owner?\" Sherry Wesley works in one of the buildings that Emma cleans and she knew Emma had lost her purse. \n",
            "\n",
            "Emma was amazed when she heard the good news. \"I couldn't believe it when they called me,\" she said. \"He has a very big heart. If someone else had found the purse, the money would be gone.\" As a reward, Emma gave the man 100 dollars. The homeless man gave half of the money to Sherry Wesley and asked her to donate it to charity for him.\n",
            "\n",
            "question:  How much was rewarded?\n",
            "\n",
            "Original answer:  100 dollars\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}