{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Assignment 2 - Question Answering with Transformers on CoQA</h1>\n",
    "    <h2>Natural Language Processing</h2>\n",
    "    <h3>Antonio Politano, Enrico Pittini, Riccardo Spolaor and Samuele Bortolato</h3>\n",
    "    <h4>antonio.politano2@studio.unibo.it, enrico.pittini@studio.unibo.it, riccardo.spolaor@studio.unibo.it, samuele.bortolato@studio.unibo.it</h4>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Assignment description: see `Assignment.ipynb`.\n",
    "\n",
    "In this notebook the QA task is addressed.\n",
    "\n",
    "For more detailed informations about the used functions, look into the corresponding docstrings inside the python files, inside the `utils` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for autoreloading\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for reproducibility\n",
    "from utils.seeder import set_random_seed\n",
    "\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Task 1] Remove unaswerable QA pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset download\n",
    "\n",
    "The dataset is downloaded and saved in the `coqua` folder using the snippet of code provided in `Assignment.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "def download_url(url, output_path):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "\n",
    "def download_data(data_path, url_path, suffix):    \n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "        \n",
    "    data_path = os.path.join(data_path, f'{suffix}.json')\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Downloading CoQA {suffix} data split... (it may take a while)\")\n",
    "        download_url(url=url_path, output_path=data_path)\n",
    "        print(\"Download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "train_url = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n",
    "download_data(data_path='coqa', url_path=train_url, suffix='train')\n",
    "\n",
    "# Test data\n",
    "test_url = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n",
    "download_data(data_path='coqa', url_path=test_url, suffix='test')  # <-- Why test? See next slides for an answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dataframe Creation\n",
    "\n",
    "The train and test dataframes (`train_df` and `test_df`) are built. Each row contains information about a specific question and the corresponding answer along with their chronological collocation (`turn_id`) in the conversation. Furthermore informations about the passage containing the context and the history of previous questions and answers of the relative conversation is contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataframe_builder import get_dataframe\n",
    "\n",
    "train_df = get_dataframe(os.path.join('coqa', 'train.json'))\n",
    "test_df = get_dataframe(os.path.join('coqa', 'test.json'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heads of the train and test dataframes are shown below along with their shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataframe shape: (108647, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>filename</th>\n",
       "      <th>story</th>\n",
       "      <th>name</th>\n",
       "      <th>question_input_text</th>\n",
       "      <th>question_turn_id</th>\n",
       "      <th>question_bad_turn</th>\n",
       "      <th>answer_span_start</th>\n",
       "      <th>answer_span_end</th>\n",
       "      <th>answer_span_text</th>\n",
       "      <th>answer_input_text</th>\n",
       "      <th>answer_turn_id</th>\n",
       "      <th>answer_bad_turn</th>\n",
       "      <th>history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
       "      <td>Vatican_Library.txt</td>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>Vatican_Library.txt</td>\n",
       "      <td>When was the Vat formally opened?</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>151</td>\n",
       "      <td>179</td>\n",
       "      <td>Formally established in 1475</td>\n",
       "      <td>It was formally established in 1475</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
       "      <td>Vatican_Library.txt</td>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>Vatican_Library.txt</td>\n",
       "      <td>what is the library for?</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>454</td>\n",
       "      <td>494</td>\n",
       "      <td>he Vatican Library is a research library</td>\n",
       "      <td>research</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[When was the Vat formally opened?, It was for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
       "      <td>Vatican_Library.txt</td>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>Vatican_Library.txt</td>\n",
       "      <td>for what subjects?</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>457</td>\n",
       "      <td>511</td>\n",
       "      <td>Vatican Library is a research library for hist...</td>\n",
       "      <td>history, and law</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[When was the Vat formally opened?, It was for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
       "      <td>Vatican_Library.txt</td>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>Vatican_Library.txt</td>\n",
       "      <td>and?</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>457</td>\n",
       "      <td>545</td>\n",
       "      <td>Vatican Library is a research library for hist...</td>\n",
       "      <td>philosophy, science and theology</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[When was the Vat formally opened?, It was for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
       "      <td>Vatican_Library.txt</td>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>Vatican_Library.txt</td>\n",
       "      <td>what was started in 2014?</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>769</td>\n",
       "      <td>879</td>\n",
       "      <td>March 2014, the Vatican Library began an initi...</td>\n",
       "      <td>a  project</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[When was the Vat formally opened?, It was for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source                              id             filename  \\\n",
       "0  wikipedia  3zotghdk5ibi9cex97fepx7jetpso7  Vatican_Library.txt   \n",
       "1  wikipedia  3zotghdk5ibi9cex97fepx7jetpso7  Vatican_Library.txt   \n",
       "2  wikipedia  3zotghdk5ibi9cex97fepx7jetpso7  Vatican_Library.txt   \n",
       "3  wikipedia  3zotghdk5ibi9cex97fepx7jetpso7  Vatican_Library.txt   \n",
       "4  wikipedia  3zotghdk5ibi9cex97fepx7jetpso7  Vatican_Library.txt   \n",
       "\n",
       "                                               story                 name  \\\n",
       "0  The Vatican Apostolic Library (), more commonl...  Vatican_Library.txt   \n",
       "1  The Vatican Apostolic Library (), more commonl...  Vatican_Library.txt   \n",
       "2  The Vatican Apostolic Library (), more commonl...  Vatican_Library.txt   \n",
       "3  The Vatican Apostolic Library (), more commonl...  Vatican_Library.txt   \n",
       "4  The Vatican Apostolic Library (), more commonl...  Vatican_Library.txt   \n",
       "\n",
       "                 question_input_text  question_turn_id question_bad_turn  \\\n",
       "0  When was the Vat formally opened?                 1               NaN   \n",
       "1           what is the library for?                 2               NaN   \n",
       "2                 for what subjects?                 3               NaN   \n",
       "3                               and?                 4               NaN   \n",
       "4          what was started in 2014?                 5               NaN   \n",
       "\n",
       "   answer_span_start  answer_span_end  \\\n",
       "0                151              179   \n",
       "1                454              494   \n",
       "2                457              511   \n",
       "3                457              545   \n",
       "4                769              879   \n",
       "\n",
       "                                    answer_span_text  \\\n",
       "0                       Formally established in 1475   \n",
       "1           he Vatican Library is a research library   \n",
       "2  Vatican Library is a research library for hist...   \n",
       "3  Vatican Library is a research library for hist...   \n",
       "4  March 2014, the Vatican Library began an initi...   \n",
       "\n",
       "                     answer_input_text  answer_turn_id answer_bad_turn  \\\n",
       "0  It was formally established in 1475               1             NaN   \n",
       "1                             research               2             NaN   \n",
       "2                     history, and law               3             NaN   \n",
       "3     philosophy, science and theology               4             NaN   \n",
       "4                           a  project               5             NaN   \n",
       "\n",
       "                                             history  \n",
       "0                                                 []  \n",
       "1  [When was the Vat formally opened?, It was for...  \n",
       "2  [When was the Vat formally opened?, It was for...  \n",
       "3  [When was the Vat formally opened?, It was for...  \n",
       "4  [When was the Vat formally opened?, It was for...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Train dataframe shape: {train_df.shape}')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataframe shape: (7983, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>filename</th>\n",
       "      <th>story</th>\n",
       "      <th>additional_answers</th>\n",
       "      <th>name</th>\n",
       "      <th>question_input_text</th>\n",
       "      <th>question_turn_id</th>\n",
       "      <th>answer_span_start</th>\n",
       "      <th>answer_span_end</th>\n",
       "      <th>answer_span_text</th>\n",
       "      <th>answer_input_text</th>\n",
       "      <th>answer_turn_id</th>\n",
       "      <th>history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mctest</td>\n",
       "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
       "      <td>mc160.test.41</td>\n",
       "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
       "      <td>{'0': [{'span_start': 68, 'span_end': 93, 'spa...</td>\n",
       "      <td>mc160.test.41</td>\n",
       "      <td>What color was Cotton?</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>93</td>\n",
       "      <td>a little white kitten named Cotton</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mctest</td>\n",
       "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
       "      <td>mc160.test.41</td>\n",
       "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
       "      <td>{'0': [{'span_start': 68, 'span_end': 93, 'spa...</td>\n",
       "      <td>mc160.test.41</td>\n",
       "      <td>Where did she live?</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>80</td>\n",
       "      <td>in a barn near a farm house, there lived a lit...</td>\n",
       "      <td>in a barn</td>\n",
       "      <td>2</td>\n",
       "      <td>[What color was Cotton?, white]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mctest</td>\n",
       "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
       "      <td>mc160.test.41</td>\n",
       "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
       "      <td>{'0': [{'span_start': 68, 'span_end': 93, 'spa...</td>\n",
       "      <td>mc160.test.41</td>\n",
       "      <td>Did she live alone?</td>\n",
       "      <td>3</td>\n",
       "      <td>196</td>\n",
       "      <td>215</td>\n",
       "      <td>Cotton wasn't alone</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>[What color was Cotton?, white, Where did she ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mctest</td>\n",
       "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
       "      <td>mc160.test.41</td>\n",
       "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
       "      <td>{'0': [{'span_start': 68, 'span_end': 93, 'spa...</td>\n",
       "      <td>mc160.test.41</td>\n",
       "      <td>Who did she live with?</td>\n",
       "      <td>4</td>\n",
       "      <td>281</td>\n",
       "      <td>315</td>\n",
       "      <td>with her mommy and 5 other sisters</td>\n",
       "      <td>with her mommy and 5 sisters</td>\n",
       "      <td>4</td>\n",
       "      <td>[What color was Cotton?, white, Where did she ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mctest</td>\n",
       "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
       "      <td>mc160.test.41</td>\n",
       "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
       "      <td>{'0': [{'span_start': 68, 'span_end': 93, 'spa...</td>\n",
       "      <td>mc160.test.41</td>\n",
       "      <td>What color were her sisters?</td>\n",
       "      <td>5</td>\n",
       "      <td>428</td>\n",
       "      <td>490</td>\n",
       "      <td>her sisters were all orange with beautiful whi...</td>\n",
       "      <td>orange and white</td>\n",
       "      <td>5</td>\n",
       "      <td>[What color was Cotton?, white, Where did she ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source                              id       filename  \\\n",
       "0  mctest  3dr23u6we5exclen4th8uq9rb42tel  mc160.test.41   \n",
       "1  mctest  3dr23u6we5exclen4th8uq9rb42tel  mc160.test.41   \n",
       "2  mctest  3dr23u6we5exclen4th8uq9rb42tel  mc160.test.41   \n",
       "3  mctest  3dr23u6we5exclen4th8uq9rb42tel  mc160.test.41   \n",
       "4  mctest  3dr23u6we5exclen4th8uq9rb42tel  mc160.test.41   \n",
       "\n",
       "                                               story  \\\n",
       "0  Once upon a time, in a barn near a farm house,...   \n",
       "1  Once upon a time, in a barn near a farm house,...   \n",
       "2  Once upon a time, in a barn near a farm house,...   \n",
       "3  Once upon a time, in a barn near a farm house,...   \n",
       "4  Once upon a time, in a barn near a farm house,...   \n",
       "\n",
       "                                  additional_answers           name  \\\n",
       "0  {'0': [{'span_start': 68, 'span_end': 93, 'spa...  mc160.test.41   \n",
       "1  {'0': [{'span_start': 68, 'span_end': 93, 'spa...  mc160.test.41   \n",
       "2  {'0': [{'span_start': 68, 'span_end': 93, 'spa...  mc160.test.41   \n",
       "3  {'0': [{'span_start': 68, 'span_end': 93, 'spa...  mc160.test.41   \n",
       "4  {'0': [{'span_start': 68, 'span_end': 93, 'spa...  mc160.test.41   \n",
       "\n",
       "            question_input_text  question_turn_id  answer_span_start  \\\n",
       "0        What color was Cotton?                 1                 59   \n",
       "1           Where did she live?                 2                 18   \n",
       "2           Did she live alone?                 3                196   \n",
       "3        Who did she live with?                 4                281   \n",
       "4  What color were her sisters?                 5                428   \n",
       "\n",
       "   answer_span_end                                   answer_span_text  \\\n",
       "0               93                 a little white kitten named Cotton   \n",
       "1               80  in a barn near a farm house, there lived a lit...   \n",
       "2              215                                Cotton wasn't alone   \n",
       "3              315                 with her mommy and 5 other sisters   \n",
       "4              490  her sisters were all orange with beautiful whi...   \n",
       "\n",
       "              answer_input_text  answer_turn_id  \\\n",
       "0                         white               1   \n",
       "1                     in a barn               2   \n",
       "2                            no               3   \n",
       "3  with her mommy and 5 sisters               4   \n",
       "4              orange and white               5   \n",
       "\n",
       "                                             history  \n",
       "0                                                 []  \n",
       "1                    [What color was Cotton?, white]  \n",
       "2  [What color was Cotton?, white, Where did she ...  \n",
       "3  [What color was Cotton?, white, Where did she ...  \n",
       "4  [What color was Cotton?, white, Where did she ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Test dataframe shape: {test_df.shape}')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the training dataframe contains $15$ different features, while the test dataframe has just $14$. In particular the train dataframe includes the additional features `question_bad_turn` and `answer_bad_turn`. With a quick inspection of the dataframe's head it can be observed that they include `NaN` values. Since the task requires to remove solely unanswerable question-answer pairs, not mentioning the handling of \"bad turn\", these two features are dropped.\n",
    "\n",
    "On the other hand the test dataframe contains the extra feature `additional_answers`, which can be removed as expressed in the specifications of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-matching features\n",
    "\n",
    "train_df.drop(['question_bad_turn', 'answer_bad_turn'], axis=1, inplace=True)\n",
    "test_df.drop('additional_answers', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition the features `source`, `name` and `filename` are removed since they are considered useless for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless columns (`source`, `name`, `filename`)\n",
    "\n",
    "train_df.drop(['source', 'name', 'filename'], axis=1, inplace=True)\n",
    "test_df.drop(['source', 'name', 'filename'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, by inspecting the `question_turn_id` and `answer_turn_id` it can be noticed that they are equivalent, since they refer to the same question-answer pair, hence they can be merged in a single feature (`turn_id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that the turn ids of the questions are the same as the respective answers\n",
    "\n",
    "assert train_df['question_turn_id'].equals(train_df['answer_turn_id']), \\\n",
    "    'Question and answer turn ids are different in the train dataset'\n",
    "    \n",
    "assert test_df['question_turn_id'].equals(test_df['answer_turn_id']), \\\n",
    "    'Question and answer turn ids are different in the test dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns `question_turn_id` and `answer_turn_id` into a singular `turn_id` column since they are equal\n",
    "refactor_turn_id_columns = lambda df: \\\n",
    "    df.drop('question_turn_id', axis=1).rename(columns = {'answer_turn_id': 'turn_id'})\n",
    "    \n",
    "train_df = refactor_turn_id_columns(train_df)\n",
    "test_df = refactor_turn_id_columns(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the columns `answer_input_text` and `question_input_text` are renamed into `answer` and `question` respectively for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns `answer_input_text` and `question_input_text` into `answer` and `question` respectively\n",
    "column_renames = {'answer_input_text': 'answer', 'question_input_text': 'question'}\n",
    "\n",
    "train_df.rename(columns=column_renames, inplace=True)\n",
    "test_df.rename(columns=column_renames, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapes of the dataframes now match on the column number and no Null values are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataframe shape after the unwanted columns drop: (108647, 9)\n",
      "Test dataframe shape after the unwanted column drop: (7983, 9)\n"
     ]
    }
   ],
   "source": [
    "print(f'Train dataframe shape after the unwanted columns drop: {train_df.shape}')\n",
    "print(f'Test dataframe shape after the unwanted column drop: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in the train dataframe: 0.\n",
      "Null values in the test dataframe: 0.\n"
     ]
    }
   ],
   "source": [
    "print(f'Null values in the train dataframe: {train_df.isna().sum().sum()}.')\n",
    "print(f'Null values in the test dataframe: {test_df.isna().sum().sum()}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>story</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_span_start</th>\n",
       "      <th>answer_span_end</th>\n",
       "      <th>answer_span_text</th>\n",
       "      <th>answer</th>\n",
       "      <th>turn_id</th>\n",
       "      <th>history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>When was the Vat formally opened?</td>\n",
       "      <td>151</td>\n",
       "      <td>179</td>\n",
       "      <td>Formally established in 1475</td>\n",
       "      <td>It was formally established in 1475</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>what is the library for?</td>\n",
       "      <td>454</td>\n",
       "      <td>494</td>\n",
       "      <td>he Vatican Library is a research library</td>\n",
       "      <td>research</td>\n",
       "      <td>2</td>\n",
       "      <td>[When was the Vat formally opened?, It was for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>for what subjects?</td>\n",
       "      <td>457</td>\n",
       "      <td>511</td>\n",
       "      <td>Vatican Library is a research library for hist...</td>\n",
       "      <td>history, and law</td>\n",
       "      <td>3</td>\n",
       "      <td>[When was the Vat formally opened?, It was for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>and?</td>\n",
       "      <td>457</td>\n",
       "      <td>545</td>\n",
       "      <td>Vatican Library is a research library for hist...</td>\n",
       "      <td>philosophy, science and theology</td>\n",
       "      <td>4</td>\n",
       "      <td>[When was the Vat formally opened?, It was for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>what was started in 2014?</td>\n",
       "      <td>769</td>\n",
       "      <td>879</td>\n",
       "      <td>March 2014, the Vatican Library began an initi...</td>\n",
       "      <td>a  project</td>\n",
       "      <td>5</td>\n",
       "      <td>[When was the Vat formally opened?, It was for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id  \\\n",
       "0  3zotghdk5ibi9cex97fepx7jetpso7   \n",
       "1  3zotghdk5ibi9cex97fepx7jetpso7   \n",
       "2  3zotghdk5ibi9cex97fepx7jetpso7   \n",
       "3  3zotghdk5ibi9cex97fepx7jetpso7   \n",
       "4  3zotghdk5ibi9cex97fepx7jetpso7   \n",
       "\n",
       "                                               story  \\\n",
       "0  The Vatican Apostolic Library (), more commonl...   \n",
       "1  The Vatican Apostolic Library (), more commonl...   \n",
       "2  The Vatican Apostolic Library (), more commonl...   \n",
       "3  The Vatican Apostolic Library (), more commonl...   \n",
       "4  The Vatican Apostolic Library (), more commonl...   \n",
       "\n",
       "                            question  answer_span_start  answer_span_end  \\\n",
       "0  When was the Vat formally opened?                151              179   \n",
       "1           what is the library for?                454              494   \n",
       "2                 for what subjects?                457              511   \n",
       "3                               and?                457              545   \n",
       "4          what was started in 2014?                769              879   \n",
       "\n",
       "                                    answer_span_text  \\\n",
       "0                       Formally established in 1475   \n",
       "1           he Vatican Library is a research library   \n",
       "2  Vatican Library is a research library for hist...   \n",
       "3  Vatican Library is a research library for hist...   \n",
       "4  March 2014, the Vatican Library began an initi...   \n",
       "\n",
       "                                answer  turn_id  \\\n",
       "0  It was formally established in 1475        1   \n",
       "1                             research        2   \n",
       "2                     history, and law        3   \n",
       "3     philosophy, science and theology        4   \n",
       "4                           a  project        5   \n",
       "\n",
       "                                             history  \n",
       "0                                                 []  \n",
       "1  [When was the Vat formally opened?, It was for...  \n",
       "2  [When was the Vat formally opened?, It was for...  \n",
       "3  [When was the Vat formally opened?, It was for...  \n",
       "4  [When was the Vat formally opened?, It was for...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>story</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_span_start</th>\n",
       "      <th>answer_span_end</th>\n",
       "      <th>answer_span_text</th>\n",
       "      <th>answer</th>\n",
       "      <th>turn_id</th>\n",
       "      <th>history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
       "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
       "      <td>What color was Cotton?</td>\n",
       "      <td>59</td>\n",
       "      <td>93</td>\n",
       "      <td>a little white kitten named Cotton</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
       "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
       "      <td>Where did she live?</td>\n",
       "      <td>18</td>\n",
       "      <td>80</td>\n",
       "      <td>in a barn near a farm house, there lived a lit...</td>\n",
       "      <td>in a barn</td>\n",
       "      <td>2</td>\n",
       "      <td>[What color was Cotton?, white]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
       "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
       "      <td>Did she live alone?</td>\n",
       "      <td>196</td>\n",
       "      <td>215</td>\n",
       "      <td>Cotton wasn't alone</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>[What color was Cotton?, white, Where did she ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
       "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
       "      <td>Who did she live with?</td>\n",
       "      <td>281</td>\n",
       "      <td>315</td>\n",
       "      <td>with her mommy and 5 other sisters</td>\n",
       "      <td>with her mommy and 5 sisters</td>\n",
       "      <td>4</td>\n",
       "      <td>[What color was Cotton?, white, Where did she ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
       "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
       "      <td>What color were her sisters?</td>\n",
       "      <td>428</td>\n",
       "      <td>490</td>\n",
       "      <td>her sisters were all orange with beautiful whi...</td>\n",
       "      <td>orange and white</td>\n",
       "      <td>5</td>\n",
       "      <td>[What color was Cotton?, white, Where did she ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id  \\\n",
       "0  3dr23u6we5exclen4th8uq9rb42tel   \n",
       "1  3dr23u6we5exclen4th8uq9rb42tel   \n",
       "2  3dr23u6we5exclen4th8uq9rb42tel   \n",
       "3  3dr23u6we5exclen4th8uq9rb42tel   \n",
       "4  3dr23u6we5exclen4th8uq9rb42tel   \n",
       "\n",
       "                                               story  \\\n",
       "0  Once upon a time, in a barn near a farm house,...   \n",
       "1  Once upon a time, in a barn near a farm house,...   \n",
       "2  Once upon a time, in a barn near a farm house,...   \n",
       "3  Once upon a time, in a barn near a farm house,...   \n",
       "4  Once upon a time, in a barn near a farm house,...   \n",
       "\n",
       "                       question  answer_span_start  answer_span_end  \\\n",
       "0        What color was Cotton?                 59               93   \n",
       "1           Where did she live?                 18               80   \n",
       "2           Did she live alone?                196              215   \n",
       "3        Who did she live with?                281              315   \n",
       "4  What color were her sisters?                428              490   \n",
       "\n",
       "                                    answer_span_text  \\\n",
       "0                 a little white kitten named Cotton   \n",
       "1  in a barn near a farm house, there lived a lit...   \n",
       "2                                Cotton wasn't alone   \n",
       "3                 with her mommy and 5 other sisters   \n",
       "4  her sisters were all orange with beautiful whi...   \n",
       "\n",
       "                         answer  turn_id  \\\n",
       "0                         white        1   \n",
       "1                     in a barn        2   \n",
       "2                            no        3   \n",
       "3  with her mommy and 5 sisters        4   \n",
       "4              orange and white        5   \n",
       "\n",
       "                                             history  \n",
       "0                                                 []  \n",
       "1                    [What color was Cotton?, white]  \n",
       "2  [What color was Cotton?, white, Where did she ...  \n",
       "3  [What color was Cotton?, white, Where did she ...  \n",
       "4  [What color was Cotton?, white, Where did she ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Remove Unanswerable Question-Answer Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As required by the task, the unanswerable question-answer pairs are removed from the dataset by dropping the rows of the dataframes where the feature `answer` is equal to \"unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows with unknown answer\n",
    "\n",
    "train_df.drop(train_df[train_df['answer'] == 'unknown'].index, inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test_df.drop(test_df[test_df['answer'] == 'unknown'].index, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataframe shape after the unanswerable question-answer pairs are removed: (107276, 9)\n",
      "Test dataframe shape after the unanswerable question-answer pairs are removed: (7917, 9)\n"
     ]
    }
   ],
   "source": [
    "print(f'Train dataframe shape after the unanswerable question-answer pairs are removed: {train_df.shape}')\n",
    "print(f'Test dataframe shape after the unanswerable question-answer pairs are removed: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section some interesting analyses on the training set are carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group dataframe by `id`\n",
    "grouped_train_df = train_df.groupby(by=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train passages: 7193\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of train passages: {len(grouped_train_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.dataset_analisys import *\n",
    "plot_converstion_length_distribution(grouped_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5AAAAG5CAYAAAD4cNvzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABzCUlEQVR4nO3deZwcdZ3/8ddn7kkyuZNJSAIBEo6AIhABETQiInihux6w3rKi67Wuqy7+1mVZ1tv1Fg9WXQQVRLyiICjCyBlIuAkJ5CR3MjOZ++7u7++Pqp6p6amu7pnpmZ6Zej8fj0m6q6q/9a1vV1fVp75HmXMOERERERERkVxKip0BERERERERmRwUQIqIiIiIiEheFECKiIiIiIhIXhRAioiIiIiISF4UQIqIiIiIiEheFECKiIiIiIhIXmIdQJrZD8zsPwqU1pFm1m5mpf77OjP7x0Kk7af3JzN7d6HSG8Z6P2dmDWZ2IM/lrzKzn411vqYqM3NmtqII611jZnvGe73FYGbvMbP7ip2PNDM718yeLVBay/19qKwQ6cnUNNrjtJltNLM1hctR1vXsNLPz/df/z8x+VMC0283sGP/1dWb2uQKmXbBri2Gu95/M7KC/bfPyWH5CHQsnm+D+Oc7rjc1xfqJdm2Re6xcgvaJc8xXClA0g/R92l5m1mVmzmT1gZh80s/5tds590Dn333mmFXmQcM7tcs7NcM4lC5D3ISd359xFzrmfjjbtYebjSOBfgVXOuUUh8yfUD1vyN5kPWsMx1ifaQtwwcc7d65w7vlB5ytd4/X51nBhgZv9gZhv8C5D9/o3Bc4qdryhhwZVz7iTnXN145sM59wXnXM6bsvnevPXP19tHm6+wICzfa4tCMrNy4OvABf62NWbMj03QMdUUK1AthrG8NinEDZNCXusPx3j9foeznikbQPpe75yrAY4CvgT8G/DjQq9kCh+QjwQanXOHip0RmXrMM6WPQXHYRsmPmX0C+CbwBaAW7/j6PeDiImYrdqbw+boWqAI2FjsjMjUVqtZtIovDNhaMc25K/gE7gfMzpp0BpICT/ffXAZ/zX88H/gg0A4eBe/EC7Bv8z3QB7cCngeWAAy4DdgH3BKaV+enVAV8EHgZagd8Dc/15a4A9YfkFLgR6gT5/fU8E0vtH/3UJ8FngeeAQcD0wy5+Xzse7/bw1AP8eUU6z/M/X++l91k//fH+bU34+rsv43PSM+e3AEcBVwM1+mm14J7PVgc8dAfzaX98O4GMRebsO+AHwFz+tvwFHBeZ/C9jtl+8jwLkZ3/UGf95B4Ov+9CrgZ0Cj/12vB2r9ee8FNvnr2g58ICM/nwb2A/uAf/TLeYU/rxL4H7/MD/r5ro7at7Jsc75prgH24NUQH/Lz9d5AOvOAP/jbvx74HHCfP+8efz0d/vf2tlzpheQzV1ldDDzur38bcGFgP/48cD/e/rMCONvPY4v//9mBdN7jp9/m7y9v96ev8PeHFrx9/JdZ8rnL39b0PvoSP837/LJt8tO9KOM38WO/DPb6ZVcaknbUbzVzG7OWFxnHA7xjwSeBJ/3t+yVQlWX7Sv3taPDT/TCDj0Oh6yX77/cM4EG8fXU/8F2gwv+MAd/w949W4CkGjqWh+2q29RT7/DDef/4+1Q68Jcfx7nM59otP+ftFh7+P1gJ/8r/fO4E5YZ8NfP58//VVwM8C834FHPD3t3uAk/zpl/v7d6+f/z8E0/L3mS78c5s/71R/fyz337/P3webgDsIHMNDyuCdeOehRuDfs+WZLMdxvN9dEuj28/tdf3mH99vYAuwITFsRKPvQcw0Z5/bAb/wfgRP9dSX99TVn+S7fD2zFO/6vJfAb8NP+oJ+3ZuAawLKUTyXeTYh9/t83/WnH4e0T6WPdXSGfHbNjYeD7uQXveNUGPAqcEph/Bd65oA14BnhTYF7o8ZzoY85rgcf86buBqzLy8y4G9qX/YPC+VBLITyPeNUv6+izrNULINueb5nIirsvwjpU/9b+DTXjXGnv8eVHXoPle5+Uqq3OAB/zt3Q28J7Affx+4DW//Oh9vn6/zl90IvCGQzmv877YNb3/5pD89r2sgRnBtQsR1UkbaUb/VzG3MWl6EX+v/N975vg34MzA/4rv4FAPXke9j8HEoar1hv99jgbvw9rcG4OfA7MBn/s3/HtqAZ4FX5rGvDllP1m3JNmOy/xESQAYK558CO046gPyiv+OV+3/n4h/EM9MK7EDX410gVWfZqfYCJ/vL/JqBk98ahnFyD6SXDiDfh3cyOgaYAfwGuCEjb//r5+sUoAc4MUs5XY8X3Nb4n30OuCxbPjM+G7YdV+H9SF+Dd3H7RWBdYKd9BLgSqPDzvx14dZb0r/N3/JfhHSS+hR8E+fPfgRcoleEdXA7gX2jjXQS/0389AzjLf/0BvMBqmp+/04GZgR/vsXgnrZcDncBp/rwL/fRP8j/7Mwb/8L+Bd2Ew1y/LPwBfzLVvhWxzvmmuARLA1X6ar/Hzm76AvMn/mwaswjsY3Re2nnzSC8lnVFmdgXch8Cr/O18CnBDYj3f55ViGd9HXhHfhWAZc6r+fh/e7aQWO9z+7mIEL2xvxLjBL8E7452TJ53KGXvy9B++i+P3+PvBPeAfz9O/9t8AP/fUvxLsJ9IEs6V9F+G81uI3lOcprDUMDhYfxLs7n4l1QfDDL+j8IbAaW+cvezeDjUN7r9aedDpzl53u5v+6P+/Nejff7ne2ndyKwOM99NetxJA5/eMePRHA/DFnmOnIHkOvwfjNL8C6mHsUL2KrwLiT+M+K73Un2APJ9/veWDlAez5avkLTuAt4fmPdV4Af+64vxzlUn+vvUZ4EHsmz/KrwLlvTx/ut+mYUFkFHH8Tr8c2UgbYcXHM5l4CZc8Fh7HVnONUQEkP7r9xA4tmaWGXAe3sXdaX7a3wHuycjbH/F+V0fi3Vy9MEsZXe3vAwuBBXgX/f+dLZ8Znw3bjvdQ2GNhH/BmvGPeJ/EC0vSNhLfgHdNK8AKDDgaOH6HHc6KPOWuAF/ifeSFe8PDGjH3pHLxrjf/x85bel/7ZL8el/nfyQ+DGXPtWyDbvzDPNdNmHXpfhtZD7GzDH//yTDP3th12D5nudF1VWR+Ht+5f639s84EWB/bgFeKn/2Rq83/P/88v1PP+z6XP0fvwb+f62pM81I7oGCuQ96lon67knJO33EP5bDW5jVY7ySpd98Fp/G95NnGr//ZcizgMHGYgLfsHg41De6/WnrcC7zqrEOx7cA3zTn3c83nXfEYHPHzuMfTXruap//bkWmKx/ZA8g1+HfqWHwQf5qvEBqRa60AgV8TMi04E71pcD8VXh3cUsZfQD5V+BDgXnH4x0c0xd9DlgamP8wcEnIdpX6eVoVmPYBoC6wM48kgLwzY7u7/NdnArsylv8M8H9Z0r8OuCnwfgbe3aNlWZZvwr/j6f+Q/ouMO0F4F0oPAC/MYx/6HfDP/uufEDgo4f1wnf+/4Z0Mjw3MfwkDd7qz7lsh68w3zTV4dySDB5NDeBf/pf7+cHxgXn8NZHA9Gd9laHp5/t6CZfVD4BtZlqsDrg68fyfwcMYyD+Id6Kfj3bH8ezLuKOLd+LiWwH6eZX3LCb9o2hp4P81fZhHexXlPcH14J9a7s6R/FeG/1atz5CtYXmsYerHwjsD7r+BfkIekcxeB4BK4IHN7811vluU/DvzWf30e3g2mswjcPc5zX417APl24ECOZa4jdwD59sD7XwPfD7z/KPC7bGVOjnNMYLnZ/j40KyxfIWn9I36Nl78v7AZe5r//E/4NSf99Cd7F31Eh672Swcf76Xjnp7AAMutxnOwB5Hkh04IBZOi5htEHkD8GvpKRdh+wPJCPcwLzbwauyPLdbANeE3j/amCn/3pIPjM+G7Yd76Gwx8J1Gd91f0ARsvzjwMX+69DjOVmOOVnS+yb+ecffl27M2K7gvrQJvzbGf7+YgWuo4Vwj7MwzzXTZh16XkXEjHe83lU8AmfM6L4+y+gz+MT5kueuA6wPvz8W7kR48/t+IX1OGd+P0A2QE3IzgGijwfg3Zr3Uizz0hab+H8N/q9TnyFCyvdNkHr/U/G1j2Q8DtWdL5CYPjguMytzff9WZZ/o3AY/7rFX45nY9/EyewXD77as4AMo59c5bgVaFn+irenZU/m9l2M7sij7R2D2P+83h3T+bnlctoR/jpBdNO1+akBUdN7cQ7aWWa7+cpM60lo8xf5rqr/H4nRwFH+IMaNZtZM96drNqQNNL6y9A514733R0BYGafNLNNZtbipzWLgfK9DO/HudnM1pvZ6/zpN+A1o7rJzPaZ2Vf8wQcws4vMbJ2ZHfbTe00gvSMY/H0GXy/AO0E9Etiu2/3pMLJ9K1ea4PVPTQTep7/nBXj7Q7b8ZpMtvSFylNUyvAudbIJ5ydyX8d8vcc514N2p/iCw38xuNbMT/GU+jXfyeNi8ESHfl3vzBunfR51znf7LGXj7aLm/vnS5/xDv7vtwDCrvHOUVmT8ivgeG7peDynK46zWz48zsj2Z2wMxa8frrzQdwzt2F16T1GuCQmV1rZjPJb1+Nu0ZgfgH63x0MvO4KeZ9tP8nKzErN7Etmts3/znf6s/I9V/0aeImZLcarwUvhNVED7/f0rcB+cRjvdxt2jhm0L/u//8aQ5SDiOB4h7/N15rlmlAYd4/y0GxlcBsP5vWeer0ebx0IeC4NlmMJrepg+X7/LzB4PpHUyA/tY6PE84piDmZ1pZnebWb2ZteCdJ0LP1/52Bfelo4DfBvKyCe+GQS0j27dypZmW7XuOur6Iktd+k6Oshnu+3u1/t2nBa8a/xzvHPG9mfzOzl/jTR3INFBR1rVOIc0/m+TqqvMIU6nw9rPWaWa2Z3WRme/1j988YOF9vxbsBfBXeb+cmM0sfK/LZV3OKVQBpZi/G29GHjMLknGtzzv2rc+4Y4A3AJ8zslenZWZLMNj1tWeD1kXgRfgPeHZNpgXyVMniHz5XuPrwdIJh2gsEXE/lo8POUmdbePD+fK5+ZduPdGZod+Ktxzr0m4jP9ZWhmM/CaKewzs3PxTjpvxWvKMBuvGYIBOOe2OOcuxTvZfRm4xcymO+f6nHP/5Zxbhdf37nXAu8ysEu9C6H/w+jvMxmsTb/7q9+NV9w/JF145duE1r0xv1yzn3Aw/L1H7VjaRaeZQj7c/ZMvvqORRVrvxmk1mE9xvMvdlCOyDzrk7nHOvwrtDthmvyQ7OuQPOufc7547Au+P5PQsfuW0k+2gPXs11utxnOudOymNbQqfnUV6jsZ+hx5l81xuW9+/jlfNK59xMvBs8/fl0zn3bOXc6XsuC4/D6c+TaV4f7HUxFD+LtV2+MWGbQeQGvFmikcp1jgv4Br6np+Xg34ZanP+b/H/n9Oeea8Pr9vM1P6ybn39bG+z19IOOYX+2ceyAkqUH7splNw2tOF7bO0ON4jvzmfb4OnmvwyhKyfzfDOl+b2XS87cr3PJs1Lbzf+748PzvWx0IYXIYleOegfWZ2FN6x+yPAPP9Y9DQD5+usx/Msxxzwmv+txWuRNAuviWTo+drMqhm8L+3G6+sZ3C+rnHN7c+xbucorNM08Pht1fQGjP4ZGldVwz9fLbPDAcMHz9Xrn3MV4112/w6tNH+k1UD6Ge52U77EhqrxGI+v5Oo/1huX9C/70F/jn63cw+Hz9C+fcOXjHDId3LQzR+2re+1osAkgzm+nXQN2E1wTmqZBlXmdmK8zM8AKRJN6dVPACs2NGsOp3mNkq/0R4NXCL84b+fQ6vVu61/p2tz+K1Q047CCy37KM33gj8i5kd7Z/ovoDX6TyRZflQfl5uBj5vZjX+Qf4TeHcx8nEQmGdms/Jc/mGgzcz+zcyq/TvfJ/uBfTavMbNzzKwCr6PyOufcbry27gm8YKnMzK4EZqY/ZGbvMLMF/p2yZn9yysxeYWYv8C+oWvEC6BRee/5KP72EmV2E1xww7WbgvWZ2ov999j/jy1/H/wLfMLOF/vqXmNmr/ddR+1aoXGnm+GwSr1/sVWY2zbxau8wT4Ej3achdVj/GK6tXmlmJn+8TwhLCC2iOM+/xBmVm9ja8C4U/+nfXLvYvuHrw+rSkAMzsLWaWPuE24R30wsq03p+e17Y65/bjXQx/zT9ulJjZsWb28iwfyfVbhdzlNRo3Ax8zs6VmNgevY3y+6w37/dbg/S7a/e/sn9IzzOzF/h3ScryL6m4glce+OtzjxJTjnGvBa1Z3jZm90f9dlptXQ/wVf7HH8Y53c81sEd7d45HKdY4JqsH7fTXiBUlfyJifz7HiF3jHmDf7r9N+AHzGzE4CMLNZZvaWLGncArwucLy/mizXKBHH8XzzGyb0XOOcq8e7QH6Hf856H4MvuA8CS/3PhbkR73j4Iv+mzheAh5xzO0eQxxuBz5rZAjObj7dP5Xu+HutjIcDpZvZ35tW0fxxvv1qH1xzZ+XnAzN6LVwOJ/z70eJ7tmOMvVwMcds51m9kZeDcv0m4BXm9mZ/vfy1UMDgB+gHfdc5S//gVmdrH/OmrfipI1zTzcjPc7mWNmS/AC7aDRnK8huqx+DpxvZm/1z8HzzOxFWdJ5CK+G7dP+8WsN8Hq82toKM3u7mc1yzvXhlV36fD2ca6C8t3UE10m5fqtpUeU1GjcD77GBuOA/h7HesN9vDd51UYu/36RvrmBmx5vZef4xp5uBwewgel/N+zgx1QPIP5hZG160/e94nfLfm2XZlXij2LXj3S3+nnPubn/eF/EO2s1m9slhrP8GvPbVB/A65n4M+i8mPgT8CO/E1IHX1CPtV/7/jWb2aEi6P/HTvgevk3o3Xv+Xkfiov/7teDWzv/DTz8k5txnvhLbdL5vIpjR+YPM64EV+vhvwyiDqwvIXeD+yw3id2d/hT78Dr6nCc3jNALoZ3DTgQmCjmbXjDYhwiXOuC+/O8S14B7dNeB3Xb3DOteF9PzfjncD+Ae9OUDrvfwK+jTdIyVa8kyJ4J0jwRrvaCqwzrynBnXh9UyF634oSlWYuH8Er1wN4+8qNgbyCd0L9qf+9vTXPNAHvbiLRZfUw3u/sG3gni78xtJYxvWwj3j7xr3gXsJ8GXueca8A7Pn0C767nYbxBYNIBzYuBh/zvdy1ev74hz3RzXtOlzwP3+9t6Vh6b+C684OsZf/tuwasBDZPrt5qzvEbpf/F+C0/gDajym3zXm+X3+0l/uTY/7V8G1jXTn9bEwOiGX/XnZd1Xh3ucmKqcc1/D258/i3eS3o33O/2dv8gNeN/jTrwL918OSST/deU6xwRdj/d97sXb59dlzP8xsMr/7n5HuLV4x7kDzrknAvn4Ld5d75v8/eJp4KIsed6IN1LqL/Du1DdF5Dn0OO7P+xbwZjNrMrNvZ/l8mGznGvAGmfkU3j5/El4fubS78EajPGBmDSHbdSfeDcdf+9t1LHDJMPIV9Dm80cWfxBuR9FF/Wk7jcCwEr5/b2xgYGO3v/Bq9Z4Cv4Z3/DuINFHJ/4HPZjudRx5wPAVf713hX4td2+du6Ee/a5ia8Mm/H6w+WPgd+y1/Pn/3Pr8MbowGi960oUWnmcjXevr4D79h5C4PP1yO9Bk2LKqtdeM1O/xVv338cb1CeIZxzvXgB40V412/fA97lH+PB+853+r/1D+L1/YbhXQNdxfCuTYZznRT5Ww3IWl6j4V9HftPPx1b//7zWm+X3+194g3O1ALcSOP/j3TD8Et73dACvVvgz/rys++pwjhPpkbZEJhwzuw6vI/lni52XTGZ2It7FUOVwa36Lwcy+DCxyzr272HkREZGpxcyuwhsM5B25lh1v5rXUasZrmr+jyNnJycz+Ce+md1Rtr0hRTfUaSJGCMbM3mVmleU0Fv4z3TLQJGTya2Qlm9kLznIE3qNBvi50vERGRsWZmrzevqfh0vH7gTzEwQNSEYmaLzeylfjPh4/FqA3W+lglNAaRI/j6A1wxmG14b/n+KXryoavCaM3TgNYX7Gl7zIhERkanuYrzuD/vwmlBe4iZuk7sKvBFu2/CaNf4er3moyISlJqwiIiIiIiKSF9VAioiIiIiISF5G+1DjcTV//ny3fPnyYmejaDo6Opg+fXqxszFhqXyiqXyiqXyijXf5PPLIIw3OueE+EDq2dH7U7zeKyieayieayie3uJ0jJ1UAuXz5cjZs2FDsbBRNXV0da9asKXY2JiyVTzSVTzSVT7TxLh8ze37cVjYF6Pyo328UlU80lU80lU9ucTtHqgmriIiIiIiI5EUBpIiIiIiIiORFAaSIiIiIiIjkRQGkiIiIiIiI5EUBpIiIiIiIiORFAaSIiIiIiIjkRQGkiIiIiIiI5EUBpIiIiIiIiORFAaSIiIiIiIjkRQGkiIiIiIiI5EUBpIiIiIiIiORFAaSIiIiIiIjkRQGkiIjIODGzn5jZITN7Ost8M7Nvm9lWM3vSzE4b7zyKiIhEUQApIiIyfq4DLoyYfxGw0v+7HPj+OORJREQkb2XFzoCMv46eBJVlJZSV6v6BiMh4cs7dY2bLIxa5GLjeOeeAdWY228wWO+f2j08ORUSGxzlHbzKFc2HzsnyG8BnZl49e/3A+k20d2T6QT17bex1NHb051uvY2djJaUfOxsyyLDU5KICMoZP+8w5eecJCfvyeFxc7KyIiMtgSYHfg/R5/mgJIkUnKOUci5ehNpOhNpOjsS9LVm6S7z/vr6kvS3p1g1+FOUg5SzuGc63+dcvjv06+998mUo7MvSSrlvGmk/6f/Pf3vvTAomXI0tvfSl0zRl3IkkikSSUdfMkVXX5JkyvUvn15vb18fJXV3QCA/KTewXDLlTYu9u/6S12J/+ZeXsbK2ZowzM7YUQMbUXzcfKnYWRERkFMzscrxmrtTW1lJXV1fcDBVRe3t7rLc/F5VPtGD5dCUc7b2Ojj7H4W5H0kHSQcp5wVfSQU8SL/hKQV8SupOOzoQjmYLuBLT0OroTjq6EozsBfanoGrR8GGA28H+J/39FKZT6tVnpeenXBD4TfF1TYZSXQGmJUWZQVQKlZVBZaYPSTqeX6HNUVKTfm/9/CQaU+MtVlHqvs+V9ODMs+yeyyvaJ4Vb0ZU0nxwd6enqorKzMufysSmPbUxvYu0k1kCIiIlIYe4FlgfdL/WlDOOeuBa4FWL16tVuzZs2YZ26iqqurI87bn8tUK5++ZIrOniQtXX00dvT015wlUo5k0vs/kUrR1p3gcEcvPYkUiWSKhvYeuvtSXu1bMkVPIkVXb5IDjSW0p3rp6EnQlxxeqFdealSXlzJrWgWVZaVUVZZw7PwqpleWUVNVxozKMqrKSigvLaGizPubVlFKVbn3V11eyrSKUirKSlg2ZxrVFaVeEGfm/1H05o5Tbf8ZC3ErIwWQIiIiE8da4CNmdhNwJtCi/o8y2Tnn6Emk6OxN0tGToCvddLM3SWdfsr9pZ3dfkvr2HvY1d7G/uZvmrj76kt68RMrR3ZekqaOXjt7ksPNQVmLMmV7BjMoyykutP6CrLi9lwbQS1hyzmJqqcmZWlTNvegXVFaUcNW8aFWUllJUYZSUlXo2dHzBWlZdSUVpCSbZqN5EpTAGkiIjIODGzG4E1wHwz2wP8J1AO4Jz7AXAb8BpgK9AJvLc4ORWJlko56tt72NPUxd7mLvY2ddHc2UtHb4L6th4SSce+lm72NXfR3pMgOYxOcrOnlbN4VjVzppVTU1XmBXt+wDd3egWzq8v7a/jmzaiguryMslKjtMQo9wO90hJjZnUZc6ZVUFlWElmL59UevaAQxSISCwogRURExolz7tIc8x3w4XHKjsggzjmaOvvY19zFwdZuGjt6qW/roSeRoqcvSWNHL/tbvNrBPU1d9CZTgz5fVV7CtIoy5k2voLK8hDnTynnx8jnMqCxjemUZ0ytKmVZZ5jXhLPObcFaUUllWQlV5CRWlpcyvqWBahS5PRSYy/UJFREREYqS7L8mD2xvZdqidnY0dPLOvlcaOXg62dtPdlwr9TEVZCfOmV7BoVhUnLK7h/FW1LJtTzZI51SydM40jZlczo1KXlSJxoF+6iIiIyBSWSDluengXT+5tYWdDB5v2t9LU2QdATWUZq46YyQuXzqa2ppIjZldzxOxqamdWMr2yjCPnTsvZBFRE4iWvANLMLgS+BZQCP3LOfSljfiVwPXA60Ai8zTm308xeBXwJqAB6gU855+7yP3M6cB1Qjdfn459dtieBioiIiEjeDrV188DWRp7Z38qvH+6isfspZk8rZ/m86aw5fiEXnbyIM46ey6zqcgWHIjIsOQNIMysFrgFehfdA4/VmttY590xgscuAJufcCjO7BPgy8DagAXi9c26fmZ0M3IH3QGSA7wPvBx7CCyAvBP5UmM0SERERiZdkynHX5kPc+PAu7tvaQG8iRXmpcfRM40tvPZ1XraotdhZFZArIpwbyDGCrc247gD+0+MVAMIC8GLjKf30L8F0zM+fcY4FlNgLVfm3lXGCmc26dn+b1wBtRACkiIiIyLDsbOlj7xD5+uX43e5u7WDSzirefeSQXv2gJqxbP5IH77mGNgkcRKZB8AsglwO7A+z14z6YKXcY5lzCzFmAeXg1k2t8DjzrnesxsiZ9OMM0lhDCzy4HLAWpra6mrq8sjy1NTe3t7Qbd/qpVloctnqlH5RFP5RFP5iExM2+vbef137qOjN8lLV8zjP163ivNPXEhZaUmxsyYiU9S4DKJjZifhNWu9YLifdc5dC1wLsHr1ardmzZrCZm4S8Z5TtGb0Cd1+K0Bh0ppAClY+U5TKJ5rKJ5rKR2TiOdTWzYd/8RhlpSX86Z/P5sTFM4udJRGJgXwCyL3AssD7pf60sGX2mFkZMAtvMB3MbCnwW+BdzrltgeWX5khTRERERDJ09ib4ft02rntgJz2JFN+99FQFjyIybvIJINcDK83saLwg7xLgHzKWWQu8G3gQeDNwl3POmdls4FbgCufc/emFnXP7zazVzM7CG0TnXcB3RrsxIiIiIlNVKuX4Xt1WfnTfDpo7+7jwpEV8/FUrOWGRgkcRGT85A0i/T+NH8EZQLQV+4pzbaGZXAxucc2uBHwM3mNlW4DBekAnwEWAFcKWZXelPu8A5dwj4EAOP8fgTGkBnXOhJKSIiIpNPR0+Cf/v1k/zxyf2cf+JC/mnNsZx+1NxiZ0tEYiivPpDOudvwHrURnHZl4HU38JaQz30O+FyWNDcAJw8nszJ6ih9FREQml12Nnbzjxw+x63Ann7noBC5/2TF6dqOIFM24DKIjE4fiRxERkcnjQEs3H73xUZo6e7nx/WfxkmPnFTtLIhJzCiBjRk1YRUREJr5kynHl75/m5w/torzU+O4/nKbgUUQmBAWQMaPwUUREZOL73WN7+flDu3j9KUfwL+ev5JgFM4qdJRERQAFk7KgCUkREZGLrS6b41l+3sGrxTL71thdRUqL+jiIycZQUOwMyvpzqIEVERCa0Xz+yh12HO/nXC45T8CgiE44CyJhRDaSIiMjE1ZNI8p27tvKiZbM574SFxc6OiMgQCiBFREREJoib1+9mb3MXn3jVcXpUh4hMSAogY0Y1kCIiIhNTd1+S7969lRcvn8O5K+cXOzsiIqEUQMaM+kCKiIhMTF+8bRMHW3v4xKuOV+2jiExYCiBjRjWQIiIiE8/Te1v46YPPc8rSWXreo4hMaAogY0bxo4iIyMRz9+ZDAHz70lOLnBMRkWgKIGPGqQpSRERkQkmmHDet381Zx8zlqHnTi50dEZFICiBjRuGjiIjIxPLnjQfY29zFe85eXuysiIjkpAAyZlQBKSIiMrH83/07OXLuNF61alGxsyIikpMCyLhRACkiIjJh1Lf1sP75w7z59KWUlmjkVRGZ+BRAxowe4yEiIjJx3L35EM7BK09cWOysiIjkRQFkzKgJq4iIyMRx56aDHDGrilWLZxY7KyIieVEAGTOKH0VERCaG7r4k925p4LwTF2Km5qsiMjkogIwZPcZDRERkYvjjk/vp6kvy6pM0eI6ITB4KIGNG4aOIiMjEcP2DOzmudgbnrJhf7KyIiORNAWTMqAJSRESk+LYcbOPJPS1cesaRar4qIpOKAsiY0SisIiIixffQjsMAvPKE2iLnRERkeBRAxo3iRxERkaJqaO/hP9duZNncapbNrS52dkREhkUBZMwE48dHdzWx/IpbeXx3c7GyIyIiEjt3PnOQZMrxoTUr1HxVRCYdBZAxE+wDeffmQwD87dn6IuVGREQkfu7d0sCimVVc8uJlxc6KiMiwKYCMmWAfSAuZJiIiImOnsb2HuzYf4hUnLFDto4hMSgogY2bQKKw6cYmIiIyru5+tp6svydvPPKrYWRERGREFkDGjukYREZHiuX9rA/OmV7Bq8cxiZ0VEZEQUQMaMC3kQpJ4NKSIiMvacc9y3tYGzV8ynpEStgERkclIAGTPBYHGgD6SIiIiMtS2H2qlv6+Glx84rdlZEREZMAWSMqQukiIjI+LlvSwMAL10xv8g5EREZOQWQMRPaXFVtWEVERMbcA9saOHLuNJbNnVbsrIiIjJgCyJgZ/BgP86eJiIjIWEokUzy0/bBqH0Vk0lMAGTPBysZv3PnckGkiIiJSeE/saaatJ8FLV6j/o4hMbgogY0axooiIyPi7/ekDlJcaLztuQbGzIiIyKgogYyb0MR4KK0VERMbUXZsPcdYx85hZVV7srIiIjIoCyJjRGDoiIiLja1djJ9vqO3jF8QuLnRURkVFTABkzChZFRETG13/f+gwA552gAFJEJj8FkLGjCFJERGS8tHX3ceemgyyZXc3y+dOLnR0RkVFTABkzYTWQCilFRETGxrrth3EO/uctpxQ7KyIiBaEAMmbUB1JERGT83LelnuryUk47anaxsyIiUhAKIGNGwaKIiMj4uW9rA2ccPZfKstJiZ0VEpCAUQMZM+pEds6rLh0wTERGRwtnf0sW2+g7OXTm/2FkRESkYBZAxk66BnFYRuBOq+FFERKTg1u9sAuCsY+YVOSciIoWjADJm0gFkdSCAVPwoIiJSeE/taaayrITjF9UUOysiIgWjADJm0s1VB9VAioiISME9saeFVUfMpLxUl1siMnXoiBYz/U1Yy8sC01QHKSIiUkjJlGPj3hZeuGRWsbMiIlJQCiBjJqwJq4iIiBTW9vp2OnqTvHDp7GJnRUSkoBRAxkxYE9b/vXcHX/vzs8XKkoiIyJTz8M7DAJx65OziZkREpMAUQMZMugbSbPD079y1dfwzIyIiMkU9sK2RRTOrOHr+9GJnRUSkoBRAxky6t2NDe++g6dXlatIqIjIezOxCM3vWzLaa2RUh8480s7vN7DEze9LMXlOMfMrIOed4aHsjZx87D8u8YysiMskpgIyZ9IA5rzh+4aDpC2oqi5EdEZFYMbNS4BrgImAVcKmZrcpY7LPAzc65U4FLgO+Nby5ltBrae2lo7+VkDaAjIlOQAsiYSddAnrB48DOpSnSDVERkPJwBbHXObXfO9QI3ARdnLOOAmf7rWcC+ccyfFMBzB9sA9PxHEZmSynIvIlNJfx/I4mZDRCSulgC7A+/3AGdmLHMV8Gcz+ygwHTg/LCEzuxy4HKC2tpa6urpC53XSaG9vn1Dbf8fOPgAatz1F3Z7in3EnWvlMNCqfaCqf3OJWRgogY8eLIDP7ZKiPhojIhHEpcJ1z7mtm9hLgBjM72TmXCi7knLsWuBZg9erVbs2aNeOf0wmirq6OibT9t/7qCeZNP8QbXv2KYmcFmHjlM9GofKKpfHKLWxmpCWvMZKuBVPgoIjIu9gLLAu+X+tOCLgNuBnDOPQhUAfPHJXdSEE/saeaFS9X/UUSmJgWQMZPuA6kKRxGRolgPrDSzo82sAm+QnLUZy+wCXglgZifiBZD145pLGbG27j62HGrnlGWzi50VEZExoQAyZgZqIBVBioiMN+dcAvgIcAewCW+01Y1mdrWZvcFf7F+B95vZE8CNwHtceghtmfCe2tOCc/AiBZAiMkWpD2TMpK9BVAMpIlIczrnbgNsypl0ZeP0M8NLxzpcUxv/eux2AU5bOLm5GRETGiGogY6a/CWtRcyEiIjL1JJIp6p6rZ0ZlGXOmVxQ7OyIiY0IBZMw4RZAiIiJjYm9zF87Bla9bVeysiIiMGQWQMePSj/HIjCAVUIqIiIzK9voOAJbPn17knIiIjB0FkHGTHkRH8aOIiEhBbT7QBsDxtTVFzomIyNhRABkzasEqIiIyNp490MriWVXMmlZe7KyIiIwZBZAx0/8YDw3DKiIiUlCbD7Rx/CLVPorI1KYAMmb6+0AqfhQRESmYvmSKbfXtnLBoZrGzIiIyphRAxkx/DWRxsyEiIjKlbK/voC/pOEE1kCIyxSmAjJn+PpCKIEVERApm84FWAE5YrABSRKY2BZAx47I8CFJ9IkVEREZu0/42ykuNYxfMKHZWRETGlALImNJjPERERApn0/5WViysobxUl1YiMrXpKBczLvciIiIiMkybD7Ryovo/ikgMKICMKdU4ioiIFEZHT4KDrT0cu1DNV0Vk6lMAKSIiIjIKe5q6AFg6p7rIORERGXsKIOMmSxvWLYfa+c5ft4xvXkRERKaA3Yc7AVg2d1qRcyIiMvYUQMZU2KirX/vLc0XIiYiIyOS2yw8gj1IAKSIxkFcAaWYXmtmzZrbVzK4ImV9pZr/05z9kZsv96fPM7G4zazez72Z8ps5P83H/b2FBtkgiOQ2jIyIiUlC7DncyvaKUudMrip0VEZExV5ZrATMrBa4BXgXsAdab2Vrn3DOBxS4DmpxzK8zsEuDLwNuAbuA/gJP9v0xvd85tGOU2yAhoEB0REZHC2H24k2Vzp+mZyiISC/nUQJ4BbHXObXfO9QI3ARdnLHMx8FP/9S3AK83MnHMdzrn78AJJmQCcKiBFREQK6vnDnRw1T81XRSQectZAAkuA3YH3e4Azsy3jnEuYWQswD2jIkfb/mVkS+DXwOeeGhjdmdjlwOUBtbS11dXV5ZHlqam9vH/X2P3UoAcCjjz4SOn8yl28hymcqU/lEU/lEU/mIhEulHLsPd/KK4xcUOysiIuMinwByrLzdObfXzGrwAsh3AtdnLuScuxa4FmD16tVuzZo145rJiaSuro7Rbn/imYPw6AZOP/10ePD+IfMnc/kWonymMpVPNJVPNJWPSLhDbT30JFIcOW96sbMiIjIu8mnCuhdYFni/1J8WuoyZlQGzgMaoRJ1ze/3/24Bf4DWVlXFi6gUpIiIyaukRWI/UCKwiEhP5BJDrgZVmdrSZVQCXAGszllkLvNt//WbgrrDmqGlmVmZm8/3X5cDrgKeHm3kZPnWBFBERKZz+Z0DOqS5yTkRExkfOJqx+n8aPAHcApcBPnHMbzexqYINzbi3wY+AGM9sKHMYLMgEws53ATKDCzN4IXAA8D9zhB4+lwJ3A/xZywySaBooTEREZvb3NXQAcMVsBpIjEQ159IJ1ztwG3ZUy7MvC6G3hLls8uz5Ls6fllUQopomIYgDd9735++6GXjlNuREREJrc9TZ0srKmkqry02FkRERkX+TRhlRh5bFdzsbMgIiIyaexp6mKpmq+KSIwogIwZ9YEUEREpnD1NXSyZowF0RCQ+FEDGlPpAioiIjE5fMsW+5i4NoCMisaIAMmZydIEUERGRPNU9W08i5Th+UU2xsyIiMm4UQMaUngMpIiIyOrc8shuAU5fNKXJORETGjwJIERERkRHY39LNGcvncuQ89YEUkfhQABk7asMqIiJSCHubujh24fRiZ0NEZFwpgIwpDaIjIiIyci1dfTR29LJsrmofRSReFEDGjAbRERERGb3nDrYBcIIG0BGRmFEAGVOqgRQRERm5XY2dABw9f0aRcyIiMr4UQMaMKiBFRERG70BrNwCLZlYVOSciIuNLAWRM6TEeIiIiI3eotZuZVWVUV5QWOysiIuNKAWTM5NMH0qmjpIiISKQDrd3UqvZRRGJIAWRMRfWBTCl+FBERiXSwtYdFsxRAikj8KICMGZdHL0jVQIqIiEQ72NrNwhoFkCISPwogYyqqB6TCRxERkexSKcehth4WzaosdlZERMadAsiYyadyMaUaSBERkawaOnpIppz6QIpILCmAjKmoPpCKH0VERLI71NoDoABSRGJJAWTMKDYUEREZnQMt3jMgFUCKSBwpgIyt7FWQasIqIiKS3YFWL4BcpABSRGJIAWTM5DPCquJHERGR7A61dlNiMH9GRbGzIiIy7hRAxlT0cyAVQYqIiGRzoLWb+TMqKSvVZZSIxI+OfDKEwkcREZHsDrb2qP+jiMSWAkgZQhWQIiIi2R1s7VYAKSKxpQAypiJasObVT1JERCSuvACystjZEBEpCgWQMZNPbKj4UUREJFx3X5Kmzj6NwCoisaUAMqYsYhQdB+xq7KSpo3f8MiQiIjIJ1Lf1AHoGpIjElwLImHF5DJGTco6XffVuXv7Vu8chRyIiIpNH+hmQtbMUQIpIPCmAjKnoPpDe/63diXHJi4iIyGRxMB1Aqg+kiMSUAsiYya8PpDpBioiIhDnQ4gWQ6gMpInGlADKmIrpA6jmQIiIiWRxq66GyrIRZ1eXFzoqISFEogIwZjcIqIiIycgdavGdARg1GJyIylSmAjCmL6AWZUgQpIiIS6mBrt5qvikisKYCMmWyh4b9deELOZUREROLuYGs3CzWAjojEmALImMpseTN3+kBfDg2iIyIiMpRzjoOtPaqBFJFYUwAZM2HB4dffesqgvhyKH0VExo6ZXWhmz5rZVjO7IssybzWzZ8xso5n9YrzzKOHaexJ09SVVAykisaYAUjj9qDmUKoAUERlzZlYKXANcBKwCLjWzVRnLrAQ+A7zUOXcS8PHxzqeEO9TWA8DCGtVAikh8KYCMmWyxYWlJIIBUL0gRkbFyBrDVObfdOdcL3ARcnLHM+4FrnHNNAM65Q+OcR8mi3g8gF9SoBlJE4qus2BmQiaEkEECmFD+KiIyVJcDuwPs9wJkZyxwHYGb3A6XAVc652zMTMrPLgcsBamtrqaurG4v8Tgrt7e3jsv3r9icA2Ln5Sfr2TJ578ONVPpOVyieayie3uJWRAsiYyhxEZ3ATVkWQIiJFVAasBNYAS4F7zOwFzrnm4ELOuWuBawFWr17t1qxZM765nEDq6uoYj+3fdt8OeOIZXnveOcyeVjHm6yuU8SqfyUrlE03lk1vcymjy3D6TwgiJDQ2jNLAnqAZSRGTM7AWWBd4v9acF7QHWOuf6nHM7gOfwAkopskNt3VSUljCrujz3wiIiU5QCyJiyjCrIkkHvFUGKiIyR9cBKMzvazCqAS4C1Gcv8Dq/2ETObj9ekdfs45lGyqG/rYUFN5ZBzqIhInCiAjJlsA+QMGkRH8aOIyJhwziWAjwB3AJuAm51zG83sajN7g7/YHUCjmT0D3A18yjnXWJwcS1A6gBQRiTP1gYyp4L1Ts8GD6CQVQYqIjBnn3G3AbRnTrgy8dsAn/D+ZQOrbelg2d1qxsyEiUlSqgYyZbLFhXyLV//qe5+rHKTciIiKTx6G2HhaqBlJEYk4BZExldt9o70n0v/7CbZvHOTciIiITW18yxeGOXjVhFZHYUwAZM9kapwYDSBERERmsob0HgIU1VUXOiYhIcSmAjCljcBXkigUzipQTERGRia++zQsgVQMpInGnADJmsvWBPHvFfD73xpPHNzMiIiKTxG8e9R7XqT6QIhJ3CiBjKuwRVotmqlmOiIhImG317QAcV1tT5JyIiBSXAsiYCXsOZDqYLNHeICIiEqqps5fzTlhIdUVpsbMiIlJUChliKqQCcki/SBEREfEcbu9l7vSKYmdDRKToFEDGTLY+kEB4VCkiIhJzzjkaO3qZpwBSREQBZGyFBIslYR0jRUREYq6rL0lPIsUcBZAiIgogBcwPHBU+ioiIDNXY3gugJqwiIiiAjJ2oFqyqgRQRERmqqdMPIKcpgBQRUQAZU2ED5oTFj4dau8chNyIiIhNXY4cfQM5QACkiogAybiJG0QmrfzzjC38du7yIiIhMAofbVQMpIpKmADKmgrWN1j9NTVhFREQy9TdhVQ2kiIgCyLiJfIqH4kcREZEhGjt6KS81airLip0VEZGiUwAZU2GxouJHERGRoZo6epkzrUItdUREUAAZO2FdINPnw5KS8BOji+g3KSIiMtU1dvTqER4iIj4FkDEVdhc1231VxY8iIhJnTQogRUT6KYCMmajaxGwtc1KKIEVEJMYOK4AUEemnADKmhtOL467Nh8YsHyIiIhOdmrCKiAxQABkzYXWJNvAgj9DPXH7DI2OWHxERkYkskUzR0tWnAFJExKcAMqY0kJyIiEhuTZ19AAogRUR8CiBjJqo7o4JKERGRwZo6ewEFkCIiaQogY8oCzVUVOIqIiIRrbPcDyGkKIEVEQAFk7ESNp6o4UkREZLDDHX4AOUMBpIgIKICML0WLIiIiOR3uVA2kiEiQAkgRERGRLA77TVjnqA+kiAigADJ2XMgoOv0P8VBnSBERkUGaOnuZWVVGeakumUREQAFkbClWFBERya2xo1cjsIqIBCiAlH6KKUVERAZrUgApIjJIXgGkmV1oZs+a2VYzuyJkfqWZ/dKf/5CZLfenzzOzu82s3cy+m/GZ083sKf8z3za1nxxXlvWNiIiIpKkGUkRksJwBpJmVAtcAFwGrgEvNbFXGYpcBTc65FcA3gC/707uB/wA+GZL094H3Ayv9vwtHsgEyPCFdIPtFhfCN7T2Fz4yIiMgEd7ijRwGkiEhAPjWQZwBbnXPbnXO9wE3AxRnLXAz81H99C/BKMzPnXIdz7j68QLKfmS0GZjrn1jlvVJfrgTeOYjtkmIZb4Xv65+7kVxt2j1FuREREJh7nHE0dfRqBVUQkoCyPZZYAwchhD3BmtmWccwkzawHmAQ0Rae7JSHNJ2IJmdjlwOUBtbS11dXV5ZHlqam9vH/X2b93RB8C9997bP+2BBx5gdmUJO1uSkZ/9zf0bWdC+bVTrH0uFKJ+pTOUTTeUTTeUjcdTRm6Q3mdIzIEVEAvIJIIvKOXctcC3A6tWr3Zo1a4qboSKqq6tjtNv/XMk2eHYzLzv3XLjzDgBeevZLWVBTyVN7WuDB+7J+dmFtLWvWnDqq9Y+lQpTPVKbyiabyiabykThq7vSfAakAUkSkXz5NWPcCywLvl/rTQpcxszJgFtCYI82lOdKUMTDSPpAiIiJx09zptdqZWV1e5JyIiEwc+QSQ64GVZna0mVUAlwBrM5ZZC7zbf/1m4C4X9sR6n3NuP9BqZmf5o6++C/j9sHMvIzaSYDEq+BQREZlqWru8AHL2NAWQIiJpOZuw+n0aPwLcAZQCP3HObTSzq4ENzrm1wI+BG8xsK3AYL8gEwMx2AjOBCjN7I3CBc+4Z4EPAdUA18Cf/T8ZYWAyomkcREZGhmhVAiogMkVcfSOfcbcBtGdOuDLzuBt6S5bPLs0zfAJycb0alsGwED39UBaSIiMRJugnr7Gr1gRQRScunCatMIaPpAxnRKllERGTKae7yBtFRDaSIyAAFkDGlZqsiIiLRWjr7qCwroaq8tNhZERGZMBRAxowLaYhq/f9HR5WqfxQRkThp7uxT7aOISAYFkJI/RZAiIhIjzV29zNIjPEREBlEAKSIiIhKipatPA+iIiGRQABkzoxpER1WQIiISI82dfcxSE1YRkUEUQMZUMFg0jagjIiIyhFcDqQBSRCRIAaT0y/0Yj/HJh4iIyETQ3NmnPpAiIhkUQMZUrhFXwyiAFBGRuOhJJOnqS2oUVhGRDAogY8aFRIH5PsZDREQkLlq6+gCYNU2D6IiIBCmAjKmRdHvUIDoiIhIXLZ1eAKk+kCIigymAjJl8R2F944uOGNZnRUREppL+GkgFkCIigyiAjKlcFZDL5k4bl3yIiIhMRM3pGkj1gRQRGUQBZMyEVSKmax6DQWVYgKkKSBERiYtm1UCKiIRSABlTOZ/9GDJfTVhFRCQu0k1YZ1drEB0RkSAFkDGTbx9IjccqIiJx1tLZixnUVJUVOysiIhOKAsiYGtxcdWi4OJJRWkVERKaKlq4+ZlaVU1KiE6KISJACyJiJfhSHTpIiIiLgBZDq/ygiMpQCyJjK2QVSw+iIiEiMNSuAFBEJpQAyZvLuAxkSP2oQHRERiYuWrj49wkNEJIQCyJiyHCPmqDGriMjYMLMLzexZM9tqZldELPf3ZubMbPV45k88LV19zFQNpIjIEAogJVRoDeT4Z0NEZEoxs1LgGuAiYBVwqZmtClmuBvhn4KHxzaGktXSqCauISBgFkDEzmiF0+pKpQmZFRCSOzgC2Oue2O+d6gZuAi0OW+2/gy0D3eGZOPIlkisaOXmYrgBQRGUIPN5LQ2kYLmXjvlgb++OQ+XvfCI8YhVyIiU9ISYHfg/R7gzOACZnYasMw5d6uZfSpbQmZ2OXA5QG1tLXV1dYXP7STR3t5e0O1/cF8CgIb9u6irO1CwdIul0OUz1ah8oql8cotbGSmAjJuIkXDCgsZMv3tsrwJIEZExYmYlwNeB9+Ra1jl3LXAtwOrVq92aNWvGNG8TWV1dHYXc/q33bocnN/Fvb3k582ZUFizdYil0+Uw1Kp9oKp/c4lZGasIaQ3nEiVmXuXPTocJmRkQkXvYCywLvl/rT0mqAk4E6M9sJnAWs1UA646u1qw8zmDOtothZERGZcBRAxky+fSDDnwPpp6HneYiIjNR6YKWZHW1mFcAlwNr0TOdci3NuvnNuuXNuObAOeINzbkNxshtPrd0JZlSWUVKiMclFRDIpgIyhzNNh2OkxqpZS8aOIyMg45xLAR4A7gE3Azc65jWZ2tZm9obi5k7TW7j5mVmkAHRGRMOoDGTNRwV8+TVsBUs5RoidFioiMiHPuNuC2jGlXZll2zXjkSQZr607oGZAiIlmoBjKG8hksJ2oJVUCKiMhU1trVR02V7rGLiIRRABkzLiT8SweUwX6PUTFmSm1YRURkCmvtTqgJq4hIFgogYyifxqfRg+gULi8iIiITTVt3HzNVAykiEkoBZMzk2wdSg+iIiEhctXb1qQ+kiEgWCiBjKN/BcrJRE1YREZmqUilHe09CNZAiIlkogIyZsNBvuPGkAkgREZmqOnoTpBzUqA+kiEgoBZAxFNW/sX+ZiGrKlOJHERGZolq7EwDMrFYNpIhIGAWQMZN3H8jIRAqVGxERkYmlrbsPUA2kiEg2CiDjKI82q3qMh4iIxFFrl18DqQBSRCSUAkgJDRajYkwFkCIiMlW1dnk1kGrCKiISTgFkzLiI9qdR/R4HpyEiIjI1tfWoCauISBQFkDGUT5gYFUzuPtzJbx7dU7gMiYiITBADTVhVAykiEkZHx7gJqT5Mj8oaDBmjKiP//vsPkHLwhlOOoKxU9yBERGTq0CA6IiLRdPUfQ/m0VI3uA1mwrIiIiEword0JqspLqCjTJZKISBgdHWMmKvYLBpYLaqpGlZaIiMhk1NrVpxFYRUQiKICMIcujF+SrT6rl2neePg65ERERmTjauhPUqP+jiEhWCiBjxoU8giNd8xgMLM2MC05alCOtgmZNRESk6Fq7+5hZrRpIEZFsFEDGUJ5P68gp6pEgIiIik1Frd0JNWEVEIiiAjJmoWsNcgeW/XXhCYTMjIiIywbR19akJq4hIBAWQMTTSCsjMJ3aoCauIiEw1asIqIhJNAWTMRI7CmuOzJYVq+yoiIjJBtWoQHRGRSAogY8hGGAhmBpCqgRQRkamkuy9JbyKlPpAiIhEUQMZMZNCXI64sLVENpIiITF1t3QkANWEVEYmgADKGMsPAfCskM+NHjcIqIiJTSWt3HwAz1YRVRCQrBZDSz3JUQZaUqAmriIhMXa1d6QBSNZAiItkogIyZ0dQalmoQHRERmcIGmrCqBlJEJBsFkHFkmW/zCwyHDKJTqPyIiIhMAOkmrDWqgRQRyUoBZMxENTvNVcE4tAmrQkgREZk6Wru8Gkg9xkNEJDsFkDE00oaopRl7i8JHERGZStpUAykikpMCSOmXK7DMbML6o3u2s/yKW0mmFEqKiMjk196ToMRgekVpsbMiIjJhKYCMIcsIBPMdGyfzOZDfvmsrAH3JVEHyJSIiUkxt3QlmVJYNOU+KiMgABZAxE9VvMdcJM9sorOoKKSIiU0Frd5+ar4qI5KAAMoaGc2P1/ece3f86cxCdtKQiSBERmQLauhMaQEdEJAcFkDETFupZxv9BH3z5sf2vM/tApqUUQIqIyBTQrgBSRCQnBZAxVKhRWNOcukCKiMgU0NajJqwiIrkogIyZ4T4HMtgvUjWQIiIylakJq4hIbgogY6jQo8v96ekDBU1PRESkGBRAiojkpgAyZlxIL8h0QGkhjVuDU7LVM/6/3z5Fq//wZRERkcmqvTvBjEo1YRURiaIAMoaGU/9o+USQQCKpZqwiIjJ5dfcl6U2mVAMpIpKDAsiYieyumCOyDKu9TFM/SBERmcz2NHUCsLCmssg5ERGZ2BRAxtBwukAGm7VGxYiqgRQRkcns4R1NAJx65Jwi50REZGJTACl5N2mNCiD7knqWh4iITF63bzzAkXOnceyC6cXOiojIhKYAMmYiW7CGRZKBaVGf7VUAKSIik1RLVx8PbG3gohcsKvhI5SIiU40CyFjK/+QYPI+6iCpINWEVEZHJ6q7NB0mkHBeetKjYWRERmfAUQE4y779+A2//0bq8ll1+xa0sv+JW9jZ39U8LiwHTQWKusDIqRFQTVhERmaxuf/oAi2ZWccrS2cXOiojIhJdXAGlmF5rZs2a21cyuCJlfaWa/9Oc/ZGbLA/M+409/1sxeHZi+08yeMrPHzWxDQbYmBv7yzEHu39qYc7lgbeFD2wcvP7xBdAakUtlDSDVhFRGRyeiBbQ3csfEgrz6plpISNV8VEcklZwBpZqXANcBFwCrgUjNblbHYZUCTc24F8A3gy/5nVwGXACcBFwLf89NLe4Vz7kXOudWj3hIZJBkI9kpL8nuYY65+HxHxI/VtPfx108F8syciIjIh/OKhXQC896VHFzknIiKTQz41kGcAW51z251zvcBNwMUZy1wM/NR/fQvwSvOikYuBm5xzPc65HcBWPz0ZY4lAtFeSERgO5/5qMKh80ZGzAbjo5KF9RD5wwyNc9tMNNHf2DiufIiIixdKTSFL3bD2XnrGM5fM1+qqISD7yCSCXALsD7/f400KXcc4lgBZgXo7POuDPZvaImV0+/KxLlESWGsjwPpDe/ByDsLJkdjU7v/RaXn7cgqzrTUZVU4qIiEwg929toL0nwQUaPEdEJG9lRVz3Oc65vWa2EPiLmW12zt2TuZAfXF4OUFtbS11d3Thnc+Job28nHdLlKoeOvoFAbtMzG5nW+CwA+/b30NubHPT59OvuhMtr2rN7+rKu98EHHmBGRXH6kLS3t8d6/8hF5RNN5RNN5SNT0S/X72bu9ArOPnZesbMiIjJp5BNA7gWWBd4v9aeFLbPHzMqAWUBj1Gedc+n/D5nZb/Gatg4JIJ1z1wLXAqxevdqtWbMmjyxPTd7FWwcAucqhsb0H/nonAC98wQtYs6oWgD81PMmzrYe8z99+66C0OnsTcOcdOacdWr8bnn4ydL3nnnsOs6rLh79xBVBXV5ezXOJM5RNN5RNN5SNTTVt3H3dvrucdZx1FZVlp7g+IiAiQXxPW9cBKMzvazCrwBsVZm7HMWuDd/us3A3c5bxjQtcAl/iitRwMrgYfNbLqZ1QCY2XTgAuDp0W+OANy3pYGDrT3970szvmUbVi/IYVILVhERmQTu2nyI3mSK175QzVdFRIYjZw2kcy5hZh8B7gBKgZ845zaa2dXABufcWuDHwA1mthU4jBdk4i93M/AMkAA+7JxLmlkt8Fu/710Z8Avn3O1jsH2xk0o53vHjh5hROfDVBgfRcVGjsIYElqHBZkT8GZW+iIjIRHH70weonVnJqcvmFDsrIiKTSl59IJ1ztwG3ZUy7MvC6G3hLls9+Hvh8xrTtwCnDzazklh48p70n0T8t8/Ecw3kOZJioj4cN0iMiIjKROOdYv7OJlx+3QM9+FBEZpmIOoiPDcNXajdz6WOeQ6Z/81RPc8sgezl05nxsuOzN0FNRUYFpUgBcWWA432Ewn/4EbNlBWWgIOEqkUP3ynHvUpIiITw/6Wbhrae3jBkpnFzoqIyKSjAHKSuO6BnaHTb3lkDwD3bmkAvGAtUyoQNabc0OdCDldmjWaQ89d1x8aDo1qHiIjIWFm/8zAAq5fPLXJOREQmHwWQk9Q9z9VTUzX467tj4wHOCDkZBmslk6nUoOdCjkTUp9u6Ezy9r3VU6YuIiIylddsPU1NZxomLVQMpIjJcCiAnqXf95OEh0z5wwyP88J2nD5kebNWadAwrgBxus9Z//uXjPLG7Oe/0RURExtvDOxpZvXzOqG+oiojEUT6P8ZBJpLWrb8i0YBPWQtRARtnZ0DFmaYuIiIxWfVsP2+o7OPOYecXOiojIpKQAcooJGyNncBNWR2mWKsTQ2sawR3tExJ8VZdqlRERk4kr3fzzjaPV/FBEZCV3tTzHt3Ykh0wbXQBagCWtEL8iKUu1SIiJRzOxCM3vWzLaa2RUh8z9hZs+Y2ZNm9lczO6oY+Zyqbnx4F9XlpbxgyaxiZ0VEZFLS1f4U05YzgMzehDUqMMyXaiBFRLIzs1LgGuAiYBVwqZmtyljsMWC1c+6FwC3AV8Y3l1PX1kPt3LulgRcsmUW5bniKiIyIjp5TTGv30D6QycCTPYY9iE7YtKgmrDohi4hEOQPY6pzb7pzrBW4CLg4u4Jy72zmXfvDvOmDpOOdxyvrLM94jpj7/ppOLnBMRkclLo7BOMW0hAWTeNZAFGFunvEwj2omIRFgC7A683wOcGbH8ZcCfwmaY2eXA5QC1tbXU1dUVKIuTT3t7e17b/9uHujhqZgl7Nz3C3k1jn6+JIt/yiSuVTzSVT25xKyMFkFNMS9gorHkOohPGQpZNTzMDlzFqz9N7o58B+f9++xTNnb187+1DHzciIiIDzOwdwGrg5WHznXPXAtcCrF692q1Zs2b8MjfB1NXVkWv7mzp62XrHX/jIK1awZs3x45OxCSKf8okzlU80lU9ucSsjBZBTTFdfasi0pBscQJaVhDczDW2uGrGuErNBaefjFw/tGtbyIiJTzF5gWeD9Un/aIGZ2PvDvwMudcz3jlLcpre65Q6QcvPLE2mJnRURkUlOHtSmmuy85ZFqgAtKrgRzlcyDTnw4+HiSffN27pX5U6xURmQLWAyvN7GgzqwAuAdYGFzCzU4EfAm9wzh0qQh6npDs3HWJBTaVGXxURGSXVQE4xoQFksAlrxCA64c1Vw5Ybfr7+32+f4jePDrnJLiISK865hJl9BLgDKAV+4pzbaGZXAxucc2uBrwIzgF/5x+Vdzrk3FC3TU0BPIsnfnq3ntS9YTMkob6KKiMSdAsgpJrwGMr9BdMbSpv1t475OEZGJyDl3G3BbxrQrA6/PH/dMTXEPbGukvSfBq09W81URkdFSE9YppjusD+SgQXQiaiDDpoXVSo7geZGb9kcPriMiIjJW/vZsPVXlJZx97PxiZ0VEZNJTADnFdAVqII+rnQGE1ECO8nkd6Y+fvGQmn33tiVx2ztGjSk9ERGQsPbTjMKcdOYeq8tJiZ0VEZNJTADnF1LcNDNb36pMWAd6Dk9c+sY9UykUOopNvXGn9/xv/eO4xzJ1eMZosi4iIjJmWzj42H2jlzKPnFTsrIiJTgvpATmHpQHH9zibW72xixYIZBRmFNS0dcJaXakACERGZmB7Y1oBz8JJjFUCKiBSCaiAnsVOPnB05vywjUOzqS5J0UTWQ+QWCmYtVlqlJkIiITEz3bGlgRmVZznOmiIjkRwHkJJbrMYyZQ5X/3/072H24i8MdvaNcswX+hapy7UYiIjIx3b+1gbOOmUd5qc5VIiKFoKPpJNbW1df/emFN5ZD5mYPl/PHJ/QBsb2gvTAb89FUDKSIiE9H+li52He7krGPmFjsrIiJThgLIIrl/awOJ5NBHbgxHSyCAfNdLjhoyvyRLk9T27sSo1ju0Cat2IxERmXge3nEYgLOOUf9HEZFC0ZV/Eazb3sjbf/QQ375r66jSufxlx/S/zmyuCtlHVe3oSYbPyFNmsmVqFiQiIhNMIpniK7c/y+xp5Zy4eGaxsyMiMmXoyr8IDrZ2A7CjoWNEnz/rmLls/8Jr+MDLj+UNpxwBhNc2mhkfPW/FkOm9o6z5zBxsR/GjiIhMNOu2H2ZvcxevPKG2YKOPi4iIAsii+sMT+3h6b8uIPpuucUzHcmHnRgMqQqK7zNFZRyqdSramsiIiIsVy79Z6ykuNqy8+qdhZERGZUhRAFoELjJ56zd2ja8aaZkMalnrBZXlG/8QVC2dw2z+fO8p1DZZ5Z/dnl505qvRFRERG674tDZx65BymV+qR1yIihaQAsggcAxFkyuV4FkfY593Q12GVgAZDhi3/xKuO47jammGvc1C6Nvj/4Givx9fWcM7K+bxgyay80npidzNt3X25FxQREcnT4Y5eNu5r5dwV84udFRGRKUcBZBGEBYCjZWacdMTgQQJKSozy0sGR5aJZVYVZIQM1kccsmDFkXjBIzqa7L8nF19zPB254pGB5EhERuX9rAwAvXakAUkSk0BRAFsGgAHIknw95XWLwh4+cM2i5sBrI8pLRf+WZtZ2LZlXx/befNmhePoFxnz+YzxO7m0edJxERkbT7tjRQU1XGC/NsDSMiIvlTAFkEgwLAQtVAEvIoD7MhAWQhxrsJ62853Ed5PLmnmVd+7W+ABuEREZHCcc5x39YGzj52nh4zJSIyBnRkLQLnwuoQh5PA0Emhz4GEIU1YCxKsZelvGZQrMP7K7c9yqK3Hy5OGVxcRkQLZ2djJ3uYuzlm5oNhZERGZkhRAFkEwuGrrTuR8lIfLiMaC/QvT8zKfzQhesDgWNZADaVng9cjT0fO5RESkUO7bUg+gAXRERMaIAsgiCAaAD+04zOu+cx+9iVT25fOopEyHYCcsGhhh1WzocyALUQMZlkJmssOpV1X8KCIihXLvlgaWzK7mqHnTip0VEZEpSQFkEYQFhD2JZNbl04/6qJ1mQz4/MIiON+/Wj53L6UfNAbxAb9a08kFpFSJYS9c8BpPK7BeZWWsaRX0gRUSkEBLJFA9ua+TclfNDW+aIiMjoKYAcZ82dvVzxm6eGTO8J1EDev7WBN3//Ae7b0sDF19xPrz9aaWVp9pNh/zMZS4wyP0o0g1nV5RnLFe6EaoMjyBFTACkiIoXwxJ4W2noSnKPHd4iIjJmyYmcgbu5+9lDo9O6+gRrIT9z8OAdbe3jvdQ/Tl3TsbeoCIB0/Dqrb898EaxbT8ZiZhQSQo8m9n0ae0yLTCHxAfSBFRKQQ7t/agBmcfawCSBGRsaIayHGWrWVnugbysV1NJFODF0qk0gPlZE837NEaxtAayIL0gQwbhTXdrNX/PzWMJqyqgBQRkUK4b0sDJx0xk7nTK4qdFRGRKUsB5DhLZYmruvuSPHugjTd97wEa2nsB6Et6C6cDyv4ayEBwlh6QJ1tQV1VeOmhaQfpAZglWR0o1kCIiMlo9iSSP727mJcfMK3ZWRESmNAWQ4yzb4DI9iRT1/nMRM6UDyKg4a9AjNfxwLr385v++cMi8QgimNWQU1mEMw6o+kCIiMlpP722hN5ni9KPmFjsrIiJTmgLIcZYtsPq77z3Apv2tofP6/EF0SkL6QLqQPpBp6bgsWAtZkD6QYeti8MisueLHe7c0FDRPIiISb1f+fiNA/0jkIiIyNhRAjjMXEVp9+fbNodPT/SNLbehjPNKCtXj9g+iE1DaWFKC5qA15MbogsFQRpIiIjEJ7r+OZ/a2csmw2C2oqi50dEZEpTaOwjoPnDrbR05diZnVZf//GMGWl1j9gTlD6GZHRTViHvg6LywrZ3TDqKR56DqSIiIyXTYeTOAf/8doTi50VEZEpTwHkOLjgG/fktVy2QKq7L3cT1rDnO4ZOK0QfyBE8x2N6RSkdvcnQeYWoFRURkfh65GCCmVVlnLJsdrGzIiIy5akJ6wSS7dEXrV19AFRFhPv5xnRjNwpr+jEe3vvglnzujSfz6JWvypqe4kcRERmpZMrx+KEkr3nBYspLdVkjIjLWdKQdY+kBcPKR7REfLf0BZP9zPPrnpftUloSMwhr1vMbRCOvHmZlsdWDgnqryUirLSslGTVhFRGSkthxqozsJZxyt0VdFRMaDAsgxlg7+8pGt32B/AFmaPdAK7QMZVlNYyD6QwXVmzPvhO0/vf52+IfyT96xmZkg16qzq8sJlSkREYuWR55sAjb4qIjJeFECOseEEkMksVZC3PLIHgGo/9sr3MR7h0woQQYZkM7Nmc+mcaZyzYv6gdZ53Qi0XnrxoyGfTAeTWQ2109CRGnz8REYmFZMpxw4PPs3CaceTcacXOjohILCiAHGPt3fkHRNmasB5q6wEGmrAGKyoHXuZX21iI/obpdQZrOMNGfk03dQ0Gl2GVrCnncM5x/tfv4f3Xbxh9BkVEJBZuWr+LzQfaeMtxFQXpoiEiIrlpFNYxFvZYjpGqzN6NMIv8RmYdqagmrBBeOxosjWMXTCeRciRTjl6/r+gD2xoLlj8REZm69jV38bU/P8cZR89ldW13sbMjIhIbqoEcY9lGVh2Jcv/bCg5iM/AYj4Hl0kFiWG3jWN2gDUt3IIAMr4EsLTGmVZSRctCbyD7Y0KG2bt79k4dpbO8pVHZFRGQSc87xkV88Sk9fkqtef5JqH0VExpECyDGWrV/jcH3g5ccwvzr71xX6GI+QE+rYjXjqj/wayEnYCLHpaUvnVPO9t59OaYl3IRAVQP583S7+9lw9P31g5xjkW0REJpvv3LWVR3c1c8VFJ7DqiJnFzo6ISKwogBxjqQIFkFdceEJ/jWKuSk3L+D+oIH0gQwfRyb7coCas/rSPn38cKxbOoMSMpBtowhrmiNlVAOxtVhMlEZG4e+T5Jr7+l+d44dJZvGX1smJnR0QkdhRAjrFkgZqwmg3U7YUHcPkNohP2aI+R5ymYbnaDm7B6mU8/2qPELGcT1poqb5TWfc1dI86riIhMfs45vnjbJhbUVHLj+8+iqnzYgwOIiMgoKYAcYyOtgPzwK44dMi0dhw1OcugKwkZEzZw3Gi50nUMTTi9VUjJ0WjqQLTGvljYqgEwPRLSvRQGkiEic/fmZg2x4vol/Of84pldqHEARkWJQADnG8mnCWhNyEpzp17oFFaLusBB9IPsH7gk+xiP9fzD5/gF+hg6ik55UWmKknKMnIoBMprx5e5sUQIqIxFV3X5LP3foMKxbO4K2rlxY7OyIisaUAcozlGkTn705dwv+89ZT+97/4xzP52HkrmDu9Ysiy/TWQbugorIOWi5hXiD6QmfnJJl1TWTpoEJ3MNGzQYzzC9CW9TxXykSgiIjK5fO/urew+3MXVF59EWakuX0REikVH4DHknGPddu+5hq95waLQZf7nLafw6pMG5p29Yj6fuOB4ykpD+jSGrSNkXtRw5uM51HnYYzzSjzVJTys1w+XoA1mokWxFRGRy2tHQwQ/+tp2LX3QEZx87v9jZERGJNQWQY2jzgTZ+dN8OgKwd/bPFc2FNTaNiv3zjwkLWQAZFhXgloc1a/Xkl3kBDfRE1kMGaRwWTIiLx0pdM8clfPUFlWQn//poTi50dEZHYUwA5hlq7+vpfhwWQT1x5QdYawdKQSC+saaqLGOU139FahysqhAuJFQf3gfSnDgyi4/WBjKyBDASXXX3J4WZXREQmsc/85ikeeb6JT1xwHAtnVhU7OyIisacAcgwFa86mVwwNIGdNGzpQTlppaA1kVNPUwGv//2LX1aX6H9mRfRAdM+OxXc08uK2xf5m3/fBB7t1S3//+qj880/+6szcxhjkWEZGJ5LFdTfzm0T1csKqW95y9vNjZERERFECOqeDAMH9/+lLe9ZKj+NllZ+b12ZKItqbBx2iEBYlj3c0xrNYzqiY0uCkDI7h6evwaxXRTX4CHdhzmpw88H5pWZ49qIEVE4uLLt29mQU0lX3vrKePah19ERLLTQ5TGUF+gWWZ1eSlXX3xy3p8NrYGMWN5C5kYFdaMR1jR1ICNhtY0hTVj9Sdke39HW3cfDOw5z4uKaQdM7e7MHkHvbUzjndJEhIjIFPPL8YdZtP8xnLjqBmpBHW4mISHEogBxD6cdPwPCfvxjWBzJtjOLCYct3i4KbPtCq15uYre/jQzsO89YfPshbTh/8rK+uvvAmrPduqeff7+uifNEe3rp6WZ45ExGRiagnkeRTtzzJktnVXHrmkcXOjoiIBKgJ6xhKpAaCo6iAMExYE9b8U/CWLHacGfaIkYFHe3j/hz2uJOjJPS2D3vcmwrdqR0OHv3zzMHMpIiITzXX372R7fQeff9PJzFTto4jIhKIAcgwFa9eGG0CGNWFNC4ZQ4SOtDmtVBREW1qWb0A5uUjp4WnmOh0G39wyucTzU1s2Hf/EoLZ19tHT18eFfPEpTRy9lJV46Cb/Wt7M3wUdvfIyDrd0j2BoRESmW7r4kP3voec46Zi5rjl9Y7OyIiEgGNWEdQ4VqwvqdS0/1XviTgn0bM1qEDjJmTV0j0g3byrAayPS08kANZIkFm7h6Wrv7Br3/5p1b2NHQwWlHzqGzJ8GtT+7n6HnTOXLuNGBg5Ns/PrmfPzyxj/JS4+tvfVHubRIRkQnhJ/fvYPfhLv7rDScVOysiIhJCNZBjaDRNWNPLr1g4g9efcgSQaxCdsNdj24g1V0wcFsAODMDj/R+sgVw8q3rI8m3dGTWQfo2iAbubOgGvGWxjRy8Auxq9aSk/kIyqyRURkYnFOcctj+zhzKPnct4JtcXOjoiIhFAAOYaCTViHGT+SjqvCPja4CevQKG3MH+ORZ2CaOeIqDDwbMj3tFYHmSYtn5X5AdIc/CuvVf3yGmzfsAbwg9Mu3bwbg4Z2HAUiGPINSREQmtit/v5Ht9R383WlLip0VERHJQgHkGBrUhHW4g+j4EVa+weB4ProisxlqcFr4ciGP9vCnvfely6kuLwVg8eyhNZD5KMso21TK9ddADrfcRUSkOB7d1cQN657n5cct4O9OW5r7AyIiUhQKIAvkb8/Vs/yKW/nbc/X90/qSgSasI+wDGfZ8x3xbpo714z7CgtbgpIHnQAam9S84kEa6VnLRzMoR5SOzlrGrL0nSDyB/8dCuyM8+s6+Vz/7uqf6AM1Mq5fjs757imX2tofNve2o/P7p3+whyXXwHW7v5l18+TlfEszULKZFM8elbnugfMVdEJM05x9V/eIbamZV87+2n5RxgTUREikdH6AJ5908eHvQ/eBfMacN+jEdIDWT+fSAnxmM8wqSb3AYHFerxm/q+cOlsPvKKFfzgHacN+dzbI54Dlhkod/Qm+gfTAWjp7COb9123np+t28XBtvDRWg+2dfOzdbt473UPh87/0M8f5XO3bsqa/kT2pT9t5reP7eVPT+8fl/U9saeZmzfs4RM3Pz4u6xORyeNPTx/g8d3N/Ourjmd6pcb3ExGZyHSUHibnHM83drJ8/vT+aZmPiujsTdDalWDD803904Y7CmvU4hPxMR5h4WpUABuWxTOPmcvrTzmCjsCjO3Z88TWYGU0dvfw8S23ivVsbBr2vb+th0/62/vcP7zzM7GnlHL+ohsqyEvY3d1NaYiybO62/9jMRaG68+3Ani2dV0d6ToKHNG5znYGsPPYkk5SUl7G7qpKy0hPkzKgatc0FN/jWom/a3MmdaBYtC+n3Wt/VQVV5CTVU5Oxs6mFFVRnlpCbOqw5+Fls5vWWnJoNdBzjkOdniBemt3H72JVH8wP9ya6q7eJC1dfaF5z8dY14wD7GzoGPQblfHz0PZG5tdUcuyCGcXOikwSh9q6ueLXT3LCohr1fRQRmQQUQA7TLY/s4VO3PMlNl5/FWcfMA+DML/x10DL/+NMNPLCtcdC04dZADjT/HPjcRBlQNCoAGNwvcmhtY1iz1rTpFd7uOK2idCC99PMiy7JXlt8TaDYM8Npv3zfo/fuv3wDAzKoyLjx5Uf/gO89c/er+Zbr6vGac9W09nPuVu7nsnKO5/sGdg/qx/t/9O3GO/gF73uCPjgvw4s/fyc4vvTZrHoPu29LAO378EDAQIAe9+PN3Ujuzkuvfdyav/uY9ANRUlvHUf716SFoHWro59yt384GXH8MHXnYs537lbt5z9nKuyhj+/nt12/jqvV2c/uJ23vKDB2jq7OONLzpiSHr5eO91D7Nu++G8tzdtPAJHgNufPsAHf/YIP373al55okZxHG8fu+kxXn7cAr7y5lOKnRWZJL5422a6+1Jc8/bThtz8EhGRiUdH6mF6dFczAFsOtQP097ULygweYfijsKaFjsKa55X4mPeBzHe5QX0g/VFYQz6dHkwnrG9l5kA5I9HanRjUR3X34a7+151+P8DD/uNA6p49NCh4BNjf3MUD2wZqO29/+sCg+WH7QpgdjQN9ANPry3SwtYf6tp7+9209idDlGtq9Zf72bD1NnV5ad20+NGS5P2880L++powmvcO9MbFuuzfSbbCPbz7SQfpY3wh5am8zABuz9FuVsdXS1Ze1tlwk04PbGvntY3v5wMuPUa21iMgkkVcNpJldCHwLKAV+5Jz7Usb8SuB64HSgEXibc26nP+8zwGVAEviYc+6OfNIcC//5+6cpKTE++9pVXPn7p3nXS5bz9N4WuvqSvGzlAr5z1xZOO2oOf3u2nk9feDzX3L2Nz7/pZO7efIg9TV1c/KIjuPFhrxmlAf92y5P8csPuvNZdyFFSBzVhDXlURvp1vo/bGM36I5cLWTD9aMyw4ogaMbVQAyocbB0Iyq5/cGf/64/f9BinHzWXXz/q1U5uqx860MtPH3yes46ZOzAhI7uX/u86ntrTwsuOm8/iWdWccfRc/v23T/HW1cuoLC+luy9JXzLF/90/sN63/OBBbv7gS6guL+U/fvc00yoHal97EkMHt3l6bws3b9jN2168jJ+te56Hd3jBnJnxf/fvAGDX4U6u+PWTnHbUHLYeaqepo5eGdi+4TO+/AL97fB/gBYJ/3niAbfUdHFc7gwe2NXK4o5fzTljIjoYO3nHWUVx+/QYe2dXEy49b0P/5T/3qCfa1dFNqxklHzGT18jksnFnFF27dxPGLauhJpDj/xIV86OeP8qW/fyFfuC26r+iOhg5+ULeNz73pZG59cj+t3X2cd8JCvnnnFj78ihV8966tfP5NJ1NVXhqZTvrmxLf+uoV7nqvnw+et4BXHL+TBbY3cu6WeT194Aut3HubOTQf5zEUnRqYF8LN1z7PrcCcdPQmuvvjk2D4apqXH8YlfPs7VbzyZGVn6qPUkknT3pZg9rSJ0vnhGc06dSh7b1cR7r3uYpXOq+dCaFcXOjoiI5ClnAGlmpcA1wKuAPcB6M1vrnHsmsNhlQJNzboWZXQJ8GXibma0CLgFOAo4A7jSz4/zP5Eqz4PY2d7GnqYsdDe38/KFdrN95mOcOejWJ566cz71bGvjVI14A8eSeZva1dPOqVbX8088fBWB7YPTI7r5k3sHjSIQ19YweRMdCX48F54YGraGP8QgJbgdqIIcneNH+s8vO5N6t9TS09fYHfDWVZbxgHjywb6Cm7uj50yNH/Ny0f6CGamdjJzsbO3Pm45FAv9bMSDodzN2x8SAA1z2wE4Af3pN9lNbtDR18v24bx9XO4DeP7R00r6Vr6OA/779+A/tbuvnFQ7sGDRTknONn6waCw5vW7+am9UP3z99mrAO82td/+/UjWZddsXBGf3/eumcHanDTASjAg9sb+dF9O6goK6E3kepf/hb/9/TpW54M2frBrvj1kzy04zBvffFSPv7LxwGvue+fnznIbU/tp7M3yfknLuSiFyyOTCe9jyVTjg3PN/He/1vPzi+9lkv/dx0AH3vlSt7ygwcB+Oh5K7MGQ2mf/d3T/a/fd87Rsa0lWbutl7/u2supR83hnWcdFbpMep+dqRrIrEZzTh3/3BZGS2cfW+vb2V7fzrb6DrbVt7Otvp3nGzuZVV3Ody49leqK6BtDIiIyceRTA3kGsNU5tx3AzG4CLgaCJ7uLgav817cA3zWvyu1i4CbnXA+ww8y2+umRR5oFt2zuNB7Y1th/EbyveWDwmz1NXYOW3dfizXt6b0v/tK2HBgZmCWumWkipkCAtLRisRTVTHfs+Z2GP8QiZFvIcyNHEuOesnM85K+cD9AeQT/3Xq6mrqxsUQN76sXNYdeUdoWlc8uJl3LHxQH+zynwFm7X2DrMJZzbPN3aGfldP7mkZ9H7d9kb2+/tlIqO5bHPESLO5PHewLXJ+ZlPdKL2J3GXS1NHLuu1Dfz/p32C6iSx4takw0MT46X0tzJkeXbuV+Vv20hxYX3B7/vTUfpbNnQbA5sNJqjLylfm93L350KCmxXFS3+kVxtaDbaHfH8C+Zq/s1YQ10ojPqS7f/gsj0Nbdx8Z9rSRTrv8vMeh1iq7eJD2JVH9Nc2ev16oikUzRl3Ikkil6Eyka2nvp7E3Q3ZfiUFt3fwsIgPJSY/m86Ry3sIaLTl7EP5x5FEtG+AxgEREpjnwCyCVAsCpjD3BmtmWccwkzawHm+dPXZXw2PcRarjQLbuXCGjp7k/2PXWgPjvaZpabqu3dv7X+9fudA7VNYP7PKspL+R1IAnLxkJjMqywZdEKfNDbkIrggMFJMe0fPclQNNBqeXe1FXsBnhS46ZxwPbGjli9sCImC9ePodbn9rP8nkDo1AumV3N3ubBF9YnLKph84GhwcMxIaNXBkcYPXKed8F9xtFz+qfVzvTW/9Jj5/VPe9nKBTx3sJ050wcuJs8+dj4P7TjMEbMGLhhmVZcPqW1bNLOKA61DH61x2pGzh0w7vram//X5J9Zy5yavBrC6vDQ0nZqqMlbW1oTW0I2X8lKjorSEjt5kf34zpWsw0y65dl3ockBoWeXrxoejy2HtE/si5w/XzsbOyG356h3P9r/O3D+vuXsb19y9bdjrDK4vXbsJ8KnMmtGHs+cLmLSPbCmknz74PD998PnIZRbNHNkIvTExmnNqA2Nk66H2yN9lmIrSEspLjbL0/yUllJcZ82dUMr2ijLnTSzh5yUxWLJzBsQu8v6VzqjVQjojIJGe5bmia2ZuBC51z/+i/fydwpnPuI4FlnvaX2eO/34Z3QrwKWOec+5k//cfAn/yPRaYZSPty4HKA2tra02+66aYRb2wi5djekiKZ8mr2MjfdzKsYm1ZutPe6yGVSznu9aLrR1uulfcT0EroSjuYeR3WZMavSKDHo7HPMrho4Ybb1OspKoLpsoBquvddRYt660xq6Usytsv5RTNvb2+kpncbsSutv0plyjsYux4JpA+k752jImNadcPQmYWZl8PmLju4kzApMa+1xVJZCZSBvrb2O8oz81nemmF9tg2ocM/ObTHllMa96IB9h+e1KOBIpqKmwQdP6MvLb3J1iWrlRUTowraXHUeXnt729nYrq6expSzGjwlg4rYSm7hQ9Se/7Kivx0l1QXUJFKWxrTpFy3gBH6a95ermRSDkqSozKMphdaextT1FdZrT0ePlMt6ZNVwKWlsC0MqOjz1FqkHAD+015KSRS3vv0/mQG86q8faO+yw1qrlxWAtWlRlufN72mwuhMOIKVncF0YPD+WFYCfcnB+3d6n7W+Llx5NbMqjeYeN+izmTL3/ZmV1v89OTewXTUVRluvG7RsZtPm8hJIOq+Ms60v23oztzff+pcSGyiTbJ/JnNfV1UV1dfaakOGsfyrq6upi2rTqnGVQWQpHzyoZdb/vV7ziFY8451aPKpEJaDTnVOdcQ0ZaBTs/diUcO1tSlJj3+yn1//f+vONVZSlUlBrlJd7veiL1B25vb2fGjHg2L8+Hyieayieayie38S6jYp8j86mB3AssC7xf6k8LW2aPmZUBs/A6/kd9NleaADjnrgWuBVi9erVbs2ZNHlmemurq6nhdjLc/l7q6Ooazf5w/dlmZkIZbPnGj8omm8imY0ZxTB9H5cYD2z2gqn2gqn2gqn9ziVkb5tCNZD6w0s6PNrAJvUJy1GcusBd7tv34zcJffV2MtcImZVZrZ0cBK4OE80xQREZlqRnNOFRERKbqcNZB+/4uPAHfgDTn+E+fcRjO7GtjgnFsL/Bi4wR8k5zDeCRF/uZvxBgdIAB92ziUBwtIs/OaJiIhMHKM5p4qIiEwEeT0H0jl3G3BbxrQrA6+7gbdk+ezngc/nk6aIiMhUN5pzqoiISLFpKDQRERERERHJiwJIERERERERyYsCSBEREREREcmLAkgRERERERHJiwJIERERERERyYsCSBEREREREcmLAkgRERERERHJiwJIERERERERyYsCSBEREREREcmLAkgRERERERHJiwJIERERERERyYsCSBEREREREcmLOeeKnYe8mVk98Hyx81FE84GGYmdiAlP5RFP5RFP5RBvv8jnKObdgHNc3qen8qN9vDiqfaCqfaCqf3GJ1jpxUAWTcmdkG59zqYudjolL5RFP5RFP5RFP5yESm/TOayieayieayie3uJWRmrCKiIiIiIhIXhRAioiIiIiISF4UQE4u1xY7AxOcyieayieayieaykcmMu2f0VQ+0VQ+0VQ+ucWqjNQHUkRERERERPKiGkgRERERERHJiwJIERERERERyYsCyAnCzJaZ2d1m9oyZbTSzf/anzzWzv5jZFv//Of50M7Nvm9lWM3vSzE4r7haMDzMrNbPHzOyP/vujzewhvxx+aWYV/vRK//1Wf/7yomZ8HJjZbDO7xcw2m9kmM3uJ9p8BZvYv/m/raTO70cyq4r7/mNlPzOyQmT0dmDbsfcbM3u0vv8XM3l2MbZGpS+fH/Oj8GE3nyGg6Rw6m82M0BZATRwL4V+fcKuAs4MNmtgq4Avirc24l8Ff/PcBFwEr/73Lg++Of5aL4Z2BT4P2XgW8451YATcBl/vTLgCZ/+jf85aa6bwG3O+dOAE7BKyftP4CZLQE+Bqx2zp0MlAKXoP3nOuDCjGnD2mfMbC7wn8CZwBnAf6ZPqiIFovNjfnR+jKZzZBY6R4a6Dp0fs3PO6W8C/gG/B14FPAss9qctBp71X/8QuDSwfP9yU/UPWIr3gz0P+CNgQANQ5s9/CXCH//oO4CX+6zJ/OSv2Noxh2cwCdmRuo/af/u1bAuwG5vr7wx+BV2v/cQDLgadHus8AlwI/DEwftJz+9FfoP50fQ8tE58fo8tE5Mrp8dI4MLxedH7P8qQZyAvKbApwKPATUOuf2+7MOALX+6/SPPW2PP20q+ybwaSDlv58HNDvnEv77YBn0l48/v8Vffqo6GqgH/s9vwvQjM5uO9h8AnHN7gf8BdgH78faHR9D+E2a4+0ys9iUpLp0fs/omOj9G0Tkygs6RedP50acAcoIxsxnAr4GPO+dag/Ocd/sils9dMbPXAYecc48UOy8TVBlwGvB959ypQAcDTSuA2O8/c4CL8S4ijgCmM7RpimSI8z4jE4/Oj+F0fsyLzpERdI4cvjjvL6AAckIxs3K8k+PPnXO/8ScfNLPF/vzFwCF/+l5gWeDjS/1pU9VLgTeY2U7gJrxmOt8CZptZmb9MsAz6y8efPwtoHM8Mj7M9wB7n3EP++1vwTpbafzznAzucc/XOuT7gN3j7lPafoYa7z8RtX5Ii0Pkxks6PuekcGU3nyPzo/OhTADlBmJkBPwY2Oee+Hpi1FkiP2vRuvL4f6env8kd+OgtoCVSrTznOuc8455Y655bjdey+yzn3duBu4M3+Ypnlky63N/vLT9k7Rc65A8BuMzven/RK4Bm0/6TtAs4ys2n+by1dPtp/hhruPnMHcIGZzfHvYl/gTxMpCJ0fo+n8mJvOkTnpHJkfnR/Tit0JU3/eH3AOXlX4k8Dj/t9r8NqU/xXYAtwJzPWXN+AaYBvwFN7IWUXfjnEqqzXAH/3XxwAPA1uBXwGV/vQq//1Wf/4xxc73OJTLi4AN/j70O2CO9p9B5fNfwGbgaeAGoDLu+w9wI15/lz68O/SXjWSfAd7nl9VW4L3F3i79Ta0/nR+HVVY6P2YvG50jo8tH58jB5aHzY8Sf+RsnIiIiIiIiEklNWEVERERERCQvCiBFREREREQkLwogRUREREREJC8KIEVERERERCQvCiBFREREREQkLwogRQLMLGlmj5vZ02b2KzObVuw8ZWNmy83s6TFId42ZnR14f52ZvTnqMyIiMvXpHKlzpAgogBTJ1OWce5Fz7mSgF/hgsTNUBGuAs3MtJCIisaNzpM6RIgogRSLcC6wws9eb2UNm9piZ3WlmtQBm9nL/Tuzj/rwaM1tsZvcE7tCe6y/7fTPbYGYbzey/0isws9eY2WYze8TMvm1mf/SnTzezn5jZw37aF0dl1MxKzeyrZrbezJ40sw/409eYWZ2Z3eKv5+dmZtnWbWbL8S4I/sXfhnP9VbzMzB4ws+260yoiIugcqXOkxJYCSJEQZlYGXAQ8BdwHnOWcOxW4Cfi0v9gngQ87514EnAt0Af8A3OFPOwV43F/2351zq4EXAi83sxeaWRXwQ+Ai59zpwIJAFv4duMs5dwbwCuCrZjY9IsuXAS3OuRcDLwbeb2ZH+/NOBT4OrAKOAV6abd3OuZ3AD4Bv+HeZ7/XTWAycA7wO+FLOAhQRkSlL50idIyXeyoqdAZEJptrMHvdf3wv8GDge+KWZLQYqgB3+/PuBr5vZz4HfOOf2mNl64CdmVg78zjmXTuutZnY53m9uMd6JqgTY7pxLp3cjcLn/+gLgDWb2Sf99FXAksClLvi8AXhi48zkLWInXxOhh59weAH/blgPtEesO8zvnXAp4Jn13WUREYkfnyHA6R0qsKIAUGazLvzPaz8y+A3zdObfWzNYAVwE4575kZrcCrwHuN7NXO+fuMbOXAa8FrjOzr+OdZD8JvNg512Rm1+Gd7KIY8PfOuWfzzLcBH3XO3ZGR9zVAT2BSkpH97oNp2Ag+LyIik5/OkeF0jpRYURNWkdxmAXv91+9OTzSzY51zTznnvgysB04ws6OAg865/wV+BJwGzAQ6gBb/zuRFfhLPAsf4fSoA3hZY5x3ARwN9MU7Nkcc7gH/y7+piZsflaM4Tte42oCbH+kREREDnSJHYUQ2kSG5XAb8ysybgLiDdb+LjZvYKIAVsBP4EXAJ8ysz68JrAvMs5t8PMHgM2A7vxmvXgnOsysw8Bt5tZB94JNu2/gW8CT5pZCV6ToNdF5PFHeM1uHvVPqPXAG7MtnGPdfwBu8Qcl+GjEOkVERK5C50iRWDHnXLHzIBJbZjbDOdfun9CuAbY4574x1dctIiKSi86RIhOTmrCKFNf7/U77G/GaAf0wJusWERHJRedIkQlINZAiIiIiIiKSF9VAioiIiIiISF4UQIqIiIiIiEheFECKiIiIiIhIXhRAioiIiIiISF4UQIqIiIiIiEhe/j9PKAezxbh3IAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_passage_length_analysis(grouped_train_df.story.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAG5CAYAAADPktaCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABXCElEQVR4nO3dd3gc5bn+8fvZXa3KqlkrWy5yL7hhG2xsQwyYEoJJwAmhJiSQQAjJj/Sck84h9Zw0cnLSCAmEBBIgECAGnAAJiGIw1cbGNgb3im3JVZKt+v7+mJG9Fiora4tW+n6uS5d2Z2Znn5l9d3bvfaeYc04AAAAAgJ4vkO4CAAAAAADxIcABAAAAQIYgwAEAAABAhiDAAQAAAECGIMABAAAAQIYgwAEAAABAhiDAodcws5vN7FsJmtcwM6s2s6B/v8LMrknEvP35/cPMrkzU/LrwvN8zs0ozezvO6W80szuTXVcmiX3tzOwqM3s2xc//ATPb7LfPE+KYfq6ZbUlFbUBrZrbCzOYe42NvN7PvJbaiuJ/7VDNb3cH4EWbmzCyUyrp6k3RsP4HeggCHjGBmG8zsoJkdMLO9ZvacmV1nZofbsHPuOufcd+Oc19kdTeOc2+Scy3fONSWg9neEIOfcPOfcH7s77y7WMUzSlyRNdM4NbGM8X/Rb6SmvXSs/kXS93z6XtB7pf6kck4a6EIdE/hjUnXkloo54fuBxzk1yzlV053nSwTn3jHPuuJb78XxuZLJ4wnJ3ti0EXiCxCHDIJOc75wokDZf0P5K+IunWRD9JL/6AGSapyjm3M92FJJJ5+tK2bLikFekuoqdr6T1HcvTi7WRK0D4BdItzjj/+evyfpA2Szm41bKakZkmT/fu3S/qef7tU0sOS9kraLekZeT9Y3OE/5qCkakn/KWmEJCfpakmbJD0dMyzkz69C0n9LelHSfkl/l1Tij5sraUtb9Uo6V1K9pAb/+V6Lmd81/u2ApG9K2ihpp6Q/SSryx7XUcaVfW6Wkb3Swnor8x+/y5/dNf/5n+8vc7Ndxe6vHRVqNr5Y0WNKNkv7qz/OAvOAwI+ZxgyX9zX++9ZI+24XXtN116o+fLek5/zV8TdLcVo/9vqRFft1jJE2S9Lj/eu+Q9PWY9ftVSWslVfnLU9LZ+o3ztbtK0rMxdY2PqWG1pEtixp0naaW/HrdK+nI766XN9iAp26/DSaqRtLaNxz4dM75a0qXy26e83tedkrZL+ljMY7Ll9ept8tfbzZJy26lttKQn/PVYKenPkopbtfsvS1omaZ+keyTldPKe/Jikh2Lm8Zake2Pub5Y0LY71e7uk30ha6C//2W3Uf5Wkdf5rsF7Sh2OGL5L0S7/uNySdFfO4j0la5T9unaRPxozrcP22ev7vS2qSdMh/fX7Z0XL563u3pBNj3m+7/Odsc16tni9H0p3+67VX0kuSyjqo4+f++t4v6RVJp8bM60ZJ9/nz2y/perXx/uho263OtycnSHrVH3ePpLvlb9P98e+TtNRfluckTfGHX+q/noX+/XmS3pbUv416/ijpS/7tIfLeL/+v1foOKGa7ro4/N+LdNt+uVu1THWw/5X2+veyv6x2Sbmq1zbpW0jZ57e3LMY9rd3vnj5+jI9vVzfLa/rX+61jvL99DbdT/jm2LP/wTktb4622BpMHtLP8m//Etny8n+8/9rLztzx5/Hcxr9Xl2q7+MWyV9T1KwnfnfKK993iOv/bwqaWrM+JZ1ckDedvgDMePGSHpK3nu/UtI9/nCT9DN57+v9kpbryPeN90pa4g/fLOnGVvV8VN42vErSt3T0+6DD14g//uL5S3sB/PEXz5/aCHD+8E2SPuXfvl1HAtx/y/simuX/nSrJ2pqXjnwg/klekMlV2wFuq6TJ/jR/k3SnP26u2glw/u0bW6aNGV+hIyHg4/I+AEdJypd0v6Q7WtX2O7+uqZLqJE1oZz39SV4QKvAf+6akq9urs9Vj21qOG+V9yTtPUtBfr4v9cQF5X/JukBT2618n6T1xvqYdrdMh/gfbef7zvNu/3z/msZvkhbaQv7zb5X2JzvHvz/Kn/ZykxZLK5YWV30q6K571G8drd5X8AOcvw2Z5X/ZD8r6MVsrbZVV+faf6t/vJ/1Lexnpptz34452kMR2s16PG+69ro6TvyHsvnCepVlI/f/zP5H3xKvHX20OS/rudeY/xX4tsSf3lfan731bt/kV5X0xL5IWe6zp6T/rLudd/nQfL+9LT8sV5lLwvdoE41u/t8r6AvcufPqdV7RF5X7aO8+8PkjQp5nVslPQFv7ZL/Xm1BP33yvtyb5JO99ffifGs33ba/TWt6upouT4h7wtnnqRHJf2kvXm18Vyf9F/PPHnv3+k6EnLe8VhJV0iK+nV8SV4IagngN8r7kv9+f/3mqo33R0fbbnW8PQn7r33La3CR/3wt2/QT5H2RnuU/9kp/3tn++D/7bSAqL9i8r4P310P+7Q/J+xJ9T8y4v7e1PVT7nxvxbptv19HtM08dbD8lPS/pI/7tfEmzWz3vXfLazvHyAmDLOu5oezdcXoC53F/HUR35ceR2xYTlOLctZ8prqyf6z/ULSU+389iWukMxw67yX+NP+K/pp/zXruWz+gG//oikAfK2LZ9sZ/43+vO6yF+2L8sLhFn++IvlbV8C8t7fNZIG+ePukvQNf1yOpDn+8Pf4r1GxvPf+hJjHzPXXfUDSFHkh+/3+uInyQuoc/7X9iV9bp68Rf/zF+5f2AvjjL54/tR/gFutIj8nhDyB5X6b+rja+6LaeV8wHy6g2hsUGuP+JGT9R3q+VQXU/wP1b0qdjxh3nb+xDMXWUx4x/UdJlbSxX0K9pYsywT0qq8G+/o85Wj29rOW6U9K9Wy33Qvz1L0qZW039N0h/ifE07WqdfUUxo8cc/KunKmMd+J2bc5ZKWtPM8q3R0b8qgeNdvHK/dVToS4C6V9EyraX8r6b/825v816Owk/XSbnvw7x9LgDuoo7847ZTXw2nyvsiMjhl3sqT1cb6G749d7/La/RUx938k6eY43pOb5X0JvEzSLf5rMF5eqFkQ5/q9XdKfOqg1Ii8oflCtehj91/HwF8eYdvCRdub1oKTPdbZ+O2j3sQGuw+Xy7y+Q9+v/MvmBpa15tfFcH1dMT1VHdbTz+D3yezHkvReebjX+RnU9wLW3PTmtjdfgOR3Zpv9G0ndbzXu1pNP928Xy3mPLJf22g3pG68iPAjfLe0+2/GDwR0lfjHld4wlwnW6b22qf6mT7Ke/HkW9LKm01Tcvzjm/1PrvVv93R9u5rkh7ooL6uBrhbJf0o5n6+/1wj2nhsS92tA9yamPt5/jQD5fUU1ynmvSpvO/9kO7XdKP/HAP9+QDE/mrUx/VJJ8/3bf5K33SlvNc2Z8n4EnS0p0Mm6+V9JP/Nv36CYQOYvV72OvA/afY06eg7++Iv960vHjaB3GiJv143WfiyvF+MxM1tnZl+NY16buzB+o7xf+UrjqrJjLb0OsfMOyfsAaxF71shaeR+UrZX6NbWe15Bu1tf6uXP841+GSxrsn1Rmr5ntlfT1VnV3pr11OlzSxa3mPUfeB11bjx0q75f0tgyX9EDMfFbJ232sq+u3M8MlzWpV84flfRmRvOBwnqSNZvaUmZ3cznziaQ9dVeWca4y537KM/eX3BMTU/E9/+DuYWZmZ3W1mW81sv7zd6Vq/B9pblx29J5+S94X5NP92hbyertP9+1Ln61fq4D3snKuRF5auk7TdzB4xs/Exk2x1zrmY+xvlvRYys3lmttjMdvvPe16r5W5v/cYjnuX6nbye6l845+rinK/k7fr3qKS7zWybmf3IzLLam9jMvmxmq8xsn19HkY5ezs62kfFob3syWG2/Bi2GS/pSq/U01H+cnHN7Jd0rbz39tL0nd86tlfejxTR5vcAPS9pmZsfp6PZ2rMvT0eseu/46235eLWmcpDfM7CUze18H8zrcVtXx9q6j7eSxOGpb5ZyrlrenRFc+cw6vP+dcrX8zX95yZMl7r7Ysy2/l9cS15/A6cc41y9u1ueU9/FEzWxozr8k60rb/U96PWS/6Z039uD+PJ+TtVv0rSTvN7BYzK/TnN8vMnjSzXWa2T952pWV+g1vVUuuvlxbxfCYBHSLAIWOZ2UnyPijecRpi59wB59yXnHOjJF0g6YtmdlbL6HZm2d7wFkNjbg+T94tZpbwvA3kxdQV19Bfgzua7Td4GPXbejfJ2yeiKSr+m1vPaGufjO6uztc3yemqKY/4KnHPndWEe7a3TzfJ64GLnHXHO/U879W6WtwtSe3XOazWvHOdcPOulK+tks6SnWj1PvnPuU5LknHvJOTdf3heQB+Ud99CWRLWHeFTK6z2aFFNzkXOuvS+hP5C3To53zhXK2+XO4nmiTt6TLQHuVP/2U3pngOtw/bY8TSc1POqce7e8HwLekBeMWgwxs9hlGSbvi322vN17fyKpzDlXLO84priWu60yWt3vcLnMLF/er/u3SrrRzEo6mNfRT+Rcg3Pu2865iZJOkXcM2UfbeqyZnSrvi+wl8nb/LJa3y1/scrZ+vq5uMzqyXW2/Bi02S/p+q/WU55y7y69/mrwex7sk/V8nz/WUvF3twv524Cl5u2T2k9cz05ZELGvrbVa720/n3FvOucvlbS9+KOk+M4vEPL71tnNbzHzb295tltcDmajlO2pb5dcXVdufOcfy+VInrweyZTkKnXOTOnjM4XXin9iqXN57eLi89/r1kqJ+235dftt2zr3tnPuEc26wvB7ZX7ecbdM593/OuenyeovHSfoP/yn+Iq9nfKhzrkheb25L293uP3dLLbny1kvssh3rZxIgiQCHDGRmhf6vkXfL231neRvTvM/MxvhfBvbJ+3Wr2R+9Q+1/2e/IFWY20czy5O0Odp/zLjPwprxfkd/r/7r9TXn7tbfYIWlEB2dKvEvSF8xspP9l7QfyjslobGf6Nvm1/FXS982swP/Q+qK8XpJ47JAUNbOiOKd/UdIBM/uKmeWaWdDMJvvBOl7trdM7JZ1vZu/x55tj3mUOytuZz8OSBpnZ580s21/+Wf64m+Wtk+GSZGb9zWx+nPV19tq1rmGcmX3EzLL8v5PMbIKZhc3sw2ZW5JxrkHcsVnM78+lue4i7ffu/Uv9O0s/MbIAkmdkQM3tPOw8pkHdsxz4zG6IjX2Y61cl78ilJZ8jbXWqLvBOcnCvvS88Sf5p212+cz19mZvP9L5l1/nLEvgYDJH3Wn+/F8o53WSjvGJZseccZNZrZPEnnxLvcbWj9+nS2XD+X9LJz7hpJj8hrz+3Nq/Uyn2Fmx/s/Ku2X9wNJe9vBAnk/FOySFDKzGyQVxrEs8b4/OvO8//wtr8GF8k7k0eJ3kq7zez7MzCL+NrfAzFpO1vJ1ebvdDjGzT3fwXE/J+zL/tH+/wr//rGv/0jHH+rnRng63n2Z2hZn199+je/3HxLbXb5lZnplNkrfM9/jDO9re/VnS2WZ2iZmFzCzqB994l6/1NHdJ+piZTfN/6PiBpBeccxvaeOwuv/54t03bJT0m6af+Z37AzEab2ekdPGy6mV1oXo/u5+W9zxfL233a+TXIzD4mrwdO/v2LYz5b9vjTNvvvw1n+53qNvOM3W16DAkm7nXOHzGymvOMpW9wn7/PrFDMLy9u9M/aHie58JgGSCHDILA+Z2QF5v159Q9JN8j642jJW0r/kfUl7XtKvnXNP+uP+W9I3zdt94ctdeP475B0n8La8A50/K0nOuX2SPi3p9/J+eayRt+tGi3v9/1Vm9mob873Nn/fT8g66PiTpM12oK9Zn/OdfJ69n8i/+/DvlnHtD3gfyOn/dDO5k+iZ5v+hP8+uulLcO4g2AUvvrdLOk+fK+kO2S95r/h9rZZjnnDsg7ucb5/rzekhcIJO8L8AJ5u+4dkPeBPqut+bShs9eudQ3nyDuOa5tfxw91JMx/RNIG83Y9vE7ebnJt6W57uFHSH/3X8JI4pv+KvF0bF/u1/UvecXdt+ba8Y9X2yQsT93ehrnbfk865N/3hz/j398trw4tavlDHsX47E5D3g8Y2ebtdny7vpAktXvBrrJR3lsaLnHNV/vN+Vt6PI3vkfVFb0IXlbu3nki4ysz1m9n8dLZf/pe7cmDq/KOlEM/twW/Nq47kGyvsyuV/eblpPyWtbbT32UXm7z74pb7e4Q+p8l8m43x+dcc7VS7pQ3nFRu+Xt7np/zPiX5Z3s4pfyXoc1/rSSt03f7Jz7jfN2Mb1C0vfMbGw7T/eUvC/gLQHuWXl7UTzdzvQtz3EsnxttimP7ea6kFWZWLe+1usw5d7DVMqyRd8zsT5xzj/nD293eOec2ydv990vy1vFSeSdfkbwe3on+8j3YTtk3Kmbb4pz7l7wzLP5NXq/TaHntuK3lrZV/5mD/8bM7WUWS11sclncSnz3y2vKgDqb/u7x2s0fe9vZCvxd6pbzdap+XF0KPl3fW2RYnSXrBX9cL5B3fuk7eDxi/8+fXckbJH/uP+bSk7/jr+AbF7FHhnFshb5t9t7z1Ui3vuNiW3Z+785kESDpyph8ASCkzq5DXg/r7dNeCvs3MrpJ3Qo856a4F6IiZjdCRsyt2aS+N3szMbpR3gpUr0l1La/6eFHsljXXOrU9zOegl6IEDAAAAEsTMzvd3cY3IO352ubwzmQIJQYADAAAAEme+vF2it8nbNfsyxy5vSCB2oQQAAACADEEPHAAAAABkiFC6C2ittLTUjRgxIinzrqmpUSQS6XxCoJtoa0gl2htSifaGVKGtIZV6Wnt75ZVXKp1z/dsa1+MC3IgRI/Tyyy8nZd4VFRWaO3duUuYNxKKtIZVob0gl2htShbaGVOpp7c3MNrY3jl0oAQAAACBDEOAAAAAAIEMQ4AAAAAAgQxDgAAAAACBDEOAAAAAAIEMQ4AAAAAAgQxDgAAAAACBDEOAAAAAAIEPEFeDM7FwzW21ma8zsq22Mzzaze/zxL5jZiJhxU8zseTNbYWbLzSwngfUDAAAAQJ/RaYAzs6CkX0maJ2mipMvNbGKrya6WtMc5N0bSzyT90H9sSNKdkq5zzk2SNFdSQ8KqBwAAAIA+JJ4euJmS1jjn1jnn6iXdLWl+q2nmS/qjf/s+SWeZmUk6R9Iy59xrkuScq3LONSWmdAAAAADoW0JxTDNE0uaY+1skzWpvGudco5ntkxSVNE6SM7NHJfWXdLdz7ketn8DMrpV0rSSVlZWpoqKii4sRn+rq6qTNG4hFW0Mq0d6QSrQ3pAptDamUSe0tngDX3fnPkXSSpFpJ/zazV5xz/46dyDl3i6RbJGnGjBlu7ty5SSmmoqJCyZo3EIu2hlSivSGVaG9IFdoaUimT2ls8u1BulTQ05n65P6zNafzj3ookVcnrrXvaOVfpnKuVtFDSid0tGgAAAAD6ongC3EuSxprZSDMLS7pM0oJW0yyQdKV/+yJJTzjnnKRHJR1vZnl+sDtd0srElJ46+w816PWt+9TY1JzuUgAAAAD0YZ0GOOdco6Tr5YWxVZL+6pxbYWbfMbML/MlulRQ1szWSvijpq/5j90i6SV4IXCrpVefcIwlfiiRbuGy73veLZ7XzQF26SwEAAADQh8V1DJxzbqG83R9jh90Qc/uQpIvbeeyd8i4lkLFKImFJUlV1vQYX56a5GgAAAAB9VVwX8u7rovl+gKuhBw4AAABA+hDg4lASyZYk7a6pT3MlAAAAAPoyAlwcWnahJMABAAAASCcCXBwKc0LKCpqqCHAAAAAA0ogAFwczU0kkrN3VBDgAAAAA6UOAi1NJJJseOAAAAABpRYCLUzQS1m7OQgkAAAAgjQhwcSqJhOmBAwAAAJBWBLg4cQwcAAAAgHQjwMUpGgnrQF2j6hqb0l0KAAAAgD6KABenknzvWnB7ahrSXAkAAACAvooAF6doJFuSVMWJTAAAAACkCQEuTlG/B243JzIBAAAAkCYEuDiVRAhwAAAAANKLABenqB/gqjgTJQAAAIA0IcDFqTAnS8GAcQwcAAAAgLQhwMUpEDD1ywuzCyUAAACAtCHAdUE0EmYXSgAAAABpQ4DrgpIIPXAAAAAA0ocA1wUl+QQ4AAAAAOlDgOuC0khYVQQ4AAAAAGlCgOuCkki29h1sUENTc7pLAQAAANAHEeC6oCTfuxbcnlp64QAAAACkHgGuC7iYNwAAAIB0IsB1QYkf4DiRCQAAAIB0IMB1weEeOAIcAAAAgDQgwHXB4R646ro0VwIAAACgLyLAdUFxXlhm7EIJAAAAID0IcF0QDJhK8rgWHAAAAID0IMB1UUkkTA8cAAAAgLQgwHVRSYQeOAAAAADpQYDromh+WFWcxAQAAABAGhDguohdKAEAAACkCwGui0oi2dp7sEFNzS7dpQAAAADoYwhwXRSNhOWctKeWXjgAAAAAqUWA66LDF/NmN0oAAAAAKUaA66JovhfgqqoJcAAAAABSiwDXRdFItiR64AAAAACkHgGui47sQsmlBAAAAACkFgGui/rlZUmSKtmFEgAAAECKEeC6KBQMqDgvi10oAQAAAKQcAe4YcDFvAAAAAOlAgDsG0UhYVRwDBwAAACDFCHDHgB44AAAAAOlAgDsG0fxsAhwAAACAlCPAHYNoJKw9tQ1qbnbpLgUAAABAH0KAOwYlkbCamp32HWxIdykAAAAA+hAC3DFouZg3JzIBAAAAkEoEuGMQjWRLkqq4mDcAAACAFCLAHYOWHjhOZAIAAAAglQhwxyCa37ILJQEOAAAAQOoQ4I5Bvzx64AAAAACkHgHuGIRDARXmhAhwAAAAAFKKAHeMovnZ7EIJAAAAIKUIcMeoJBLWbi4jAAAAACCFCHDHqCQS5jICAAAAAFIqrgBnZuea2WozW2NmX21jfLaZ3eOPf8HMRvjDR5jZQTNb6v/dnOD60yYaCbMLJQAAAICUCnU2gZkFJf1K0rslbZH0kpktcM6tjJnsakl7nHNjzOwyST+UdKk/bq1zblpiy06/kkhYe2rq5ZyTmaW7HAAAAAB9QDw9cDMlrXHOrXPO1Uu6W9L8VtPMl/RH//Z9ks6yXp5qSiJhNTY77T/YmO5SAAAAAPQRnfbASRoiaXPM/S2SZrU3jXOu0cz2SYr640aa2RJJ+yV90zn3TOsnMLNrJV0rSWVlZaqoqOjKMsSturo6YfPeuc0Lbv948hkNjHAoIY6WyLYGdIb2hlSivSFVaGtIpUxqb/EEuO7YLmmYc67KzKZLetDMJjnn9sdO5Jy7RdItkjRjxgw3d+7cpBRTUVGhRM3b3tylW5a9qDGTpmnGiJKEzBO9RyLbGtAZ2htSifaGVKGtIZUyqb3F03W0VdLQmPvl/rA2pzGzkKQiSVXOuTrnXJUkOedekbRW0rjuFt0TRCNhSeJEJgAAAABSJp4A95KksWY20szCki6TtKDVNAskXenfvkjSE845Z2b9/ZOgyMxGSRoraV1iSk+vaL4X4HYT4AAAAACkSKe7UPrHtF0v6VFJQUm3OedWmNl3JL3snFsg6VZJd5jZGkm75YU8STpN0nfMrEFSs6TrnHO7k7EgqVbS0gNXzcW8AQAAAKRGXMfAOecWSlrYatgNMbcPSbq4jcf9TdLfulljj5QdCio/O8QulAAAAABShtMndkNJJMwulAAAAABShgDXDQQ4AAAAAKlEgOuGaCSsqmoCHAAAAIDUIMB1Az1wAAAAAFKJANcN0fxs7a6pl3Mu3aUAAAAA6AMIcN0QjYRV39Ss6rrGdJcCAAAAoA8gwHXDkWvBsRslAAAAgOQjwHVDSb4f4DgODgAAAEAKEOC6Ier3wHEiEwAAAACpQIDrhpLDAa4uzZUAAAAA6AsIcN0QjWRLYhdKAAAAAKlBgOuG3HBQuVlB7eYkJgAAAABSgADXTdF8LuYNAAAAIDUIcN0UjYTZhRIAAABAShDguqkkQg8cAAAAgNQgwHVTSSRbVdWchRIAAABA8hHguima7+1C6ZxLdykAAAAAejkCXDeVRMKqa2xWbX1TuksBAAAA0MsR4LrpyMW8OQ4OAAAAQHIR4Lop6gc4zkQJAAAAINkIcN0Uzc+WJO2u4UQmAAAAAJKLANdNh3vgqumBAwAAAJBcBLhu4hg4AAAAAKlCgOumvHBQ2aEAx8ABAAAASDoCXDeZmaKRMLtQAgAAAEg6AlwClOSHOYkJAAAAgKQjwCVASSSbY+AAAAAAJB0BLgGikTDHwAEAAABIOgJcApREwvTAAQAAAEg6AlwCRPPDqq1v0qGGpnSXAgAAAKAXI8AlwOGLedMLBwAAACCJCHAJUBLJliRVVXMmSgAAAADJQ4BLgBJ64AAAAACkAAEuAVp2odzNxbwBAAAAJBEBLgFK8v0ARw8cAAAAgCQiwCVAQXZIWUFjF0oAAAAASUWASwAz868Fx0lMAAAAACQPAS5BopFsdqEEAAAAkFQEuASJ5ofZhRIAAABAUhHgEqQkElYVZ6EEAAAAkEQEuATxjoEjwAEAAABIHgJcgkQjYVXXNaqusSndpQAAAADopQhwCVISyZbEteAAAAAAJA8BLkFKIt7FvDkODgAAAECyEOASJJrvBTh64AAAAAAkCwEuQaIRAhwAAACA5CLAJUjUPwaOa8EBAAAASBYCXIIU5oYUCpiqquvSXQoAAACAXooAlyBmpn5cCw4AAABAEhHgEigaCbMLJQAAAICkIcAlUAk9cAAAAACSiACXQAQ4AAAAAMlEgEugaCTMSUwAAAAAJA0BLoGi+dnaf6hRDU3N6S4FAAAAQC9EgEugEv9i3nvYjRIAAABAEhDgEijqB7jKagIcAAAAgMSLK8CZ2blmttrM1pjZV9sYn21m9/jjXzCzEa3GDzOzajP7coLq7pFaeuA4kQkAAACAZOg0wJlZUNKvJM2TNFHS5WY2sdVkV0va45wbI+lnkn7YavxNkv7R/XJ7tmi+F+CqajiRCQAAAIDEi6cHbqakNc65dc65ekl3S5rfapr5kv7o375P0llmZpJkZu+XtF7SioRU3IOVRLIl0QMHAAAAIDlCcUwzRNLmmPtbJM1qbxrnXKOZ7ZMUNbNDkr4i6d2S2t190syulXStJJWVlamioiLe+rukuro6afOWpGbnZJJeXfmWRjZsTNrzoOdLdlsDYtHekEq0N6QKbQ2plEntLZ4A1x03SvqZc67a75Brk3PuFkm3SNKMGTPc3Llzk1JMRUWFkjXvFiXPPq6C0oGaO/f4pD4PerZUtDWgBe0NqUR7Q6rQ1pBKmdTe4glwWyUNjblf7g9ra5otZhaSVCSpSl5P3UVm9iNJxZKazeyQc+6X3S28p4rmh7Wbs1ACAAAASIJ4AtxLksaa2Uh5Qe0ySR9qNc0CSVdKel7SRZKecM45Sae2TGBmN0qq7s3hTfLORMkxcAAAAACSodOTmDjnGiVdL+lRSask/dU5t8LMvmNmF/iT3SrvmLc1kr4o6R2XGugropFsVXIWSgAAAABJENcxcM65hZIWthp2Q8ztQ5Iu7mQeNx5DfRmHHjgAAAAAyRLXhbwRv5JIWHtrG9TY1JzuUgAAAAD0MgS4BGu5mPee2oY0VwIAAACgtyHAJVhJxAtw7EYJAAAAINEIcAnWEuCqOJEJAAAAgAQjwCVYNJItiR44AAAAAIlHgEuwlmPgCHAAAAAAEo0Al2D98sIykyqrCXAAAAAAEosAl2DBgKk4N0u7OQYOAAAAQIIR4JKAi3kDAAAASAYCXBJEI9mqYhdKAAAAAAlGgEsCeuAAAAAAJAMBLglK8glwAAAAABKPAJcE0UhYe2rr1dzs0l0KAAAAgF6EAJcE0UhYzU7ae7Ah3aUAAAAA6EUIcElQkp8tSaqq5lICAAAAABKHAJcE0UhYklTFcXAAAAAAEogAlwQlfoDjRCYAAAAAEokAlwT0wAEAAABIBgJcEvRr6YHjYt4AAAAAEogAlwRZwYAKc0LaXcNJTAAAAAAkDgEuSaL52exCCQAAACChCHBJEo2EOYkJAAAAgIQiwCVJSSSsKo6BAwAAAJBABLgkieaH2YUSAAAAQEIR4JKkJBLWntp6NTe7dJcCAAAAoJcgwCVJSSRbTc1O+w81pLsUAAAAAL0EAS5JuJg3AAAAgEQjwCVJScvFvAlwAAAAABKEAJckLQGOM1ECAAAASBQCXJKU5mdLogcOAAAAQOIQ4JKkXyRLklRVXZfmSgAAAAD0FgS4JMkOBVWQHeIkJgAAAAAShgCXRCX5YXahBAAAAJAwBLgkKokQ4AAAAAAkDgEuiaKRMLtQAgAAAEgYAlwSeT1wnMQEAAAAQGIQ4JKoJJKt3TX1cs6luxQAAAAAvQABLomikbAampwO1DWmuxQAAAAAvQABLomi+WFJUlU1x8EBAAAA6D4CXBKVRLwAx3FwAAAAABKBAJdE0Ui2JHrgAAAAACQGAS6JSvJbeuAIcAAAAAC6jwCXRFF/F0quBQcAAAAgEQhwSZSTFVReOEgPHAAAAICEIMAlmXcxbwIcAAAAgO4jwCVZNBJmF0oAAAAACUGAS7JofraqqrmMAAAAAIDuI8AlGbtQAgAAAEgUAlyStexC6ZxLdykAAAAAMhwBLslKImHVNzarpr4p3aUAAAAAyHAEuCQr8a8Ft7ua3SgBAAAAdA8BLsmi+S0X8+ZEJgAAAAC6hwCXZCWRbEniRCYAAAAAuo0Al2TRSEsPHAEOAAAAQPcQ4JLs8C6UHAMHAAAAoJviCnBmdq6ZrTazNWb21TbGZ5vZPf74F8xshD98ppkt9f9eM7MPJLj+Hi8vHFJOVkC7OQYOAAAAQDd1GuDMLCjpV5LmSZoo6XIzm9hqsqsl7XHOjZH0M0k/9Ie/LmmGc26apHMl/dbMQgmqPWNEI9lavnWfmpq5FhwAAACAYxdPD9xMSWucc+ucc/WS7pY0v9U08yX90b99n6SzzMycc7XOuUZ/eI6kPplgPj5npBav260b/v46F/QGAAAAcMzi6Q0bImlzzP0tkma1N41zrtHM9kmKSqo0s1mSbpM0XNJHYgLdYWZ2raRrJamsrEwVFRVdXIz4VFdXJ23eHRkt6b0js/TnFzZp367tumhcOOU1ILXS1dbQN9HekEq0N6QKbQ2plEntLem7MzrnXpA0ycwmSPqjmf3DOXeo1TS3SLpFkmbMmOHmzp2blFoqKiqUrHl35vTTnQofeF13vbhJU8aP1rWnjU5LHUiNdLY19D20N6QS7Q2pQltDKmVSe4snwG2VNDTmfrk/rK1ptvjHuBVJqoqdwDm3ysyqJU2W9PIxV5yhzEzfe/9kHTjUoB8sfEOFOVm6bOawdJcFAAAAIIPEcwzcS5LGmtlIMwtLukzSglbTLJB0pX/7IklPOOec/5iQJJnZcEnjJW1ISOUZKBgw3XTJNJ0+rr++/sByLVy+Pd0lAQAAAMggnQY4/5i16yU9KmmVpL8651aY2XfM7AJ/slslRc1sjaQvSmq51MAcSa+Z2VJJD0j6tHOuMsHLkFHCoYBuvmK6ThzWT5+7e4meeWtXuksCAAAAkCHiOgbOObdQ0sJWw26IuX1I0sVtPO4OSXd0s8ZeJzcc1K1XnaTLblmsa//0iu68ZpamD++X7rIAAAAA9HBxXcgbiVeUm6U/fXymygqz9bE/vKg33t6f7pIAAAAA9HAEuDTqX5CtO66epbxwSB+59UVtrKpJd0kAAAAAejACXJoNLcnTHVfPVGNTs6649QXt2H+o8wcBAAAA6JMIcD3A2LIC3f6xmdpdXa+P3PqC9tbWp7skAAAAAD0QAa6HmDq0WL+7coY2VNXqqj+8pJq6xnSXBAAAAKCHIcD1IKeMLtUvLz9By7fu07V3vKy6xqZ0lwQAAACgByHA9TDnTBqoH31wihatqdLn7lqqxqbmdJcEAAAAoIcgwPVAH5xerhveN1H/XPG2vnb/cjnn0l0SAAAAgB4grgt5I/U+Pmek9h1s0M///ZYKcrL0lXnHKTsUTHdZAAAAANKIANeDff7ssdp3sEG3LVqve1/ZrHdPLNN7jx+kOWNLCXMAAABAH0SA68HMTP91/kSdMX6AHn5tmx5buUP3v7pVBTkhwhwAAADQBxHgejgz0+nj+uv0cf31/cZmLVpbqYXLthPmAAAAgD6IAJdBwqGAzjhugM44bsBRYe7RFW8T5gAAAIA+gACXoY4Kcx84XovWVuqRZdv1GGEOAAAA6LUIcL1AbJir/8DxWrSmUo8sjwlz2SGdPbFM504eqNPH9VdOFmEOAAAAyEQEuF4mHArojPEDdMb4I2Fu4fLtenzVDj2wZKvywkGdMX6A5k0eqDOOG6BINk0AAAAAyBR8e+/FYsNcQ1OzFq+r0j9ef1uPrXhbjyzbruxQQKeP66/zjh+kMycMUGFOVrpLBgAAANABAlwfkRUM6NSx/XXq2P767vzJemnDbv1j+Xb9c8XbemzlDoWDAb1rTFTzjh+kd08oU79IOK751tQ1auveg9q656C27KnVlj0Hvb+9B7V1T63MTKP7RzS6f75G9c8/fHtIca4CAUvyUgMAAAC9CwGuDwoGTLNHRTV7VFT/df4kLdm8V/9Yvl3/eP1tPbl6mYIB0ymjo4ePmauua/QD2pGQtnWvd393Tf1R8w6HAiovztWQfrmaOLFMjU1O6ypr9PCy7dp3sOHwdNmhgEaWRjR6QL5GxwS7kaURdusEAAAA2sE35T4uEDBNH95P04f30zfeO0HLt+7TP15/W/9Yvl3feOD1d0wfDgVU3i9X5f3yNHlIkcr75WpIsXd/aL9cleZnt9mz5pzT7pp6rd1Vo3W7qrV2V7XW7qrR61v36R/Lt6vZHZl2cFGORvXP15gB+Tp/6iCdOKyfzOitAwAAAAhwOMzMNKW8WFPKi/Wf7zlOb7x9QC+u362SSNgLav1yVRppO6DFM+9ofrai+dmaObLkqHF1jU3aWFWrtTu9YLduV43W7qrWPS9t1u3PbdD4gQX68Ozhev+0wSrgOD0AAAD0YQQ4tMnMNGFQoSYMKkz6c2WHghpXVqBxZQVHDa+pa9SC17bpzsUb9a0HX9d/L1yl+dOG6IrZwzRpcFHS6wIAAAB6GgIceqxIdkiXzxymy04aqmVb9unOxRv1wJItuuvFTZo2tFgfnjVM75syWLlhrmsHAACAvoEAhx7PzDR1aLGmDi3WN987Ufcv2aI/v7BJ/3HfMn334ZW6aPpQfWjWMI0ZkJ/uUgEAAICkIsAhoxTlZelj7xqpq04ZoRfX79adL2zSHYs36LZF6zV7VImumD1c50wcqHAokO5SAQAAgIQjwCEjmZlmjYpq1qioKqsn6t6Xt+gvL27U9X9ZotL8sC6ZMVSXzxymoSV56S4VAAAASBgCHDJeaX62PjV3tD552ig9/dYu/fmFTbr5qbX6dcVazRlTqktOGqpzJpYpJ4tj5QAAAJDZCHDoNQIB09zjBmjucQO0be9B3ffKFv315c367F1LVJyXpfdPG6JLTxqakjNrAgAAAMlAgEOvNLg4V589a6yuP2OMnltbpXte3qy/vLBJtz+3QVPKi3TpSUN1/tTBKuS6cgAAAMggBDj0aoGAac7YUs0ZW6o9NfV6cOlW3fPSZn3jgdf13YdX6rzjB+myk4bppBH9ZNb1C5QDAAAAqUSAQ5/RLxI+fAbLZVv26Z6XN+uhpdt0/6tbNao0ootnDNUHpw/RgIKcdJcKAAAAtIkAhz4n9rpy33rvRC1cvl33vLRZP/znG/rJY6t15vgBumTGUE0pL1L//GwFAvTMAQAAoGcgwKFPyw0H9cHp5frg9HKt21Wtv768Rfe9skWPr9whSQoHAxrSL1dDinNV3q/lL09D/NsDCnIUJOABAAAgRQhwgG9U/3x9dd54femccXph3W6tr6rRlj212rLnoLbsOah/rdqpyuq6ox6TFTQNLo4NeHkaUpyr+urmNC0FAAAAejMCHNBKVjBw+MQnrR2sb9LWvQcPBzvvtnf/ydW7tOuAF/ACJm3JekOfPWusskNcfw4AAACJQYADuiA3HNSYAfkaMyC/zfGHGpq0Zc9B/dfdi/SrJ9fqXyt36icXT9Xx5UUprhQAAAC9USDdBQC9SU6WF/A+MSVbt101Q3sP1uv9v16knzy6WnWNTekuDwAAABmOAAckyZnjy/TY50/X+6cN0S+fXKMLfrFIy7fsS3dZAAAAyGAEOCCJivKy9NNLph7VG/fTx+iNAwAAwLEhwAEpENsb94snemZv3P5DDVq8rkr7DjakuxQAAAC0g5OYACnS0ht33vED9bX7l+v9v16kT88drc+cOVbhUOp/S6msrtNL63frhfW79dKG3Vq1fb+anVQSCeuL7x6ny04aqlCQ33gAAAB6EgIckGJnTSjT418o0bcfXqFfPLFGj6/coZ9cPFWThyT3TJVb9tTqRT+svbB+t9btqpEk5WQFdOKwfvrsWWN1XFmBbn9ug7754Ou6c/FG3fC+iTplzDsvpwAAAID0IMABaVCUl6WbLpmm9x4/SF+7f7nm/yqxvXHOOa3dVa0X1+/Ri+ur9NKGPdq696AkqTAnpJNGlOiSGUM1c2SJJg8uOuo5z508UP98/W19f+Eqfej3L+iciWX6+nkTNKI00u26AAAA0D0EOCCNOuqNc87pYEOTauubdLC+6fDt2vpGHaw/Mry2vlG1DU2Hh23ZU6uXN+xRVU29JKl/QbZmjizRtaeN0syRJTqurECBgLVbk5lp3vGDdMb4Abr12fX61ZNrdM7PntbH3jVC1585RgU5WalaPQAAAGiFAAekWUtv3HmTB+nrDyzXBb98VtmhoA42dO1MlWZSblZQ/QuyNfe4AZo1skQnjSzRiGiezNoPbO3JyQrq/50xRhdNL9eP/rlav316nf726hZ9+ZzjdPGMoQp2EAIBAACQHAQ4oIc4e2KZThpRolsXrdfB+kblhkPKCweVFw4qJyt4+HZu1pHhueGg8sIh5WYFlZMVOKag1pmywhz99JKpuvKU4fr2Qyv11fuX6w7/+LhZo6IJfz4AAAC0jwAH9CBFeVn64rvHpbuMNk0pL9Z9152sh5Zt1/8sXKVLb1nsnVFz3gQNLclLd3kAAAB9AucIBxA3M9MFUwfr31+aqy+cPU5PvLFTZ930lH786BuqqWtMd3kAAAC9HgEOQJflhoP63Nlj9cSX5mre5IH61ZNrdcZPKnT3i5tUW0+QAwAASBYCHIBjNrg4Vz+/7AT97VOnaFBRjr56/3LN+N6/9MW/LtWiNZVqanbpLhEAAKBX4Rg4AN02fXg/PfDpd+nFDbv1wKtbtXD5dt3/6lYNLMzR+08YogtPHKJxZQXpLhMAACDjEeAAJEQgYJo9KqrZo6L69vxJenzlDj2wZKt+98w63fzUWk0aXKgLTyzXBVMHq39BdrrLBQAAyEgEOAAJl5MV1PlTB+v8qYNVWV2nBUu36YElW/Xdh1fqBwtX6dSxpbrwxHKdM7FMOVnBdJcLAACQMQhwAJKqND9bH58zUh+fM1Jv7Tig+5ds1YNLtuqzdy1RfnZI8yYP1IUnlmvWyBIFuDh43JqanV7asFurtu9X/3qONQQAoK8gwAFImbFlBfrKueP1H+ccp8Xrq3S/f7zcva9s0ZDiXJ0yOqqJgws1cVChJgwuVGFOVrpL7lGcc3ptyz4tWLpNjyzfph376yRJkSxpb+FGXT5zmIKEYAAAejUCHICUCwRMp4wu1SmjS/Xd+ZP12Mq3tWDpNj3xxk7d+8qWw9OV98vVxEGFmji4UBMGecGuvF+uzPpWSHnj7f166LVteui17dq0u1bhYECnH9dfF0wdrGElefrPvzyvbz74uu56cZO+M3+Spg8vSXfJAAAgSeIKcGZ2rqSfSwpK+r1z7n9ajc+W9CdJ0yVVSbrUObfBzN4t6X8khSXVS/oP59wTCawfQIbLDQc1f9oQzZ82RM457TxQp5Xb92vltv1auX2/Vm3fr8dX7ZDz9xIsyAkdDnMt4W5sWb6yQ73rWLoNlTVeaFu2TW/uqFYwYDpldFSfOXOMzpk0UEW5R3onvzozR9Ulx+n7j6zSB3/zvD54Yrm+Mu84DSjISeMSAACAZOg0wJlZUNKvJL1b0hZJL5nZAufcypjJrpa0xzk3xswuk/RDSZdKqpR0vnNum5lNlvSopCGJXggAvYOZqawwR2WFOTrjuAGHh9fWN+qNtw9oVUywu+elzTrY0CRJCgVM5f1yFQ4FFAwEFAqYggE78j9obQ8PxAwPtjO85X6wneEBUzgUUHFeWCV5YfWLZKkkElZuVrDLPYXb9h7UI8u2a8Fr27R86z5J0swRJfru/Emad/wglea3ffZOM9P5UwfrzPED9Msn1+j3z6zTYyve1uffPU4fPXm4soJc8hMAgN4inh64mZLWOOfWSZKZ3S1pvqTYADdf0o3+7fsk/dLMzDm3JGaaFZJyzSzbOVfX7coB9Bl54ZBOHNZPJw7rd3hYU7PTxqoardp+QCu379PGqlo1NTs1NruY/81qbHKqa2hWY3PT0cNbpmtyamxuVlOzjh7u/z/Wi5FnhwIqiYTVLy+skkhYxXlZR93vF/ECX1FulpZu3qOHXtuuFzfsliRNKS/SN86boPdOGaTBxblxP2ckO6SvnDteF08v140PrdR3H16pe17apBsvmKRTRpce03IAAICexZzr+MuJmV0k6Vzn3DX+/Y9ImuWcuz5mmtf9abb499f601S2ms91zrmz23iOayVdK0llZWXT77777m4vWFuqq6uVn5+flHkDsWhrvYdzTs1OanI66r/359TkpPomqabBqbrB6UC9U3W9U3WDvNsxww40ONU2SG1tdQfnm2YPCmnmwJAGRrrWY9ZWe3POacnOJv3ljXpVHnSaOTCoy8aHVZJDbxy6h+0bUoW2hlTqae3tjDPOeMU5N6OtcSk5iYmZTZK3W+U5bY13zt0i6RZJmjFjhps7d25S6qioqFCy5g3Eoq2hPY1Nzdp3sEF7ahu0p7Zeu2vqNTyap+PKCo755CzttbczJH26oUk3P7VWv6lYq+WL6nX9mWN0zakje90xg0gdtm9IFdoaUimT2ls8AW6rpKEx98v9YW1Ns8XMQpKK5J3MRGZWLukBSR91zq3tdsUAkMFCwYCi+dmKtnM8W6LlZAX1+bPH6YMnluu7D6/Ujx9drfte2aL/On+i5sYcZ9hdzjnVNTarpq5RtfVNqqlvVE1dkw76t2v9+7H/DzY0qb6xWfWNzWpo8h5f39Ss+kZ/eFPz4fEt9+v8207S5MGFmj0qqtmjopo+vJ8i2ZxYGQDQ+8XzafeSpLFmNlJeULtM0odaTbNA0pWSnpd0kaQnnHPOzIolPSLpq865RQmrGgDQJUNL8nTLR2foqTd36dsLVuiqP7yksyeU6cITh6iusUmHGpp1qCHmf2OT6g4P84c3Nh01TV1js2rrG1Vb54W0rhwumJsVVE5WQOGQ/xcMKBwKKhwKKDsYUF44pOLDw1tPF1Bjk9OSzXt0y9Pr9OuKtQoFTMeXF2nWyKhmjyrRjBElyifQAQB6oU4/3ZxzjWZ2vbwzSAYl3eacW2Fm35H0snNugaRbJd1hZmsk7ZYX8iTpekljJN1gZjf4w85xzu1M9IIAADp3+rj++ufnT9Nti9br//79lv61akeb04WDAWVnBZTjB62cUPDw7YKckErzs5WTFVAkHFJedvDo/+GgItlH/88LhxQJB5WXHVJuVjBhFxyvqWvUKxv36IX1VVq8brdufXadbn5qrYIB0+QhRZo9skSzR0U1Y0Q/FXBheABALxDXz5POuYWSFrYadkPM7UOSLm7jcd+T9L1u1ggASKBwKKDrTh+tS2cM1dv7D70jpGWHAgokKGAlWyQ7pNPG9ddp4/pL8i458erGvX6gq9Jti9brt0+vU8DkBbpRUc0aWaJZo6L00AEAMhKfXgDQR/XzL2fQm+SFQ5oztlRzxnqXTThY36Qlm/Zo8frdWryuSrcv2qBbnl6ngpyQrpkzSh+fM4KeOQBARiHAAQB6rdxwUKeMKdUpY7xAd6ihSa9u3KPbn9ugn/3rTf3hufW69rRRuuqUEcoL85GYDM457TxQJ5M0oDAn3eUAQMbj0woA0GfkZB0JdMu27NVNj7+pH/1ztW59Zr0+NXe0rpg9XDlZveMSC8451Tc162B9k2r9PzOpNJKtwtzQMV+2oj2HGpq0sapWa3dVa92uaq3dVePfrlF1XaNCAdMX3j1O150+OmHHQAJAX0SAAwD0SVPKi3X7x2bqlY17dNPjq/W9R1bplqfX6fozx+jSk4b2qGvl7amp19LNe7V86z7trW3QwYbGw6HsUMORgHawvtH/36TahiY1tXNq0KygKRrJVmlBWKX52TF/Mff9cf3ywocDl3NO++ucXlhXpXWVNVq7s1pr/bC2ZU/tUWciHVyUo9ED8nXR9HKN7h/RC+t368ePrtZTq3fppkunqrxfXipWHQD0OgQ4AECfNn14P/35mtlavK5KNz32pm74+wrdXLFWnzlrrC6aXq6sYCCl9TQ0NWvV9v1aunmvlmzaq6Wb92p9Zc3h8ZFwULnhlrN7BpXr/++XF37HsLywd9bPlmHOSZXVdaqqqVflgTpVVtepsrpeq98+oMrqOjU0vTPwBUwqiYTVLy+snQfqtO9gg/TkYklSdiigUf3zNaW8SB84YYhG9Y9odP98jeofeccuqVfMHq4zxw/QDX9foXk/f0bfe/9kzZ82JLkrEwB6IQIcAACSZo+K6p5PztYzb1Xqp4+/qa/dv1y/qVirz541Vu+fNlihJAQ555y27zukJZv2asmmPYd72eoamyVJ/QuydcLQYl0yY6imDS3WlPKipF2w3Dmn/Qcbtau6TlV+sPMCnve3u6ZeM0eWyO17W+85ZZpG949ocFFu3GcsNTNdeGK5Zgwv0efvWaLP3b1UFat36dvzJ6mQE8kAQNwIcAAA+MxMp43rr1PHluqJN3bqpsff1JfvfU2/rlijz501VudPGXxMl1hwzqm2vkkHDjVqQ1WN37u2R0s27dXOA3WSvMs7HD+kSB+ZPVzThhXrhGH9NLgoJ+HHqrXHzFSUl6WivCyNGZDf7nQVFVU63b9sw7EYFs3TXz95sn755Br94ok1emnDbv3vpdM0Y0TJMc8TAPoSAhwAAK2Ymc6aUKYzxw/Qoyve1s8ef0ufu3upfvXkGn3mzLEqK8zR/oMN2n+owf/fePj+gUON/vDGo8a3Ph5tRDRP7xpTqmlDi3XCsGKNH1iocCi1u2umSygY0OfPHqdTx/bX5+9Zokt++7yuP2OMPnPW2JTvsgoAmYYABwBAO8xM504epHMmDtTDy7frf//1pj5z15I2p80LB1WYk6XC3JAKcrJUmh/WqP6Rw8O8/1kaWJijqUOLVdLLrsF3LKYP76eFnz1VNy5Yqf97Yo2efqtS/3vpNI0ojSTsOeobm7WxyjsTZm19k2pa/tc3qrbO/x87vK5RBxuO3K+tb9KJw4r1ydNHa8KgwoTVBQDHigAHAEAnAgHTBVMH67zJA7V43W5JOiqUFeSE6Dk6RgU5WfrpJVM197j++sYDy3Xe/z2jGy+YpIunlx/T7qONTc16fdt+Pbe2Us+vrdLLG/boYENTh4/JyQooEg4pLzvo/Q8HFckOqTQ/W6Gg6fGVO/Tg0m2ae1x/XXf6aM0aWZKyXVsBoDUCHAAAcQoFA5oztjTdZfRK508drOnD++kL9yzVf963TBWrd+oHHzhexXkd91Q2Nzutenu/nl9bpefXVumF9btVXdcoSTqurECXnjRUJwwrVmFulvKyvGDWEtBaztTZ2XXp9tU26I7FG/SHRRt02S2LNW1osa47fbTOmVh2TMdEAkB3EOAAAECPMLg4V3/5xGzd8vQ6/fSx1Xp1417ddOlUnTL6SGh2zmnNzmo9v65Kz62p0uL1Vdpb2yBJGlka0QXTBuvkUVHNHhVV/4LshNRVlJel688cq2tOHaV7X96sW55Zp+vufEWj+0f0ydNGa/4Jg3vUdQMB9G4EOAAA0GMEA6ZPzR2tOWNK9bm7l+jDv39Bnzh1lEaWRvSc38tWWe2duXNIca7ePaFMJ4+O6uTRUQ0qyk1qbTlZQX3k5BG6fOYwLXz9bd1csVb/+bdl+unjq3X1nJH60Kzhyk/SZR4AoAVbGQAA0OMcX16khz87R997ZJVueXqdJGlAQbbmjPHC2imjSzW0JC8ttYWCAV0wdbDOnzJIz7xVqZufWqsfLHxDv3xijT5y8nBddcrIhPX+AUBrBDgAANAj5YVD+sEHjteHZg5TbjioUaWRHnXykJbrBp42rr9e27xXNz+1Vr+uWKvfPbNeF08v17WnjdLwaOLOqBnLOaf9hxq1c/8h7dhfpx37D2nHgUPaub9OOw94w2rqGjW2rEBThhTp+PIiTRpcqAIumg5kPAIcAADo0SYPKUp3CZ2aOrRYv7liutbtqtbvnlmne1/eorte3KTTxvVXNJKtcMiUFQwc/gsH/fuhVvdbhgW8+9V1jV4421/nBzTv9s4Dh3SoofkddRRkhzSgMFtlhTkqzs3Vqxv36KHXth0eP6p/RFOGFGnykCJNKS/WpMGFirDbJ5BReMcCAAAkyKj++frvC6foC2eP062L1uvxlTv01o5qNTQ1+39O9U3Nqm98Z/jqSF44qIGFORpQmK0ThhWrrDBHAwqyNaAwR2UFXmAbUJitvPA7v9pVVtdp+dZ9en3LPi3buk+L1+3Wg0u9UGcmje6ff7iX7vghRZo4uLDN+bSnqdmpvtFbprqmJtU3essZMClgJvP/e39ez2UgZpgFdHhcy/QNTU5765q1qapWtQ3eNfkO+tflq61vPHz7YENTzG1vOuekD80aptmjol1ax0CmIMABAAAk2IDCHH1t3gR9bd6ENsc759TU7A4HusMBr9G739jsBaL87JAGFOZ06+QopfnZOuO4ATrjuAGHh+08cEivb92nZVv2afmWfXpmTaXuX7JVkhQwaeyAAkXzw14w8wNnfWOz6lrdr29qVlOzO+baOvXkk51OEjBvd9vccFB54aAOHGrUgte26dxJA/W188YnbTdWIF0IcAAAAClmZgoFTaGglKvUX4JgQEGOzhyfozPHlx0etmP/IS/Qbd2n5Vv2qrquUdlZARXkhBRu2dUzFFB2KKCwfzscCigcDMbctsPTOic5Sc3OyTmnZufdbnZegG1uPjLMxYxrdk6hgGnLxnWaOmmC8sJBL5xlBY8KarlZ3vDsUOCoYyMPNTTp98+s068r1urfN+3QVaeM0PVnjlVRLsf/oXcgwAEAAEBlhTl698QcvXtiWecTp0BFxWbNnV7e5cflZAV1/ZljdcmMofrJY6v1+2fX62+vbtUXzh6ry2cOUygYSEK1QOrQggEAANDrDCjM0Y8umqqHrp+jcWX5+tbfV2jez59Rxeqd6S4N6BYCHAAAAHqtyUOKdNcnZuu3H5mu+qZmXfWHl3TlbS/qrR0H0l0acEwIcAAAAOjVzEzvmTRQj3/hdH3zvRP06qY9Ovfnz+hbD76u3TX1CXueg/7ZMpN6Yhf0eRwDBwAAgD4hHAromlNH6cITy/W//3pTf35hkx5culWfPXOsrjxlhMKhjvs2nHPaU9ugDVU12lhVow2Vtdq0u9a/X3tUGAwGrNXJXvwTwMTcb327ND9bk4d4l3MYMyBfwUDPuXA9eg4CHAAAAPqUkkhY35k/WR+ZPVzfX7hK31+4Sne+sFFfmzdB50ws067qOm2o9ELZxt012lBVq41VNdpYWasDdY2H52MmDS7K1fBont4zqUzl/fJkpiOXWWh9GYbYSzD4f9V1jYdvv73/kG5/boMkKTcrqEmDC/2LrnuhblR/Qh0IcAAAAOijxpYV6PaPzdRTb+7S9x5eqevufEXhYED1TUcutB4MmIb2y9WwaEQnDuun4dGIRkTzNDyap/J+ecrJStxlIJqandZXVsdczmGf7nlp8+FQlxcOavLgosOhbvKQIo0qjShwjKGuudnpUKO326d3eQaiQSbgVQIAAECfdvq4/nrX507Vfa9s0Vs7qzUimqdhflAbXJyrrBRdeiAYMI0ZUKAxAwp04YneJRSamp3W7vJCnXfx9b36y4sbddsiL2TmZ4c0aXChJg4uVFYwoEMNXiA72NDk3T58v/mocQcbmlTfeCSohkMBnTOxTBdNL9epY/vT09eDEeAAAADQ54WCAV02c1i6y3iHYMA0rqxA48oKdJF/XbzGpmatOSrU7dNdL26S5O16mZsVVE7Lxc79C6CXRLwLn+dmBd4xPjcc1Nqd1fr7a9v08LLtKivM1oUnluui6eUa3T8/acvmnNOm3bWqa2zWuLKCpD1Pb0OAAwAAADJIKBjQ+IGFGj+wUJfMGJqw+X79vRP0xKqduu+VLbrl6XX6TcVanTisWBdNH6r3TR2kwpysbj/Hjv2H9NzaSj23pkrPra3S1r0HFQqYHvx/79LkIUUJWIrejwAHAAAAQNmhoOYdP0jzjh+knQcO6cElW3Xvy1v09QeW69sPrdC5kwfqounlOmV0ady7WO6trdfidV5YW7SmUmt31UiSivOydPKoqK49bZR+9eQafeGepXroM3MSekxhb0WAAwAAAHCUAQU5uva00frEqaO0bMs+3ffKFv196Vb9fek2DS7K0YUnluuD08s1sjRy1ONq6xv10oY9em5NpRatrdSKbfvlnHcClpkjS3TpSUN1yuhSTRxUePjkKyNKI7rythf140dX61vvm5iOxc0oBDgAAAAAbTIzTR1arKlDi/WN907Qv1bt0H2vbNGvK9bol0+u0Ukj+umCaUNUeaBOz6+t0pLNe9TQ5JQVNJ0wrJ8+f9Y4nTImqqnlxe1eZ+/0cf31kdnDdeuz63XW+AE6ZUxpipcysxDgAAAAAHQqJyuo900ZrPdNGay39x3SA0u26t5XNutbD74uM+n4IUW6es4onTI6qpNGlCg3HP/ukF87b7yeXVOpL9/7mv7x+dNUlNv94+16KwIcAAAAgC4ZWJSjT80dretOH6U1O6s1oCBHRXnHHrrywiHddMlUXXTz8/r2ghW66dJpiSu2l0nNRS0AAAAA9DpmprFlBd0Kby1OGNZP/++MMbp/yVYtXL49AdX1TgQ4AAAAAD3CZ84coynlRfr6A8u1c/+hdJfTIxHgAAAAAPQIWcGAbrpkmg7WN+k//7ZMzrl0l9TjEOAAAAAA9BhjBuTra/PGq2L1Lv3lxU3pLqfHIcABAAAA6FE+evIIzRlTqu89vErrK2vSXU6PQoADAAAA0KMEAqYfXzxFWUHTF/+6VI1NzekuqccgwAEAAADocQYV5eq775+sJZv26uan1qa7nB6DAAcAAACgR5o/bYjOnzpY//uvt7R8y750l9MjEOAAAAAA9FjfnT9J0fywvvDXpTrU0JTuctKOAAcAAACgxyrOC+vHF03Vmp3V+uE/30h3OWlHgAMAAADQo502rr+uPHm4/rBogxatqUx3OWlFgAMAAADQ43113gSN6h/Rl+99TfsONqS7nLQhwAEAAADo8XLDQf3skmnaeaBO//X319NdTtoQ4AAAAABkhKlDi/WZM8fowaXb9Miy7ekuJy0IcAAAAAAyxv87Y4ymlhfpGw8u1479h9JdTsoR4AAAAABkjKxgQDddOk2HGpr0H/ctk3Mu3SWlFAEOAAAAQEYZ3T9fXz9vgp5+c5fufWVLustJKQIcAAAAgIzzkdnDNaW8SL+pWKvm5r7TC0eAAwAAAJBxzEzXnDpK6ytr9O83dqa7nJQhwAEAAADISOdNHqghxbn63TPr0l1KyhDgAAAAAGSkUDCgj71rhF5cv1vLtuxNdzkpEVeAM7NzzWy1ma0xs6+2MT7bzO7xx79gZiP84VEze9LMqs3slwmuHQAAAEAfd+lJQ5WfHdLvn1mf7lJSotMAZ2ZBSb+SNE/SREmXm9nEVpNdLWmPc26MpJ9J+qE//JCkb0n6csIqBgAAAABfQU6WLjtpqB5Zvl1b9x5MdzlJF08P3ExJa5xz65xz9ZLuljS/1TTzJf3Rv32fpLPMzJxzNc65Z+UFOQAAAABIuI/NGSlJun1R7++FC8UxzRBJm2Pub5E0q71pnHONZrZPUlRSZTxFmNm1kq6VpLKyMlVUVMTzsC6rrq5O2ryBWLQ1pBLtDalEe0Oq0NbQVTMGBHTn8+t1YvYO5YasS4/NpPYWT4BLOufcLZJukaQZM2a4uXPnJuV5KioqlKx5A7Foa0gl2htSifaGVKGtoatKxuzVBb9cpK3Zw3XNqaO69NhMam/x7EK5VdLQmPvl/rA2pzGzkKQiSVWJKBAAAAAAOjOlvFgzR5boD4s2qLGpOd3lJE08Ae4lSWPNbKSZhSVdJmlBq2kWSLrSv32RpCecc33ncugAAAAA0u6aOSO1de9B/eP1t9NdStJ0GuCcc42Srpf0qKRVkv7qnFthZt8xswv8yW6VFDWzNZK+KOnwpQbMbIOkmyRdZWZb2jiDJQAAAAB029kTyjSyNKLfP7NOvbU/Ka5j4JxzCyUtbDXshpjbhyRd3M5jR3SjPgAAAACISyBg+vickfrWg6/r5Y17dNKIknSXlHBxXcgbAAAAADLBRSeWqzgvS797el26S0kKAhwAAACAXiM3HNQVs4br8VU7tL6yJt3lJBwBDgAAAECv8tFThisrENBtz/a+C3sT4AAAAAD0KgMKcjR/2mDd+8pm7a2tT3c5CUWAAwAAANDrXHPqKB1qaNafX9iU7lISigAHAAAAoNc5bmCBTh1bqtuf26C6xqZ0l5MwBDgAAAAAvdInTh2lXQfqtGDptnSXkjAEOAAAAAC90qljSzV+YIFufXZ9r7mwNwEOAAAAQK9kZrp6zki98fYBPbumMt3lJAQBDgAAAECvdcG0wepfkK3fPdM7LilAgAMAAADQa2WHgrry5OF6+s1dWv32gXSX020EOAAAAAC92odnDVdOVkC3Prsu3aV0GwEOAAAAQK/WLxLWxdOH6sEl27TzwKF0l9MtBDgAAAAAvd7H54xUQ3Oz7nh+Y7pL6RYCHAAAAIBeb2RpRGdPKNOdizfqYH3mXtibAAcAAACgT/jEqaO0p7ZBf3t1S7pLOWYEOAAAAAB9wkkj+mlqeZFue3a9mpsz88LeBDgAAAAAfYKZ6ZpTR2ldZY3+/cbOdJdzTAhwAAAAAPqMeZMHakhxrn73TGZeUoAABwAAAKDPCAUD+ti7RujF9bu1bMvedJfTZQQ4AAAAAH3KpScNVUF2SL9/Zn26S+kyAhwAAACAPqUgJ0uXzRyqR5Zv1879mXVh71C6CwAAAACAVLvm1FE6d/JADSjM0cp0F9MFBDgAAAAAfU5ZYY7KCnPSXUaXsQslAAAAAGQIAhwAAAAAZAgCHAAAAABkCAIcAAAAAGQIAhwAAAAAZAgCHAAAAABkCAIcAAAAAGQIAhwAAAAAZAgCHAAAAABkCAIcAAAAAGQIAhwAAAAAZAgCHAAAAABkCAIcAAAAAGQIAhwAAAAAZAgCHAAAAABkCHPOpbuGo5jZLkkbkzT7UkmVSZo3EIu2hlSivSGVaG9IFdoaUqmntbfhzrn+bY3ocQEumczsZefcjHTXgd6PtoZUor0hlWhvSBXaGlIpk9obu1ACAAAAQIYgwAEAAABAhuhrAe6WdBeAPoO2hlSivSGVaG9IFdoaUilj2lufOgYOAAAAADJZX+uBAwAAAICMRYADAAAAgAzRKwOcmZ1rZqvNbI2ZfbWN8dlmdo8//gUzG5GGMtELxNHWvmhmK81smZn928yGp6NO9A6dtbeY6T5oZs7MMuJ0yOh54mlrZnaJv31bYWZ/SXWN6D3i+CwdZmZPmtkS//P0vHTUicxnZreZ2U4ze72d8WZm/+e3xWVmdmKqa4xHrwtwZhaU9CtJ8yRNlHS5mU1sNdnVkvY458ZI+pmkH6a2SvQGcba1JZJmOOemSLpP0o9SWyV6izjbm8ysQNLnJL2Q2grRW8TT1sxsrKSvSXqXc26SpM+nuk70DnFu274p6a/OuRMkXSbp16mtEr3I7ZLO7WD8PElj/b9rJf0mBTV1Wa8LcJJmSlrjnFvnnKuXdLek+a2mmS/pj/7t+ySdZWaWwhrRO3Ta1pxzTzrnav27iyWVp7hG9B7xbNsk6bvyfpQ6lMri0KvE09Y+IelXzrk9kuSc25niGtF7xNPenKRC/3aRpG0prA+9iHPuaUm7O5hkvqQ/Oc9iScVmNig11cWvNwa4IZI2x9zf4g9rcxrnXKOkfZKiKakOvUk8bS3W1ZL+kdSK0Jt12t78XT2GOuceSWVh6HXi2baNkzTOzBaZ2WIz6+gXbaAj8bS3GyVdYWZbJC2U9JnUlIY+qKvf7dIilO4CgL7AzK6QNEPS6emuBb2TmQUk3STpqjSXgr4hJG8Xo7ny9ix42syOd87tTWdR6LUul3S7c+6nZnaypDvMbLJzrjndhQHp0Bt74LZKGhpzv9wf1uY0ZhaS1x1flZLq0JvE09ZkZmdL+oakC5xzdSmqDb1PZ+2tQNJkSRVmtkHSbEkLOJEJjkE827YtkhY45xqcc+slvSkv0AFdFU97u1rSXyXJOfe8pBxJpSmpDn1NXN/t0q03BriXJI01s5FmFpZ3sOuCVtMskHSlf/siSU84rmiOruu0rZnZCZJ+Ky+8cYwIuqPD9uac2+ecK3XOjXDOjZB3zOUFzrmX01MuMlg8n6MPyut9k5mVytulcl0Ka0TvEU972yTpLEkyswnyAtyulFaJvmKBpI/6Z6OcLWmfc257uotqrdftQumcazSz6yU9Kiko6Tbn3Aoz+46kl51zCyTdKq/7fY28AxkvS1/FyFRxtrUfS8qXdK9/npxNzrkL0lY0Mlac7Q3otjjb2qOSzjGzlZKaJP2Hc449WdBlcba3L0n6nZl9Qd4JTa7ih3ccCzO7S96PT6X+MZX/JSlLkpxzN8s7xvI8SWsk1Ur6WHoq7ZjR/gEAAAAgM/TGXSgBAAAAoFciwAEAAABAhiDAAQAAAECGIMABAAAAQIYgwAEAAABAhiDAAQAyjpk1mdlSM3vdzO41s7w01DDXzE6JuX+dmX3Uv327mV2U6poAAL0fAQ4AkIkOOuemOecmS6qXdF08DzKzRF7/dK6kwwHOOXezc+5PCZw/AADvQIADAGS6ZySNMbOImd1mZi+a2RIzmy9JZnaVmS0wsyck/dvM8s3sD2a23MyWmdkH/enOMbPnzexVv1cv3x++wcy+7Q9fbmbjzWyEvND4Bb8n8FQzu9HMvty6ODObbmZPmdkrZvaomQ1K2ZoBAPQ6BDgAQMbye9TmSVou6RuSnnDOzZR0hqQfm1nEn/RESRc5506X9C1J+5xzxzvnpkh6wsxKJX1T0tnOuRMlvSzpizFPVekP/42kLzvnNki6WdLP/J7AZ9qpL0vSL/znni7pNknfT+AqAAD0MYnclQQAgFTJNbOl/u1nJN0q6TlJF8T0guVIGubfftw5t9u/fbaky1pm5JzbY2bvkzRR0iIzk6SwpOdjnu9+//8rki7sQp3HSZos6XF/vkFJ27vweAAAjkKAAwBkooPOuWmxA8xLSB90zq1uNXyWpJpO5mfyQt7l7Yyv8/83qWufnSZphXPu5C48BgCAdrELJQCgt3hU0mf8ICczO6Gd6R6X9P9a7phZP0mLJb3LzMb4wyJmNq6T5zsgqaCTaVZL6m9mJ/vzzTKzSZ0uCQAA7SDAAQB6i+9KypK0zMxW+Pfb8j1J/fxLELwm6Qzn3C5JV0m6y8yWydt9cnwnz/eQpA+0nMSkrQmcc/WSLpL0Q/+5lirmzJUAAHSVOefSXQMAAAAAIA70wAEAAABAhiDAAQAAAECGIMABAAAAQIYgwAEAAABAhiDAAQAAAECGIMABAAAAQIYgwAEAAABAhvj/onQDk+RrjfIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_answer_span_text_percentile(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] Train, Validation and Test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the train dataframe is split into an actual train and a validation dataframes.\n",
    "\n",
    "The split is performed as follows:\n",
    "1. The random seed is set to $42$ for reproducibility purposes.\n",
    "2. The train proportion of the actual training dataset to the original dataset is of $0.8$.\n",
    "3. The train dataframe is shuffled and divided into the two new dataframes making sure that no conversation is split among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from typing import Tuple\n",
    "\n",
    "def train_validation_split(df: pd.DataFrame, train_size: int = .8, random_seed: int = 42) \\\n",
    "    -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\" Get train and validation dataframes by shuffling and splitting an original dataframe according to a given proportion\n",
    "    and a specific random seed.\n",
    "    \n",
    "    Note: The order of the rows of the same conversation is preserved in the shuffle. Moreover, the conversations are never\n",
    "    split across the two resulting dataframes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The dataframe from which the train and validation dataframes are obtained.\n",
    "    train_size : int, optional\n",
    "        The proportion of the train split. Defaults to 0.8.\n",
    "    random_seed : int, optional\n",
    "        The random seed for the shuffle. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: _description_\n",
    "    \"\"\"\n",
    "    # Get indices of train and test rows in the dataframe\n",
    "    group_shuffle_split = GroupShuffleSplit(n_splits=2, train_size=train_size, random_state=random_seed)\n",
    "    train_ix, test_ix = next(group_shuffle_split.split(df, groups=df.id))\n",
    "\n",
    "    train_df = df.loc[train_ix]\n",
    "    train_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    val_df = df.loc[test_ix]\n",
    "    val_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_validation_split(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tail of the obtain dataframe (`train_df`) and the head of the validation dataframe (`val_df`) are shown below to assert that the conversations are not splitted and that their question-answer pairs are still chronologically ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataframe shape after the split: (85824, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>story</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_span_start</th>\n",
       "      <th>answer_span_end</th>\n",
       "      <th>answer_span_text</th>\n",
       "      <th>answer</th>\n",
       "      <th>turn_id</th>\n",
       "      <th>history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85819</th>\n",
       "      <td>31qnsg6a5rtt5m7pens7xklnbwf87b</td>\n",
       "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
       "      <td>Who was a sub?</td>\n",
       "      <td>1405</td>\n",
       "      <td>1427</td>\n",
       "      <td>substitute Xabi Alonso</td>\n",
       "      <td>Xabi Alonso</td>\n",
       "      <td>10</td>\n",
       "      <td>[Who was in charge of FIFA?, Sepp Blatter, Wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85820</th>\n",
       "      <td>31qnsg6a5rtt5m7pens7xklnbwf87b</td>\n",
       "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
       "      <td>Was it his first game this year?</td>\n",
       "      <td>1415</td>\n",
       "      <td>1467</td>\n",
       "      <td>Xabi Alonso made his first appearance of the ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>11</td>\n",
       "      <td>[Who was in charge of FIFA?, Sepp Blatter, Wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85821</th>\n",
       "      <td>31qnsg6a5rtt5m7pens7xklnbwf87b</td>\n",
       "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
       "      <td>What position did the team reach?</td>\n",
       "      <td>1520</td>\n",
       "      <td>1555</td>\n",
       "      <td>Real moved up to third in the table</td>\n",
       "      <td>third</td>\n",
       "      <td>12</td>\n",
       "      <td>[Who was in charge of FIFA?, Sepp Blatter, Wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85822</th>\n",
       "      <td>31qnsg6a5rtt5m7pens7xklnbwf87b</td>\n",
       "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
       "      <td>Who was ahead of them?</td>\n",
       "      <td>1557</td>\n",
       "      <td>1582</td>\n",
       "      <td>six points behind Barca.</td>\n",
       "      <td>Barca.</td>\n",
       "      <td>13</td>\n",
       "      <td>[Who was in charge of FIFA?, Sepp Blatter, Wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85823</th>\n",
       "      <td>31qnsg6a5rtt5m7pens7xklnbwf87b</td>\n",
       "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
       "      <td>By how much?</td>\n",
       "      <td>1557</td>\n",
       "      <td>1581</td>\n",
       "      <td>six points behind Barca.</td>\n",
       "      <td>six points</td>\n",
       "      <td>14</td>\n",
       "      <td>[Who was in charge of FIFA?, Sepp Blatter, Wha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id  \\\n",
       "85819  31qnsg6a5rtt5m7pens7xklnbwf87b   \n",
       "85820  31qnsg6a5rtt5m7pens7xklnbwf87b   \n",
       "85821  31qnsg6a5rtt5m7pens7xklnbwf87b   \n",
       "85822  31qnsg6a5rtt5m7pens7xklnbwf87b   \n",
       "85823  31qnsg6a5rtt5m7pens7xklnbwf87b   \n",
       "\n",
       "                                                   story  \\\n",
       "85819  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
       "85820  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
       "85821  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
       "85822  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
       "85823  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
       "\n",
       "                                question  answer_span_start  answer_span_end  \\\n",
       "85819                     Who was a sub?               1405             1427   \n",
       "85820   Was it his first game this year?               1415             1467   \n",
       "85821  What position did the team reach?               1520             1555   \n",
       "85822             Who was ahead of them?               1557             1582   \n",
       "85823                       By how much?               1557             1581   \n",
       "\n",
       "                                        answer_span_text       answer  \\\n",
       "85819                             substitute Xabi Alonso  Xabi Alonso   \n",
       "85820   Xabi Alonso made his first appearance of the ...          Yes   \n",
       "85821                Real moved up to third in the table        third   \n",
       "85822                          six points behind Barca.        Barca.   \n",
       "85823                           six points behind Barca.   six points   \n",
       "\n",
       "       turn_id                                            history  \n",
       "85819       10  [Who was in charge of FIFA?, Sepp Blatter, Wha...  \n",
       "85820       11  [Who was in charge of FIFA?, Sepp Blatter, Wha...  \n",
       "85821       12  [Who was in charge of FIFA?, Sepp Blatter, Wha...  \n",
       "85822       13  [Who was in charge of FIFA?, Sepp Blatter, Wha...  \n",
       "85823       14  [Who was in charge of FIFA?, Sepp Blatter, Wha...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Train dataframe shape after the split: {train_df.shape}')\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataframe shape after the split: (21452, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>story</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_span_start</th>\n",
       "      <th>answer_span_end</th>\n",
       "      <th>answer_span_text</th>\n",
       "      <th>answer</th>\n",
       "      <th>turn_id</th>\n",
       "      <th>history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3wj1oxy92agboo5nlq4r7bndc3t8a8</td>\n",
       "      <td>New York (CNN) -- More than 80 Michael Jackson...</td>\n",
       "      <td>Where was the Auction held?</td>\n",
       "      <td>243</td>\n",
       "      <td>284</td>\n",
       "      <td>Hard Rock Cafe in New York's Times Square</td>\n",
       "      <td>Hard Rock Cafe</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3wj1oxy92agboo5nlq4r7bndc3t8a8</td>\n",
       "      <td>New York (CNN) -- More than 80 Michael Jackson...</td>\n",
       "      <td>How much did they make?</td>\n",
       "      <td>180</td>\n",
       "      <td>210</td>\n",
       "      <td>reaping a total $2 million. \\n</td>\n",
       "      <td>$2 million.</td>\n",
       "      <td>2</td>\n",
       "      <td>[Where was the Auction held?, Hard Rock Cafe]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3wj1oxy92agboo5nlq4r7bndc3t8a8</td>\n",
       "      <td>New York (CNN) -- More than 80 Michael Jackson...</td>\n",
       "      <td>How much did they expected?</td>\n",
       "      <td>292</td>\n",
       "      <td>342</td>\n",
       "      <td>pre-sale expectations of only $120,000 in sal...</td>\n",
       "      <td>$120,000</td>\n",
       "      <td>3</td>\n",
       "      <td>[Where was the Auction held?, Hard Rock Cafe, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3wj1oxy92agboo5nlq4r7bndc3t8a8</td>\n",
       "      <td>New York (CNN) -- More than 80 Michael Jackson...</td>\n",
       "      <td>WHo buy the Jackson Glove</td>\n",
       "      <td>1295</td>\n",
       "      <td>1365</td>\n",
       "      <td>Hoffman Ma, who bought the glove on behalf of ...</td>\n",
       "      <td>Hoffman Ma</td>\n",
       "      <td>4</td>\n",
       "      <td>[Where was the Auction held?, Hard Rock Cafe, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3wj1oxy92agboo5nlq4r7bndc3t8a8</td>\n",
       "      <td>New York (CNN) -- More than 80 Michael Jackson...</td>\n",
       "      <td>Where was the buyer of the glove from?</td>\n",
       "      <td>1331</td>\n",
       "      <td>1366</td>\n",
       "      <td>behalf of Ponte 16 Resort in Macau,</td>\n",
       "      <td>Macau</td>\n",
       "      <td>5</td>\n",
       "      <td>[Where was the Auction held?, Hard Rock Cafe, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id  \\\n",
       "0  3wj1oxy92agboo5nlq4r7bndc3t8a8   \n",
       "1  3wj1oxy92agboo5nlq4r7bndc3t8a8   \n",
       "2  3wj1oxy92agboo5nlq4r7bndc3t8a8   \n",
       "3  3wj1oxy92agboo5nlq4r7bndc3t8a8   \n",
       "4  3wj1oxy92agboo5nlq4r7bndc3t8a8   \n",
       "\n",
       "                                               story  \\\n",
       "0  New York (CNN) -- More than 80 Michael Jackson...   \n",
       "1  New York (CNN) -- More than 80 Michael Jackson...   \n",
       "2  New York (CNN) -- More than 80 Michael Jackson...   \n",
       "3  New York (CNN) -- More than 80 Michael Jackson...   \n",
       "4  New York (CNN) -- More than 80 Michael Jackson...   \n",
       "\n",
       "                                 question  answer_span_start  answer_span_end  \\\n",
       "0             Where was the Auction held?                243              284   \n",
       "1                 How much did they make?                180              210   \n",
       "2             How much did they expected?                292              342   \n",
       "3               WHo buy the Jackson Glove               1295             1365   \n",
       "4  Where was the buyer of the glove from?               1331             1366   \n",
       "\n",
       "                                    answer_span_text          answer  turn_id  \\\n",
       "0          Hard Rock Cafe in New York's Times Square  Hard Rock Cafe        1   \n",
       "1                     reaping a total $2 million. \\n     $2 million.        2   \n",
       "2   pre-sale expectations of only $120,000 in sal...        $120,000        3   \n",
       "3  Hoffman Ma, who bought the glove on behalf of ...      Hoffman Ma        4   \n",
       "4                behalf of Ponte 16 Resort in Macau,           Macau        5   \n",
       "\n",
       "                                             history  \n",
       "0                                                 []  \n",
       "1      [Where was the Auction held?, Hard Rock Cafe]  \n",
       "2  [Where was the Auction held?, Hard Rock Cafe, ...  \n",
       "3  [Where was the Auction held?, Hard Rock Cafe, ...  \n",
       "4  [Where was the Auction held?, Hard Rock Cafe, ...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Validation dataframe shape after the split: {val_df.shape}')\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train passages count: 5754\n",
      "Validation passages count: 1439\n",
      "\n",
      "Train QaA count: 85824 \t\t Train QaA ratio: 0.80\n",
      "Validation QaA count: 21452 \t Validation QaA ratio: 0.20\n"
     ]
    }
   ],
   "source": [
    "print(f'Train passages count: {len(train_df.groupby(by=[\"id\"]))}')\n",
    "print(f'Validation passages count: {len(val_df.groupby(by=[\"id\"]))}')\n",
    "\n",
    "print()\n",
    "\n",
    "len_tot=len(train_df)+len(val_df)\n",
    "print(f'Train QaA count: {len(train_df)} \\t\\t Train QaA ratio: {len(train_df)/len_tot:.2f}')\n",
    "print(f'Validation QaA count: {len(val_df)} \\t Validation QaA ratio: {len(val_df)/len_tot:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the train, validation and test dataloaders are provided for future training purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloader_builder import get_dataloader\n",
    "\n",
    "train_dataloader = get_dataloader(train_df)\n",
    "val_dataloader = get_dataloader(val_df)\n",
    "test_dataloader = get_dataloader(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a21de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Task 3] Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from models.model import Model \n",
    "\n",
    "model_name = 'distilroberta-base'\n",
    "\n",
    "model = Model(model_name=model_name, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question sample: \"When was the Vat formally opened?\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pitti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "c:\\Users\\pitti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer by the model: \",.,,,, Park,ness,ism\"\n",
      "\n",
      "True answer: \"It was formally established in 1475\"\n"
     ]
    }
   ],
   "source": [
    "question_sample = train_df.iloc[0]['question']\n",
    "passage_sample = train_df.iloc[0]['story']\n",
    "answer_sample = train_df.iloc[0]['answer']\n",
    "\n",
    "print(f'Question sample: \"{question_sample}\"')\n",
    "print()\n",
    "print(f'Predicted answer by the model: \"{model.generate(passage_sample, question_sample)}\"')\n",
    "print()\n",
    "print(f'True answer: \"{answer_sample}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 6] Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pitti\\Desktop\\Artificial Intelligence - Master degree\\Second year\\Natural Language Processing\\Assignments\\NLP-assignment-2\\main_trainer_redone.ipynb Cella 55\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pitti/Desktop/Artificial%20Intelligence%20-%20Master%20degree/Second%20year/Natural%20Language%20Processing/Assignments/NLP-assignment-2/main_trainer_redone.ipynb#Y144sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_dataloader))\n",
      "File \u001b[1;32mc:\\Users\\pitti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\pitti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\pitti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\pitti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pitti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pitti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pitti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pitti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    169\u001b[0m elem_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mnext\u001b[39m(it))\n\u001b[0;32m    170\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(elem) \u001b[39m==\u001b[39m elem_size \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m it):\n\u001b[1;32m--> 171\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meach element in list of batch should be of equal size\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pitti\\Desktop\\Artificial Intelligence - Master degree\\Second year\\Natural Language Processing\\Assignments\\NLP-assignment-2\\main_trainer_redone.ipynb Cella 55\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pitti/Desktop/Artificial%20Intelligence%20-%20Master%20degree/Second%20year/Natural%20Language%20Processing/Assignments/NLP-assignment-2/main_trainer_redone.ipynb#Y132sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m train \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pitti/Desktop/Artificial%20Intelligence%20-%20Master%20degree/Second%20year/Natural%20Language%20Processing/Assignments/NLP-assignment-2/main_trainer_redone.ipynb#Y132sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train(train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader, model\u001b[39m=\u001b[39;49mmodel, device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\pitti\\Desktop\\Artificial Intelligence - Master degree\\Second year\\Natural Language Processing\\Assignments\\NLP-assignment-2\\utils\\training.py:17\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_dataloader, model, epochs, steps_per_update, steps_empty_cache, device, plot)\u001b[0m\n\u001b[0;32m     14\u001b[0m epochs2 \u001b[39m=\u001b[39m epochs[\u001b[39m1\u001b[39m]\n\u001b[0;32m     15\u001b[0m epochs3 \u001b[39m=\u001b[39m epochs[\u001b[39m2\u001b[39m]\n\u001b[1;32m---> 17\u001b[0m loss_history1, optim1 \u001b[39m=\u001b[39m train_tokenImportancesExtractor(train_dataloader, token_importances_extractor, tokenizer, \n\u001b[0;32m     18\u001b[0m                                               epochs\u001b[39m=\u001b[39;49mepochs1, \n\u001b[0;32m     19\u001b[0m                                               learning_rate\u001b[39m=\u001b[39;49m\u001b[39m1e-5\u001b[39;49m,\n\u001b[0;32m     20\u001b[0m                                               steps_per_update\u001b[39m=\u001b[39;49msteps_per_update, steps_empty_cache\u001b[39m=\u001b[39;49msteps_empty_cache,\n\u001b[0;32m     21\u001b[0m                                               device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m     22\u001b[0m \u001b[39mif\u001b[39;00m plot \u001b[39mand\u001b[39;00m epochs1\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[0;32m     23\u001b[0m     plt\u001b[39m.\u001b[39mplot(loss_history1)\n",
      "File \u001b[1;32mc:\\Users\\pitti\\Desktop\\Artificial Intelligence - Master degree\\Second year\\Natural Language Processing\\Assignments\\NLP-assignment-2\\utils\\training.py:67\u001b[0m, in \u001b[0;36mtrain_tokenImportancesExtractor\u001b[1;34m(train_dataloader, token_importances_extractor, tokenizer, epochs, learning_rate, optimizer, loss_history, steps_per_update, steps_empty_cache, device)\u001b[0m\n\u001b[0;32m     65\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     66\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 67\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader, \u001b[39m0\u001b[39m):\n\u001b[0;32m     68\u001b[0m     \u001b[39m# get the inputs; data is a list of [inputs, labels]\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     (passage, question), (answer, sep_starts, sep_ends) \u001b[39m=\u001b[39m data\n\u001b[0;32m     71\u001b[0m     inputs \u001b[39m=\u001b[39m tokenizer(\n\u001b[0;32m     72\u001b[0m                 question,\n\u001b[0;32m     73\u001b[0m                 passage,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     78\u001b[0m             )\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\pitti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\pitti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\pitti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\pitti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pitti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pitti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pitti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pitti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    169\u001b[0m elem_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mnext\u001b[39m(it))\n\u001b[0;32m    170\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(elem) \u001b[39m==\u001b[39m elem_size \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m it):\n\u001b[1;32m--> 171\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meach element in list of batch should be of equal size\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "from utils.training import train \n",
    "\n",
    "train(train_dataloader=train_dataloader, model=model, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "from utils.build_model import Model\n",
    "\n",
    "M1_name = 'distilroberta-base'\n",
    "M2_name = 'prajjwal1/bert-tiny'\n",
    "\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained(M1_name)\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(M2_name)\n",
    "\n",
    "M1 = Model(M1_name, tokenizer=tokenizer_1)\n",
    "M2 = Model(M2_name, tokenizer=tokenizer_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "def f_theta_QP(model: Model, tokenizer: PreTrainedTokenizer, question: str, passage: str, max_length: int = 512, \n",
    "               device: str = 'cuda') -> str:\n",
    "    inputs = tokenizer(question, passage, max_length=max_length, truncation=True, padding=True,\n",
    "                       return_tensors=\"pt\").to(device)\n",
    "    generated_ids = model.forward(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, device=device)\n",
    "    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question sample: \"When was the Vat formally opened?\"\n",
      "\n",
      "Predicted answer by the first model: \".,\"\n",
      "Predicted answer by the second model: \"mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar\"\n",
      "\n",
      "True answer: \"It was formally established in 1475\"\n"
     ]
    }
   ],
   "source": [
    "question_sample = train_df.iloc[0]['question']\n",
    "passage_sample = train_df.iloc[0]['story']\n",
    "answer_sample = train_df.iloc[0]['answer']\n",
    "\n",
    "print(f'Question sample: \"{question_sample}\"')\n",
    "print()\n",
    "print(f'Predicted answer by the first model: \"{f_theta_QP(M1, tokenizer_1, question_sample, passage_sample)}\"')\n",
    "print(f'Predicted answer by the second model: \"{f_theta_QP(M2, tokenizer_2, question_sample, passage_sample)}\"')\n",
    "print()\n",
    "print(f'True answer: \"{answer_sample}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def f_theta_QPH(model: Model, tokenizer: PreTrainedTokenizer, question: str, passage: str, history: List[str],\n",
    "                max_length: int = 512, device: str = 'cuda') -> str:\n",
    "    separator = f' {tokenizer.sep_token} '\n",
    "    question_and_history = question + f'{separator if len(history) else \"\"}' + separator.join(history)\n",
    "    \n",
    "    inputs = tokenizer(question_and_history, passage, max_length=max_length, truncation=True, padding=True,\n",
    "                       return_tensors=\"pt\").to(device)\n",
    "    generated_ids = model.forward(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, device=device)\n",
    "    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question sample: \"how do scholars divide the library?\"\n",
      "\n",
      "Predicted answer by the first model: \".,\"\n",
      "Predicted answer by the second model: \"mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar\"\n",
      "\n",
      "True answer: \"into periods\"\n"
     ]
    }
   ],
   "source": [
    "question_sample = train_df.iloc[5]['question']\n",
    "passage_sample = train_df.iloc[5]['story']\n",
    "history = train_df.iloc[5]['history']\n",
    "answer_sample = train_df.iloc[5]['answer']\n",
    "\n",
    "print(f'Question sample: \"{question_sample}\"')\n",
    "print()\n",
    "print(f'Predicted answer by the first model: \"{f_theta_QPH(M1, tokenizer_1, question_sample, passage_sample, history)}\"')\n",
    "print(f'Predicted answer by the second model: \"{f_theta_QPH(M2, tokenizer_2, question_sample, passage_sample, history)}\"')\n",
    "print()\n",
    "print(f'True answer: \"{answer_sample}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, return_history=False):\n",
    "        self.story=[d['story'] for d in data]\n",
    "        self.questions=[d['questions'] for d in data]\n",
    "        self.answers=[d['answers'] for d in data]\n",
    "        lengths = [len(doc['questions']) for doc in data]\n",
    "        self.lengths = np.cumsum(np.array(lengths,dtype=np.int32))\n",
    "        self.R_H=return_history\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.lengths[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        f_idx=int(np.where(self.lengths > idx)[0][0])\n",
    "        if f_idx>0:\n",
    "            q_idx=idx-self.lengths[f_idx-1]\n",
    "        else:\n",
    "            q_idx=idx\n",
    "\n",
    "        passage=self.story[f_idx]\n",
    "        questions=self.questions[f_idx]\n",
    "        answers=self.answers[f_idx]\n",
    "        question=questions[q_idx]['input_text']\n",
    "        answer=answers[q_idx]['input_text']\n",
    "\n",
    "        if self.R_H:\n",
    "            print([ [questions[i]['input_text'],answers[i]['input_text']] for i in range(q_idx)])\n",
    "            history=np.concatenate([ [questions[i]['input_text'],answers[i]['input_text']] for i in range(q_idx)],0)\n",
    "            return (passage,question,history), answer\n",
    "        \n",
    "        #input_ids = torch.tensor(self.encodings['input_ids'])\n",
    "        #target_ids = torch.tensor(self.labels[idx])\n",
    "    \n",
    "        inputs = self.tokenizer(\n",
    "            question,\n",
    "            passage,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids\n",
    "\n",
    "        labels = self.tokenizer(\n",
    "            answer,\n",
    "            max_length=100,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids\n",
    "\n",
    "        return {\"input_ids\": inputs.squeeze(0).to('cuda'), \"labels\": labels.squeeze(0).to('cuda')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=T2)\n",
    "\n",
    "# metric = load('accuracy')\n",
    "training_args = TrainingArguments(output_dir='/prova', evaluation_strategy=\"epoch\", num_train_epochs=3)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "    }\n",
    "\n",
    "M2.config.decoder_start_token_id = T2.cls_token_id\n",
    "M2.config.pad_token_id = T2.pad_token_id\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=M2,\n",
    "    tokenizer=T2,\n",
    "    args=training_args,\n",
    "    train_dataset=CustomDataset(train, T2),\n",
    "    eval_dataset=CustomDataset(validation, T2),\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(torch.optim.AdamW(M2.parameters(), lr=0.001), None),\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 85810\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32181\n",
      "  Number of trainable parameters = 8935226\n",
      " 15%|        | 4800/32181 [06:50<37:19, 12.23it/s]Saving model checkpoint to /prova\\checkpoint-500\n",
      "Configuration saved in /prova\\checkpoint-500\\config.json\n",
      "Model weights saved in /prova\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in /prova\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in /prova\\checkpoint-500\\special_tokens_map.json\n",
      "c:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to /prova\\checkpoint-1000\n",
      "Configuration saved in /prova\\checkpoint-1000\\config.json\n",
      "Model weights saved in /prova\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in /prova\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in /prova\\checkpoint-1000\\special_tokens_map.json\n",
      "c:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      " 15%|        | 4800/32181 [08:27<48:15,  9.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1498\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1499\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1500\u001b[0m )\n\u001b[1;32m-> 1501\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1502\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1503\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1504\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1505\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1506\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1723\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1720\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[0;32m   1722\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1723\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1724\u001b[0m \n\u001b[0;32m   1725\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[0;32m   1726\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1727\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn [19], line 36\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[39mreturn\u001b[39;00m (passage,question,history), answer\n\u001b[0;32m     33\u001b[0m \u001b[39m#input_ids = torch.tensor(self.encodings['input_ids'])\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m#target_ids = torch.tensor(self.labels[idx])\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer(\n\u001b[0;32m     37\u001b[0m     question,\n\u001b[0;32m     38\u001b[0m     passage,\n\u001b[0;32m     39\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,\n\u001b[0;32m     40\u001b[0m     padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     41\u001b[0m     truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     42\u001b[0m     return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     43\u001b[0m )\u001b[39m.\u001b[39minput_ids\n\u001b[0;32m     45\u001b[0m labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[0;32m     46\u001b[0m     answer,\n\u001b[0;32m     47\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     51\u001b[0m )\u001b[39m.\u001b[39minput_ids\n\u001b[0;32m     53\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m: inputs\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m: labels\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)}\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2488\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2486\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2487\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2488\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_one(text\u001b[39m=\u001b[39mtext, text_pair\u001b[39m=\u001b[39mtext_pair, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_kwargs)\n\u001b[0;32m   2489\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2490\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2594\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2574\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2575\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2576\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2591\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2592\u001b[0m     )\n\u001b[0;32m   2593\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2594\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2595\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2596\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2597\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2598\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2599\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   2600\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2601\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2602\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2603\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2604\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2605\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2606\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2607\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2608\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2609\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2610\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2611\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2612\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2613\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2667\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2657\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2658\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2659\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2660\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2664\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2665\u001b[0m )\n\u001b[1;32m-> 2667\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_plus(\n\u001b[0;32m   2668\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2669\u001b[0m     text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2670\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2671\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2672\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2673\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2674\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2675\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2676\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2677\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2678\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2679\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2680\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2681\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2682\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2683\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2684\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2685\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2686\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:502\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[0;32m    480\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    481\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    499\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[0;32m    501\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[1;32m--> 502\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m    503\u001b[0m         batched_input,\n\u001b[0;32m    504\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m    505\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m    506\u001b[0m         padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m    507\u001b[0m         truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m    508\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m    509\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m    510\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    511\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m    512\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    513\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    514\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m    515\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    516\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m    517\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m    518\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m    519\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[0;32m    522\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[0;32m    523\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[0;32m    524\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:429\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[0;32m    422\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m    423\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    426\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    427\u001b[0m )\n\u001b[1;32m--> 429\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode_batch(\n\u001b[0;32m    430\u001b[0m     batch_text_or_text_pairs,\n\u001b[0;32m    431\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m    432\u001b[0m     is_pretokenized\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m    433\u001b[0m )\n\u001b[0;32m    435\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[0;32m    441\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[0;32m    442\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[0;32m    443\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[0;32m    453\u001b[0m ]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, n_epochs=3, learning_rate=1e-3):\n",
    "    model.to('cuda')\n",
    "\n",
    "    L=[]\n",
    "\n",
    "    model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            (passage, question), answer = data\n",
    "\n",
    "            # text_input = [question[i] + ' [SEP] ' + passage[i] for i in range(len(passage))]\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = tokenizer(\n",
    "                question,\n",
    "                passage,\n",
    "                padding=True,\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).input_ids\n",
    "\n",
    "            labels = tokenizer(\n",
    "                answer,\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).input_ids\n",
    "\n",
    "            #X=torch.tensor(input_ids,device='cuda')\n",
    "            #y=torch.tensor(labels,device='cuda')\n",
    "            \n",
    "            #print(X.shape,y.shape)\n",
    "            \n",
    "            #if X.shape[1]>500:\n",
    "            #    continue\n",
    "\n",
    "            # the forward function automatically creates the correct decoder_input_ids\n",
    "            outputs = model(inputs.to('cuda'), labels=labels.to('cuda'))\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            L.append(loss.detach().cpu().numpy())\n",
    "            \n",
    "            epoch_time = time.time() - start_time\n",
    "            batch_time = epoch_time/(i+1)\n",
    "            \n",
    "            print(f\"epoch: {epoch + 1}/{n_epochs}, {i + 1}/{len(train_dataloader)}, {epoch_time:.0f}s {batch_time*1e3:.0f}ms/step, lr: {optimizer.param_groups[0]['lr']:.3g}, loss: {running_loss/(i+1):.3g}\", end = '\\r')\n",
    "\n",
    "        print(f\"epoch: {epoch + 1}/{n_epochs}, {i + 1}/{len(train_dataloader)}, {epoch_time:.0f}s {batch_time*1e3:.0f}ms/step, lr: {optimizer.param_groups[0]['lr']:.3g}, loss: {running_loss/(i+1):.3g}\")\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/3, 229/1341, 175s 762ms/step, lr: 0.001, loss: 1.43\r"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 806.00 MiB (GPU 0; 6.00 GiB total capacity; 4.41 GiB already allocated; 0 bytes free; 4.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(M2,T2)\n",
      "Cell \u001b[1;32mIn [21], line 53\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, tokenizer, n_epochs, learning_rate)\u001b[0m\n\u001b[0;32m     50\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m), labels\u001b[39m=\u001b[39mlabels\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     51\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m---> 53\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     54\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     56\u001b[0m \u001b[39m# print statistics\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 806.00 MiB (GPU 0; 6.00 GiB total capacity; 4.41 GiB already allocated; 0 bytes free; 4.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train(M2,T2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d14990a399795653de35f1f5395269163893147857e74010bc072f6042c8e8da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
